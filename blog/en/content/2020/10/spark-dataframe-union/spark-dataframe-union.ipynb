{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Union DataFrames in Spark\n",
    "- Slug: spark-dataframe-union\n",
    "- Date: 2020-10-30 19:44:44\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, union\n",
    "- Author: Ben Du\n",
    "- Modified: 2020-10-30 19:44:44\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment \n",
    "\n",
    "1. `union` relies on column order rather than column names. \n",
    "    This is the same as in SQL.\n",
    "    For columns that the type don't match, \n",
    "    the super type is used.\n",
    "    However, \n",
    "    this is really dangerous if you are careful. \n",
    "    It is suggested that you define a function call unionByName to hanle this.\n",
    "    ```\n",
    "    def unionByName(df1, df2):\n",
    "        ...\n",
    "    ```\n",
    "    A way to avoid the ordering issue is to select columns\n",
    "    to make sure that columns of the 2 DataFrames have the same ordering.\n",
    "\n",
    "2. Union 2 PySpark DataFrames. Notice that pyspark.sql.DataFrame.union does not dedup by default (since Spark 2.0).\n",
    "\n",
    "3. Union multiple PySpark DataFrames at once using `functools.reduce`.\n",
    "\n",
    "4. The number of partitions of the final DataFrame equals \n",
    "    the sum of the number of partitions of each of the unioned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.0.1-bin-hadoop3.2\")\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"PySpark_Notebook\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=(\n",
    "            (1, \"a\", \"foo\", 3.0), (2, \"b\", \"bar\", 4.0), (3, \"c\", \"foo\", 5.0),\n",
    "            (4, \"d\", \"bar\", 7.0)\n",
    "        ),\n",
    "        columns=(\"col1\", \"col2\", \"col3\", \"col4\")\n",
    "    )\n",
    ")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.filter(col(\"col1\") <= 2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.union(df2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.filter(col(\"col1\") > 2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduce(DataFrame.union, [df1, df2, df3]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(DataFrame.union, [df1, df2, df3]).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to avoid the ordering issue is to select columns\n",
    "to make sure that columns of the 2 DataFrames have the same ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col4|col3|col2|col1|\n",
      "+----+----+----+----+\n",
      "| 3.0| foo|   a|   1|\n",
      "| 4.0| bar|   b|   2|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df2.select(\"col4\", \"col3\", \"col2\", \"col1\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col4|col3|col2|col1|\n",
      "+----+----+----+----+\n",
      "| 3.0| foo|   a|   1|\n",
      "| 4.0| bar|   b|   2|\n",
      "| 5.0| foo|   c|   3|\n",
      "| 7.0| bar|   d|   4|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(*df4.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col4|col3|col2|col1|\n",
      "+----+----+----+----+\n",
      "| 3.0| foo|   a|   1|\n",
      "| 4.0| bar|   b|   2|\n",
      "| 5.0| foo|   c|   3|\n",
      "| 7.0| bar|   d|   4|\n",
      "| 3.0| foo|   a|   1|\n",
      "| 4.0| bar|   b|   2|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(*df4.columns).union(df4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An exception is raised if the numbers of columns of the 2 DataFrames do not match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|freq| word|\n",
      "+----+-----+\n",
      "|   1|hello|\n",
      "|   2|world|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = spark.createDataFrame(\n",
    "    pd.DataFrame(data=((1, \"hello\"), (2, \"world\")), columns=(\"freq\", \"word\"))\n",
    ")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"how\", 1000, 0), (\"are\", 300, 0), (\"you\", 100, 0)),\n",
    "        columns=(\"word\", \"freq\", \"group\")\n",
    "    )\n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns;;\n'Union\n:- LogicalRDD [freq#370L, word#371], false\n+- LogicalRDD [word#383, freq#384L, group#385L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-533c1d47b5b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \"\"\"\n\u001b[0;32m-> 1558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns;;\n'Union\n:- LogicalRDD [freq#370L, word#371], false\n+- LogicalRDD [word#383, freq#384L, group#385L], false\n"
     ]
    }
   ],
   "source": [
    "df5.union(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Union RDDs in Spark](http://www.legendu.net/misc/blog/union-RDDs-in-spark)\n",
    "\n",
    "https://stackoverflow.com/questions/37612622/spark-unionall-multiple-dataframes\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}