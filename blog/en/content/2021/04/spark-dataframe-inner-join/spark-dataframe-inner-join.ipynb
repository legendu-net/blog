{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2021-04-13 10:38:47\n",
    "- Title: Inner Join of Spark DataFrames\n",
    "- Slug: spark-dataframe-inner-join\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, PySpark, DataFrame, inner join, big data, join\n",
    "- Modified: 2021-03-13 10:38:47\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps \n",
    "\n",
    "1. Select only needed columns before joining.\n",
    "\n",
    "2. Rename joining column names to be identical (if different) before joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.1.1-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"Join\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Names in Both Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "| name|      date|duration|\n",
      "+-----+----------+--------+\n",
      "|  bob|2015-01-13|       4|\n",
      "|alice|2015-04-23|      10|\n",
      "+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\", 10)),\n",
    "        columns=(\"name\", \"date\", \"duration\")\n",
    "    )\n",
    ")\n",
    "left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|upload|\n",
      "+-----+------+\n",
      "|alice|   100|\n",
      "|  bob|    23|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right = spark.createDataFrame(\n",
    "    pd.DataFrame(data=((\"alice\", 100), (\"bob\", 23)), columns=(\"name\", \"upload\"))\n",
    ")\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate columns happens if you use an expression as join condition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+-----+------+\n",
      "| name|      date|duration| name|upload|\n",
      "+-----+----------+--------+-----+------+\n",
      "|alice|2015-04-23|      10|alice|   100|\n",
      "|  bob|2015-01-13|       4|  bob|    23|\n",
      "+-----+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, left[\"name\"] == right[\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+-----+------+\n",
      "| name|      date|duration| name|upload|\n",
      "+-----+----------+--------+-----+------+\n",
      "|alice|2015-04-23|      10|alice|   100|\n",
      "|  bob|2015-01-13|       4|  bob|    23|\n",
      "+-----+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, left.name == right.name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using (a list of) string names can avoid duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+\n",
      "| name|      date|duration|upload|\n",
      "+-----+----------+--------+------+\n",
      "|alice|2015-04-23|      10|   100|\n",
      "|  bob|2015-01-13|       4|    23|\n",
      "+-----+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, [\"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+\n",
      "| name|      date|duration|upload|\n",
      "+-----+----------+--------+------+\n",
      "|alice|2015-04-23|      10|   100|\n",
      "|  bob|2015-01-13|       4|    23|\n",
      "+-----+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Columns Not in Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "| name|      date|duration|\n",
      "+-----+----------+--------+\n",
      "|  bob|2015-01-13|       4|\n",
      "|alice|2015-04-23|      10|\n",
      "+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\", 10)),\n",
    "        columns=(\"name\", \"date\", \"duration\")\n",
    "    )\n",
    ")\n",
    "left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "| name|upload|duration|\n",
      "+-----+------+--------+\n",
      "|alice|   100|       1|\n",
      "|  bob|    23|       2|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"alice\", 100, 1), (\"bob\", 23, 2)),\n",
    "        columns=(\"name\", \"upload\", \"duration\")\n",
    "    )\n",
    ")\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the 2 DataFrame by the `name` column. \n",
    "Duplicate columns happen as the `duration` column is in both DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+--------+\n",
      "| name|      date|duration|upload|duration|\n",
      "+-----+----------+--------+------+--------+\n",
      "|alice|2015-04-23|      10|   100|       1|\n",
      "|  bob|2015-01-13|       4|    23|       2|\n",
      "+-----+----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select via string names works on non duplicate columns.\n",
    "Exception will be throw if you select a duplicate column using string names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|      date|\n",
      "+-----+----------+\n",
      "|alice|2015-04-23|\n",
      "|  bob|2015-01-13|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.alias(\"l\").join(right.alias(\"r\"), \"name\").select(\"name\", \"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select using `column` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "| name|      date|duration|\n",
      "+-----+----------+--------+\n",
      "|alice|2015-04-23|      10|\n",
      "|  bob|2015-01-13|       4|\n",
      "+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, \"name\").select(left[\"name\"], left[\"date\"], left[\"duration\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using table alias is probably the most convenient way (in syntax).\n",
    "Similar to SQL, \n",
    "you don't have to specify table when there's no ambiguition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+\n",
      "| name|      date|duration|upload|\n",
      "+-----+----------+--------+------+\n",
      "|alice|2015-04-23|      10|   100|\n",
      "|  bob|2015-01-13|       4|    23|\n",
      "+-----+----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.alias(\"l\").join(right.alias(\"r\"),\n",
    "                     \"name\").select(\"name\", \"date\", \"l.duration\", \"upload\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star in Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `*` can be used to select all columns from a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "| name|      date|duration|\n",
      "+-----+----------+--------+\n",
      "|alice|2015-04-23|      10|\n",
      "|  bob|2015-01-13|       4|\n",
      "+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.alias(\"l\").join(right.alias(\"r\"), \"name\").select(\"l.*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Names for Joining\n",
    "\n",
    "If you want to do inner join only, \n",
    "it is suggested that you rename the columns to join to have the same names\n",
    "so that \n",
    "\n",
    "1. minimal number of columns\n",
    "2. no duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "| name|      date|duration|\n",
      "+-----+----------+--------+\n",
      "|  bob|2015-01-13|       4|\n",
      "|alice|2015-04-23|      10|\n",
      "+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\", 10)),\n",
    "        columns=(\"name\", \"date\", \"duration\")\n",
    "    )\n",
    ")\n",
    "left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "|   nm|upload|duration|\n",
      "+-----+------+--------+\n",
      "|alice|   100|       1|\n",
      "|  bob|    23|       2|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"alice\", 100, 1), (\"bob\", 23, 2)), columns=(\"nm\", \"upload\", \"duration\")\n",
    "    )\n",
    ")\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+-----+------+--------+\n",
      "| name|      date|duration|   nm|upload|duration|\n",
      "+-----+----------+--------+-----+------+--------+\n",
      "|alice|2015-04-23|      10|alice|   100|       1|\n",
      "|  bob|2015-01-13|       4|  bob|    23|       2|\n",
      "+-----+----------+--------+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right, left[\"name\"] == right[\"nm\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+------+--------+\n",
      "| name|      date|duration|upload|duration|\n",
      "+-----+----------+--------+------+--------+\n",
      "|alice|2015-04-23|      10|   100|       1|\n",
      "|  bob|2015-01-13|       4|    23|       2|\n",
      "+-----+----------+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left.join(right.withColumnRenamed(\"nm\", \"name\"), [\"name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## References \n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}