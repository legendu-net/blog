Status: published
Date: 2020-03-02 21:35:55
Author: Benjamin Du
Slug: number-precision-in-deep-learning
Title: Number Precision in Deep Learning
Category: AI
Tags: AI, data science, machine learning, deep learning, number precision, float, int8
Modified: 2020-03-02 21:35:55

**
Things on this page are fragmentary and immature notes/thoughts of the author.
Please read with your own judgement!
**

https://www.mathworks.com/company/newsletters/articles/what-is-int8-quantization-and-why-is-it-popular-for-deep-neural-networks.html

https://engineering.fb.com/ai-research/floating-point-math/

[Rethinking floating point for deep learning](https://research.fb.com/publications/rethinking-floating-point-for-deep-learning/)

[Training Deep Neural Networks with 8-bit Floating Point Numbers](https://papers.nips.cc/paper/7994-training-deep-neural-networks-with-8-bit-floating-point-numbers.pdf)

[8-Bit Quantization and TensorFlow Lite: Speeding up mobile inference with low precision](https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd)