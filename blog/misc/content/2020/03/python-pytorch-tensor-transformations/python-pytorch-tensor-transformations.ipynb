{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Tensor Transformations in TorchVision \n",
    "- Slug: python-pytorch-tensor-transformations\n",
    "- Date: 2020-03-03 11:45:08\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Python, AI, data science, machine learning, deep learning, PyTorch, tensor, transformation, TorchVision\n",
    "- Author: Ben Du\n",
    "- Modified: 2020-03-03 11:45:08\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACUAAAA2CAIAAADPvsdhAAAJ2klEQVR4nKWYXYxd1XXHf2vtfe65l+uZ8RTs2A62hRMIivmQI2DASR3TuA3FgBBxlYZ8CImnqCoFkQcaRSovrvKQSG4r5a2JYkUKKEodx2iQajUEJ0ExLQIcVwSDXRvF44+Mscfzce85Z++9+nDufDCewR5n6Whmz7l39tr/tdb+r//ecuOWTz/55JMPbN3WaDQgiUgSADUAMwMUgEo8IMaMmQBkloDCAeQRIAGQJAFIApxvjo2N/XDvz+Sfdv7zU0895aMVReEzq6pK8WZWNtTM+s3HGJ1zQNQYY8QyMzM1QE1nZvxwE82896cmJ/3q1auzLOtOjQMppVarVXZDPU4pdYqq0WioagihihWAJRGpcdc/wRZ31LMYQgih1Wp5URM1F2Oe581zE+fPHG8MDLYHB9UFM1uuje54pzt6qr+/368cqKoqOhURLM3Eto7qZU1VkwUvIjHGPM9HRkZe+v4PX3311cbA4JYtW7Z9ZUe73X5l/8vDw8PnTpxcv379vV/74u233z5ZVap6RR4+aCkl8c5niWVZztE/HvrO997/8Q9uQs/B7/f9etOpzuDNN7/75LMy9c5qXJf08/95a+jb35a7bq6qqswATFP9a/7cC2W0zoKv/xg5cGB4eHgjjQaNEhcIu3fvHhwcHJ863k+/Qww7cPjwf7744pY7PyEiInODODuezuii5pcVkSJmB9+5buzsCnIlDVIIMj563Eb/7zqa0M2QQLg+hvHf/HeoprIsK8yY3RtxvufF8+rrlXY6HUU9vqJSLJEyMkUjUVGDetzpdFJKdXA+gOuK8WmhRqb9G2+YpDlJB1RIRghMRCYzrAEVOkUs8Rs23dJo5tGSJbEkiTJRJmzmMWHmScQ0DV3rx5KKCCkt37r1wfse7NItKSPRsBqfxwdCJHbp3rHxjs2PPALUEK/OtHIumnHLRzY/+3d33vulk7QmSRU+4aFRYV3CIWz1pr/a/i87uefWjlnwPklKksQysWxJ/ny9XtfpNNeuveOZZ+64++793/3Xo+XRHC/IBBPLWf7E41+/YccONq6jLCV3Vw0O8IVa5aXsawLtjX1sfPAvh24d3bXr3V++GCn9DZse+8Y3Gg9vJcvOayDTSGCawWPN7CxhBX6Bdzfd9KVvfevlkcm3j7z91aefbm7bRiaxLGn+Sch6/lxUF3XCKRCbHmj64FeuufG7/zBw9mx68LOTlFM+xzfE4iyaK+4M8/1d+iqEYGZrbrttTYyTYGYppbmMIiKLbbMPUmvdOxVI05Tgg1gQw4KIBFERKZvXAJMZTHfOTBKYSwBJqLcZ4GzGvbWimFnr4gSqxAhMNFOWZZN5s/6mIOAWyt8STURSSs75/fv3//ane51zJlIUxcDH1zz++OOtj/Z3u13Xq7CF4tn7RGa5v+bJNIe+am6s0edF2dds6stvHf7H77SPH8zJE3Tpnr75nhX3PVp8VCMaFRGSLFifSzRVVdU3hoePHD8yxJ8JkkBR5xwxzmPUHl9Lr9KcmS3Ew7OY5uolH01Eml0be/ONo7t/+knKNgjSxTnKzARxSYiYooIkJ1ePryfdVKm6zz333ET3eIsWdAFBdDot8/FdZtZeFi95bQjSijScZ88vzuzeu4rUJFaIIIYJvf3jEi4RHCJkkatRIkDd4lV1fHz8hZ/8ZCyMNWkWFImUSIbZIqJNxRAjmiYc04109mNQev0MU0wNNVRj8JayseK9/xgeefXXN+JOw9o1d27d/EhFXwKZJtX6f8VMzESuNn9m5pxj5L3nn3/+WqYysibL7nviCfqX5a/8qsuELFJ0ukDzlLQwMUpCUoIEAyWtSs58f68dfC3jmknK/P6tfOHPcWG0nUOyOaJmrl1tfTYap48d+/nevYYFYh99Dz30EO02RdGT3ovmTw1JTqSunJnyrfP6QYsQc7MmcHrqte/9qDP29lrsXWzdPfeu/IvN5IZzpUhkEXQssT5ram40Gp3XX9+3b5/DBcKq1qrtjz7qli0jhMvO8CH+Ll2lA9dfucbZi7/7t90fOT+iNKYo13zxr9l2V+UiuUCCpHidztQ8cby0/IkIqr89cOA3r/0qJy8o1rDm8w88QLMJkSs4VyimlsTM5hFPFBdlWkCYYtqq6Cfjf997Y9e/O8avxb1LtuFvtg98ZlM3D1EiznAZaEQSiim4hJo4n5xPzuzSo8biVnf5Iy+88Pqh13PyknLD9Rs279jhnOu19RjrhiDIYvvPX3L4WMDqQm2X2nnn7cM/eG4DU5BHwse++jBDn+wmTUajCriaV0yohOBNSCSVpBLNRMD8UvJXFHv27BkZO9ak2aVzfXb99u3byfM8z1NKDZR2G+dExLBI9N7jXJZlKaWqqnr4FjthzA10I0mWZRw6/Psf7VlLpwWnsMql0YO/ax8+dtEDLEvJe28H32xOdXI0o1GcOnPhv1668IejZVly28fWrVtXtpeIb3Jy0uMNy8kvdC/s3LnTOTeeSUrpmhCAm0a7BUUf4vGj50Z37dp1ZLDZ6XTuf+bvH3vsMecup5dqGu1xTUjtRIUKfgBLlLe9f1pRhwpSUiVSjnM4D0a6larbOZN3GgVF8+y5rAj0lUvBF0JRFB06JeX0SrzDFUSHi6RAaNACDFN0im5JWSEdOmaG2ex5ejFzImZWeirCwPpVmx79QuvEaJb1zkSzihSuScQYO68dGilGPFlJacs/vnlo6Oy1/UVRrBi6G+er6nLxNDMRsWQpJVat+tunn+ZiZGb/9MJdf1UI4fSzO4+/ctyTBcLtt9yy6Zvf5LoBioIVHudSSkuI59iKpnPtS1ujSQT6ukYI4205T/RQEd5cKZ+5zqZWpxi1Q+W9iWSX8Vc3BBVVVcVSSqLzmc/qFcRU30nV4szjvfdMq29xoqoYXlU/hF9m7q3MrHIJcBaR3snP9a6zMgCNmDh6d0+BlNThfGmxInmcxBR80kuZ+k8xM6v1WSLFGDFT1Zn6uhq9VCOrazJLAIVLgCnROOvSKaqCWBBWmhDNS1aEwlwyIUtXxtdXbp/evn3t2rXXRleW5ZnPD5FlVVWp6kyN+aianFPxZuadr6pqAXHWu02qw65MZy7MuZm9mAmZ7//y/Wu/fL+YtiHL07hZDMHMhMyJV6I/ceJEWZZZllVVFUIQES6B2wvAImGYm/wYI+BjBKJnpjhUNc/zP14Y9fte2r9+4yce/uznVJVoQJojPeqBigJm1YdEshKA9+t7w0YdBzHw4oBK9OL5Cz9+8QVZdevG/v7+z33qrr6+PjVCCJo15s2lSQHRhUXe3PJOUmucuqgMyBCgwk6ePHnw2FuyeuhT3W43dTq1XFBVs/lyRs0BSWfjObfEFhvXHXRuLbi+1v8DWE0boU5N9vEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=37x54 at 0x7F0238065080>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"../../home/media/poker/4h.png\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 37,  62,  59],\n",
       "        [149, 174, 171],\n",
       "        [225, 238, 239],\n",
       "        ...,\n",
       "        [232, 250, 249],\n",
       "        [217, 235, 234],\n",
       "        [122, 156, 154]],\n",
       "\n",
       "       [[127, 133, 134],\n",
       "        [240, 246, 247],\n",
       "        [244, 243, 246],\n",
       "        ...,\n",
       "        [239, 240, 243],\n",
       "        [243, 244, 247],\n",
       "        [218, 233, 236]],\n",
       "\n",
       "       [[152, 158, 159],\n",
       "        [245, 251, 252],\n",
       "        [237, 236, 239],\n",
       "        ...,\n",
       "        [235, 236, 239],\n",
       "        [235, 236, 239],\n",
       "        [227, 242, 245]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 45,  66,  64],\n",
       "        [153, 174, 172],\n",
       "        [233, 237, 239],\n",
       "        ...,\n",
       "        [235, 239, 241],\n",
       "        [226, 230, 232],\n",
       "        [132, 157, 152]],\n",
       "\n",
       "       [[ 24,  45,  43],\n",
       "        [ 38,  59,  57],\n",
       "        [105, 109, 111],\n",
       "        ...,\n",
       "        [119, 123, 125],\n",
       "        [ 92,  96,  98],\n",
       "        [ 37,  62,  57]],\n",
       "\n",
       "       [[ 25,  55,  50],\n",
       "        [ 17,  47,  42],\n",
       "        [ 15,  38,  33],\n",
       "        ...,\n",
       "        [ 16,  40,  40],\n",
       "        [ 16,  40,  40],\n",
       "        [ 19,  53,  49]]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array(img)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 37, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torchvision.transforms.ToTensor](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor)\n",
    "\n",
    "Converts a PIL Image or numpy.ndarray `(H x W x C)` in the range [0, 255] \n",
    "to a `torch.FloatTensor` of shape `(C x H x W)` in the range [0.0, 1.0] \n",
    "if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) \n",
    "or if the numpy.ndarray has `dtype = np.uint8`.\n",
    "\n",
    "**This is the transformation that you alway need** \n",
    "when preparing dataset for a computer vision task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1451, 0.5843, 0.8824,  ..., 0.9098, 0.8510, 0.4784],\n",
       "         [0.4980, 0.9412, 0.9569,  ..., 0.9373, 0.9529, 0.8549],\n",
       "         [0.5961, 0.9608, 0.9294,  ..., 0.9216, 0.9216, 0.8902],\n",
       "         ...,\n",
       "         [0.1765, 0.6000, 0.9137,  ..., 0.9216, 0.8863, 0.5176],\n",
       "         [0.0941, 0.1490, 0.4118,  ..., 0.4667, 0.3608, 0.1451],\n",
       "         [0.0980, 0.0667, 0.0588,  ..., 0.0627, 0.0627, 0.0745]],\n",
       "\n",
       "        [[0.2431, 0.6824, 0.9333,  ..., 0.9804, 0.9216, 0.6118],\n",
       "         [0.5216, 0.9647, 0.9529,  ..., 0.9412, 0.9569, 0.9137],\n",
       "         [0.6196, 0.9843, 0.9255,  ..., 0.9255, 0.9255, 0.9490],\n",
       "         ...,\n",
       "         [0.2588, 0.6824, 0.9294,  ..., 0.9373, 0.9020, 0.6157],\n",
       "         [0.1765, 0.2314, 0.4275,  ..., 0.4824, 0.3765, 0.2431],\n",
       "         [0.2157, 0.1843, 0.1490,  ..., 0.1569, 0.1569, 0.2078]],\n",
       "\n",
       "        [[0.2314, 0.6706, 0.9373,  ..., 0.9765, 0.9176, 0.6039],\n",
       "         [0.5255, 0.9686, 0.9647,  ..., 0.9529, 0.9686, 0.9255],\n",
       "         [0.6235, 0.9882, 0.9373,  ..., 0.9373, 0.9373, 0.9608],\n",
       "         ...,\n",
       "         [0.2510, 0.6745, 0.9373,  ..., 0.9451, 0.9098, 0.5961],\n",
       "         [0.1686, 0.2235, 0.4353,  ..., 0.4902, 0.3843, 0.2235],\n",
       "         [0.1961, 0.1647, 0.1294,  ..., 0.1569, 0.1569, 0.1922]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = trans(img)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 54, 37])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1451, 0.5843, 0.8824,  ..., 0.9098, 0.8510, 0.4784],\n",
       "         [0.4980, 0.9412, 0.9569,  ..., 0.9373, 0.9529, 0.8549],\n",
       "         [0.5961, 0.9608, 0.9294,  ..., 0.9216, 0.9216, 0.8902],\n",
       "         ...,\n",
       "         [0.1765, 0.6000, 0.9137,  ..., 0.9216, 0.8863, 0.5176],\n",
       "         [0.0941, 0.1490, 0.4118,  ..., 0.4667, 0.3608, 0.1451],\n",
       "         [0.0980, 0.0667, 0.0588,  ..., 0.0627, 0.0627, 0.0745]],\n",
       "\n",
       "        [[0.2431, 0.6824, 0.9333,  ..., 0.9804, 0.9216, 0.6118],\n",
       "         [0.5216, 0.9647, 0.9529,  ..., 0.9412, 0.9569, 0.9137],\n",
       "         [0.6196, 0.9843, 0.9255,  ..., 0.9255, 0.9255, 0.9490],\n",
       "         ...,\n",
       "         [0.2588, 0.6824, 0.9294,  ..., 0.9373, 0.9020, 0.6157],\n",
       "         [0.1765, 0.2314, 0.4275,  ..., 0.4824, 0.3765, 0.2431],\n",
       "         [0.2157, 0.1843, 0.1490,  ..., 0.1569, 0.1569, 0.2078]],\n",
       "\n",
       "        [[0.2314, 0.6706, 0.9373,  ..., 0.9765, 0.9176, 0.6039],\n",
       "         [0.5255, 0.9686, 0.9647,  ..., 0.9529, 0.9686, 0.9255],\n",
       "         [0.6235, 0.9882, 0.9373,  ..., 0.9373, 0.9373, 0.9608],\n",
       "         ...,\n",
       "         [0.2510, 0.6745, 0.9373,  ..., 0.9451, 0.9098, 0.5961],\n",
       "         [0.1686, 0.2235, 0.4353,  ..., 0.4902, 0.3843, 0.2235],\n",
       "         [0.1961, 0.1647, 0.1294,  ..., 0.1569, 0.1569, 0.1922]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = trans(arr)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 54, 37])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://stackoverflow.com/questions/54268029/how-to-convert-a-pytorch-tensor-into-a-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}