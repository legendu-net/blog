{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Handling Complicated Data Types in Python and PySpark\n",
    "- Slug: python-complicated-data-types\n",
    "- Date: 2020-05-07 20:42:08\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Python, PySpark, Parquet, IO, data types, complicated, None, list, ndarray, StructType, StructField\n",
    "- Author: Ben Du\n",
    "- Modified: 2021-04-07 20:42:08\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. An element in a pandas DataFrame can be any (complicated) type in Python.\n",
    "    To save a padnas DataFrame with arbitrary (complicated) types as it is, \n",
    "    you have to use pickle. \n",
    "    For example, \n",
    "    you can use `pandas.DataFrame.to_pickle` and `pandas.DataFrame.read_pickle`\n",
    "    or you can use `pick.dump` and `pickle.load` directly\n",
    "    since `to_pickle` and `read_pickle` in pandas are simple wrappers over `pickle.dump` and `pickle.load`.\n",
    "    \n",
    "2. Apache Parquet is a binary file format \n",
    "    that stores data in a columnar fashion \n",
    "    for compressed, efficient columnar data representation.\n",
    "    It is a very popular file format when working with big data (Hadoop/Spark, etc.) ecosystem. \n",
    "    However, \n",
    "    be aware that a Parquet file does not support arbitrary data types in Python!\n",
    "    For example, \n",
    "    an element of the list type is converted to a numpy array first.\n",
    "    This requires types of elements of a column to be consistent.\n",
    "    For this reason,\n",
    "    `numpy.ndarray` is preferred to `list` \n",
    "    if you want write the pandas DataFrame to a Parquet file later.\n",
    "    \n",
    "3. It is good practice to have consistent and specific types when working with Parquet file in Python,\n",
    "    especially when you have to deal with the Parquet file in Spark/PySpark later.\n",
    "    \n",
    "    - `numpy.ndarray` is preferred to `list` and `tuple`.\n",
    "    - Avoid mixing different types (`numpy.ndarray`, `list`, `tuple`, etc.) in the same column,\n",
    "        even if it still might work.\n",
    "    - An empty `numpy.ndarray` is preferred to `None` as handling of `None` can be inconssitent in different situations.\n",
    "        Specically, \n",
    "        avoid a column with all `None`'s. \n",
    "        When written to a Parquet file and then read into Spark/PySpark,\n",
    "        a column with all `None`'s is inferred as `IntegerType` (due to lack of specific type information). \n",
    "        This might or might not what you want.\n",
    "\n",
    "4. You can specify a schema to help Spark/PySpark to read a Parquet file. \n",
    "    However, \n",
    "    I don't think this is a good practice.\n",
    "    One advantage of Parquet file is that it has schema. \n",
    "    The accurate schema should be stored in the Parquet file.\n",
    "    Otherwise, it is hard for other people for figure the correct shcema to use."
   ]
  },
  {
   "source": [
    "## Types in pandas DataFrame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "### Complicated Data Types"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "An element in a pandas DataFrame can be any (complicated) type in Python.\n",
    "Below is an example pandas DataFrame with complicated data types."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   x1               x2\n",
       "0   1             None\n",
       "1   2               []\n",
       "2   3       {'key': 1}\n",
       "3   4  [0.1, 0.2, 0.3]\n",
       "4   5  [how, 0.5, 0.6]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>{'key': 1}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[0.1, 0.2, 0.3]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[how, 0.5, 0.6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "pdf_1 = pd.DataFrame(\n",
    "    {\n",
    "        \"x1\": [1, 2, 3, 4, 5],\n",
    "        \"x2\":\n",
    "            [\n",
    "                None,\n",
    "                np.array([]), {\n",
    "                    \"key\": 1\n",
    "                },\n",
    "                np.array([0.1, 0.2, 0.3]), [\"how\", 0.5, 0.6]\n",
    "            ]\n",
    "    }\n",
    ")\n",
    "\n",
    "pdf_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "x1     int64\n",
       "x2    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "pdf_1.dtypes"
   ]
  },
  {
   "source": [
    "### Mixed Data Types in pandas DataFrame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The pandas DataFrame `pdf_1` contains mixed data types in its column `x2`.\n",
    "It cannot be written into a Parquet file \n",
    "as data types in the column `x2` are not compatible in Parquet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ArrowInvalid",
     "evalue": "('cannot mix list and non-list, non-null values', 'Conversion failed for column x2 with type object')",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c3bfee9de3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/pdf_1.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         return to_parquet(\n\u001b[0m\u001b[1;32m   2456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFilePathOrBuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     impl.write(\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preserve_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         arrays = [convert_column(c, f)\n\u001b[0m\u001b[1;32m    591\u001b[0m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[1;32m    592\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         arrays = [convert_column(c, f)\n\u001b[0m\u001b[1;32m    591\u001b[0m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[1;32m    592\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    575\u001b[0m             e.args += (\"Conversion failed for column {!s} with type {!s}\"\n\u001b[1;32m    576\u001b[0m                        .format(col.name, col.dtype),)\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfield_nullable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             raise ValueError(\"Field {} was non-nullable but pandas column \"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         except (pa.ArrowInvalid,\n\u001b[1;32m    573\u001b[0m                 \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: ('cannot mix list and non-list, non-null values', 'Conversion failed for column x2 with type object')"
     ]
    }
   ],
   "source": [
    "pdf_1.to_parquet(\"/tmp/pdf_1.parquet\")"
   ]
  },
  {
   "source": [
    "However, \n",
    "you can serialize and deserialize the pandas DataFrame using pickle. \n",
    "As a matter of factor,\n",
    "almost all Python objects can be serialized and deserialized using pickle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_1.to_pickle(\"/tmp/pdf_1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   x1               x2\n",
       "0   1             None\n",
       "1   2               []\n",
       "2   3       {'key': 1}\n",
       "3   4  [0.1, 0.2, 0.3]\n",
       "4   5  [how, 0.5, 0.6]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>{'key': 1}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[0.1, 0.2, 0.3]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[how, 0.5, 0.6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "with open(\"/tmp/pdf_1.pickle\", \"rb\") as fin:\n",
    "    pdf_1c = pickle.load(fin)\n",
    "pdf_1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data types are compatible in Parquet. \n",
    "For example,\n",
    "`None`, `numpy.ndarray` and `list` \n",
    "can be mixed in a pandas DataFrame column\n",
    "and can be written into a Parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   x1     x2               x3\n",
       "0   1   True             None\n",
       "1   2  False               []\n",
       "2   3   True               []\n",
       "3   4  False  [0.1, 0.2, 0.3]\n",
       "4   5   True  [0.4, 0.5, 0.6]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>True</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>False</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>False</td>\n      <td>[0.1, 0.2, 0.3]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>True</td>\n      <td>[0.4, 0.5, 0.6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "pdf_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"x1\": [1, 2, 3, 4, 5],\n",
    "        \"x2\": [True, False, True, False, True],\n",
    "        \"x3\": [None, np.array([]), [],\n",
    "              np.array([0.1, 0.2, 0.3]), [0.4, 0.5, 0.6]]\n",
    "    }\n",
    ")\n",
    "\n",
    "pdf_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "x1     int64\n",
       "x2      bool\n",
       "x3    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "pdf_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_2.to_parquet(\"/tmp/pdf_2.parquet\", flavor=\"spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   x1     x2               x3\n",
       "0   1   True             None\n",
       "1   2  False               []\n",
       "2   3   True               []\n",
       "3   4  False  [0.1, 0.2, 0.3]\n",
       "4   5   True  [0.4, 0.5, 0.6]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>True</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>False</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>False</td>\n      <td>[0.1, 0.2, 0.3]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>True</td>\n      <td>[0.4, 0.5, 0.6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "pdf_2c = pd.read_parquet(\"/tmp/pdf_2.parquet\")\n",
    "pdf_2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "x1     int64\n",
       "x2      bool\n",
       "x3    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "pdf_2c.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "type(pdf_2.x3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "type(pdf_2.x3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "type(pdf_2.x3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "type(pdf_2.x3[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "type(pdf_2.x3[4])"
   ]
  },
  {
   "source": [
    "Even if some data types can be mixed together and are compatible in Parquet format,\n",
    "you should avoid doing this. \n",
    "It is good practice to have consistent and specific types when working with Parquet file in Python,\n",
    "especially when you have to deal with the Parquet file in Spark/PySpark later.\n",
    "\n",
    "- `numpy.ndarray` is preferred to `list` and `tuple`.\n",
    "- Avoid mixing different types (`numpy.ndarray`, `list`, `tuple`, etc.) in the same column,\n",
    "    even if it still might work.\n",
    "- An empty `numpy.ndarray` is preferred to `None` as handling of `None` can be inconssitent in different situations.\n",
    "    Specically, \n",
    "    avoid a column with all `None`'s. \n",
    "    When written to a Parquet file and then read into Spark/PySpark,\n",
    "    a column with all `None`'s is inferred as `IntegerType` (due to lack of specific type information). \n",
    "    This might or might not what you want.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Parquet File into PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-*\"))))\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName(\"PySpark\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+---------------+\n| x1|   x2|             x3|\n+---+-----+---------------+\n|  1| true|           null|\n|  2|false|             []|\n|  3| true|             []|\n|  4|false|[0.1, 0.2, 0.3]|\n|  5| true|[0.4, 0.5, 0.6]|\n+---+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/tmp/pdf_2.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(x1,LongType,true),StructField(x2,BooleanType,true),StructField(x3,ArrayType(DoubleType,true),true)))"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "source": [
    "Notice that the `None` value is represented as `null` in the above PySpark DataFrame. \n",
    "The `x3` column is represented as an array of double values. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You can provide an customized schema when reading a table into Spark. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"x1\", LongType(), False),\n",
    "        StructField(\"x2\", BooleanType(), False),\n",
    "        StructField(\"x3\", ArrayType(DoubleType()), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+---------------+\n| x1|   x2|             x3|\n+---+-----+---------------+\n|  1| true|           null|\n|  2|false|             []|\n|  3| true|             []|\n|  4|false|[0.1, 0.2, 0.3]|\n|  5| true|[0.4, 0.5, 0.6]|\n+---+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.read.schema(schema).parquet(\"/tmp/pdf_2.parquet\")\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(x1,LongType,true),StructField(x2,BooleanType,true),StructField(x3,ArrayType(DoubleType,true),true)))"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "df_2.schema"
   ]
  },
  {
   "source": [
    "An empty array is NOT considered as `null`!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+---------------+-------+\n| x1|   x2|             x3|is_null|\n+---+-----+---------------+-------+\n|  1| true|           null|   true|\n|  2|false|             []|  false|\n|  3| true|             []|  false|\n|  4|false|[0.1, 0.2, 0.3]|  false|\n|  5| true|[0.4, 0.5, 0.6]|  false|\n+---+-----+---------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df_2.select(col(\"x1\"), col(\"x2\"), col(\"x3\"), col(\"x3\").isNull().alias(\"is_null\")).show()"
   ]
  },
  {
   "source": [
    "You can write the PySpark DataFrame into a Parquet file \n",
    "and then load it into a pandas DataFrame.\n",
    "\n",
    "1. `null` is converted to `None`.\n",
    "2. An array is represented as `numpy.ndarray`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.write.mode(\"overwrite\").parquet(\"/tmp/df_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   x1     x2               x3\n",
       "0   1   True             None\n",
       "1   2  False               []\n",
       "2   3   True               []\n",
       "3   4  False  [0.1, 0.2, 0.3]\n",
       "4   5   True  [0.4, 0.5, 0.6]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>x3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>True</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>False</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>True</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>False</td>\n      <td>[0.1, 0.2, 0.3]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>True</td>\n      <td>[0.4, 0.5, 0.6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "pdf_3 = pd.read_parquet(\"/tmp/df_2.parquet\")\n",
    "pdf_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "x1     int64\n",
       "x2      bool\n",
       "x3    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "pdf_3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "type(pdf_3.x3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "type(pdf_3.x3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "type(pdf_3.x3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "type(pdf_3.x3[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "type(pdf_3.x3[4])"
   ]
  },
  {
   "source": [
    "## Schema of PySpark DataFrames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----+---------------+\n| x1|   x2|             x3|\n+---+-----+---------------+\n|  1| true|           null|\n|  2|false|             []|\n|  3| true|             []|\n|  4|false|[0.1, 0.2, 0.3]|\n|  5| true|[0.4, 0.5, 0.6]|\n+---+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(x1,LongType,true),StructField(x2,BooleanType,true),StructField(x3,ArrayType(DoubleType,true),true)))"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "source": [
    "`DataFrame.schema` is of the `pyspark.sql.types.StructType` type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "type(df.schema)"
   ]
  },
  {
   "source": [
    "A `StructType` is iterable\n",
    "and each element is of the `StructField` type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "StructField(x1,LongType,true)\nStructField(x2,BooleanType,true)\nStructField(x3,ArrayType(DoubleType,true),true)\n"
     ]
    }
   ],
   "source": [
    "for field in df.schema:\n",
    "    print(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructField(x1,LongType,true)"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "f = df.schema[0]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'x1'"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "f.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LongType"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "f.dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.types.LongType"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "type(f.dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'long'"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "f.dataType.typeName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'bigint'"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "f.dataType.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "f.nullable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'x1:bigint'"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "f.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecimalType(18,3)"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "DecimalType(18, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructField(gmb,DecimalType(18,2),false)"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "f2 = StructField(\"gmb\", DecimalType(18, 2), False)\n",
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'gmb'"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "f2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecimalType(18,2)"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "f2.dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'decimal'"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "f2.dataType.typeName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'decimal(18,2)'"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "f2.dataType.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'gmb:decimal(18,2)'"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "f2.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x1    bigint\nx2    boolean\nx3    array<double>\n"
     ]
    }
   ],
   "source": [
    "for field in df.schema:\n",
    "    print(f\"{field.name}    {field.dataType.simpleString()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://arrow.apache.org/docs/python/parquet.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}