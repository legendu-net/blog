{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Tips on Dataset in PyTorch\n",
    "- Slug: python-pytorch-dataset\n",
    "- Date: 2020-05-18 08:12:02\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Python, AI, data science, machine learning, deep learning, PyTorch, Dataset\n",
    "- Author: Ben Du\n",
    "- Modified: 2020-05-18 08:12:02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is a good practice to save your data (e.g., images) into one pickle file\n",
    "   (or other format that you know how to deserialize).\n",
    "    This comes with several advantages.\n",
    "    First, it is easier and faster to read from a single big file rather than many small files. \n",
    "    Second, it avoids the system error of openning too many files.\n",
    "    Some example datasets (e.g., MNIST)\n",
    "    have separate training and testing files (i.e., 2 pickle files), \n",
    "    so that research work based on it can be easily reproduced.\n",
    "    I personally suggest that you keep only 1 file containing all data\n",
    "    when implementing your own Dataset class.\n",
    "    You can always use the function `torch.utils.data.random_split`\n",
    "    to split your dataset into training and testing datasets later.\n",
    "    For more details, \n",
    "    please refer to \n",
    "    [http://www.legendu.net/misc/blog/python-ai-split-dataset/](http://www.legendu.net/misc/blog/python-ai-split-dataset/#PyTorch).\n",
    "    \n",
    "    If one single file is too big (to load into memory),\n",
    "    you can split the data into several parts and use the class \n",
    "    [torchvision.datasets.DatasetFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder)\n",
    "    to help you load them.\n",
    "    If you do want to keep the raw images as separate files, \n",
    "    you can place them into different subfolders whose names represent the class names\n",
    "    and then use the class \n",
    "    [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder)\n",
    "    to help you load the data.\n",
    "    [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) \n",
    "    supports image extensions: `.jpg`, `.JPG`, `.jpeg`, `.JPEG`,\n",
    "    `.png`, `.PNG`, `.ppm`, `.PPM`, `.bmp` and `.BMP`.\n",
    "    \n",
    "2. It is a good practice to always shuffle the dataset for training\n",
    "    as it helps on the model convergence.\n",
    "    However,\n",
    "    never shuffle the dataseet for testing or prediction\n",
    "    as it helps avoid surprises \n",
    "    if you have to rely on the order of data points for evaluation.\n",
    "    \n",
    "1. When you implement your own Dataset class,\n",
    "    you need to inherit from \n",
    "    [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "    (or one of its subclasses).\n",
    "    You must overwrite the 2 methods `__len__` and `__getitem__`.\n",
    "\n",
    "2. When you implement your own Dataset class for image classification,\n",
    "    it is best to inherit from \n",
    "    [torchvision.datasets.vision.VisionDataset](https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py#L6)\n",
    "    .\n",
    "    For example, \n",
    "    [torchvision.datasets.MNIST](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py)\n",
    "    subclasses \n",
    "    [torchvision.datasets.vision.VisionDataset](https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py#L6)\n",
    "    . \n",
    "    You can use it as a template.\n",
    "    Notice you still only have to overwrite the 2 methods `__len__` and `__getitem__`\n",
    "    (even though the implementation of \n",
    "    [torchvision.datasets.MNIST](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py)\n",
    "    is much more complicated than that).\n",
    "    [torchvision.datasets.MNIST](https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py)\n",
    "    downloads data into the directory `MNIST/raw` \n",
    "    and make a copy of ready-to-use data into the directory `MNIST/processed`. \n",
    "    It doesn't matter whether you follow this convention or not\n",
    "    as long as you overwrite the 2 methods `__len__` and `__getitem__`.\n",
    "    What's more, the parameter `root` for the constructor of \n",
    "    [torchvision.datasets.vision.VisionDataset](https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py#L6)\n",
    "    is not critical \n",
    "    as long as your Dataset subclass knows where and how to load the data\n",
    "    (e.g., you can pass the full path of the data file as parameter for your Dataset subclass). \n",
    "    You can set it to `None` if you like. \n",
    "  \n",
    "3. When you implement a Dataset class for image classification,\n",
    "    it is best to have the method `__getitem__` return `(PIL.Image, target)`\n",
    "    and then use `torchvision.transforms.ToTensor` to convert `PIL.Image` to tensor\n",
    "    in the DataLoader.\n",
    "    The reason is that transforming modules in `trochvision.transforms` \n",
    "    behave differently on `PIL.Image` \n",
    "    and their equivalent numpy array. \n",
    "    You might get surprises if you have `__getitem__` return `(torch.Tensor, target)`.\n",
    "    If you do have `__getitem__` return `(torch.Tensor, target)`,\n",
    "    make sure to double check that they tensors are as expected \n",
    "    before feeding them into your model for training/prediction.\n",
    "\n",
    "4. `torchvision.transforms.ToTensor` (refered to as `ToTensor` in the following) \n",
    "    converts a `PIL.Image` to a numerical tensor with each value between [0, 1].\n",
    "    `ToTensor` on a boolean numpy array (representing a black/white image) \n",
    "    returns a boolean tensor (instead of converting it to a numeric tensor). \n",
    "    This is one reason that you should return `(PIL.Image, target)` \n",
    "    and avoid returning `(numpy.array, target)`\n",
    "    when implement your own Dataset class for image classification.\n",
    "        \n",
    "5. There is no need to return the target as a `torch.Tensor` (even though you can)\n",
    "    when you implement the method `__getitem__` of your own Dataset class.\n",
    "    The DataLoader will convert the batch of target values to `torch.Tensor` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False],\n",
       "       [ True, False,  True]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[True, True, False], [True, False, True]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True, False],\n",
       "         [ True, False,  True]]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = trans(arr)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f2c440b5da0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "y = torch.tensor([1, 0, 1, 0])\n",
    "dset = torch.utils.data.TensorDataset(x, y)\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1), tensor(1))\n",
      "(tensor(2), tensor(0))\n",
      "(tensor(3), tensor(1))\n",
      "(tensor(4), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "for d in dset:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImagePaths - a More Generalized Dataset Class for Images\n",
    "\n",
    "If you have a trained model\n",
    "and want to run it on unlabled data,\n",
    "you need a dataset for unlabled data. \n",
    "PyTorch does not have such a class but it is very easy to implement one by yourself.\n",
    "The class ImagePaths implemented below \n",
    "is able to handle the situations of both with and without labels. \n",
    "Actually, \n",
    "it can be seen as a more generalized version of the \n",
    "[torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder)\n",
    "class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ImagePaths(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for Image paths.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, paths, transform=None, transform_target=None, cache: bool = False\n",
    "    ):\n",
    "        \"\"\"Initialize an Image Path object.\n",
    "        :param paths: An iterable of paths to images. \n",
    "            For example, you can get image paths using pathlib.Path.glob.\n",
    "        :param transform: The transform function for the image (or input tensor).\n",
    "        :param transform_target: The transform function for the target/label.\n",
    "        \"\"\"\n",
    "        self.paths = list(paths)\n",
    "        labels = set(path.parent.name for path in self.paths)\n",
    "        if all(label.isdigit() for label in labels):\n",
    "            self.class_to_idx = {label: int(label) for label in labels}\n",
    "        else:\n",
    "            self.class_to_idx = {label: i for i, label in enumerate(labels)}\n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "        self.cache = cache\n",
    "        self._data = None\n",
    "        if self.cache:\n",
    "            self._data = [None] * len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.cache and self._data[index]:\n",
    "            return self._data[index]\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        target = self.class_to_idx[path.parent.name]\n",
    "        if self.transform_target:\n",
    "            target = self.transform_target(target)\n",
    "        pair = img, target\n",
    "        if self.cache:\n",
    "            self._data[index] = pair\n",
    "        return pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n",
    "\n",
    "Each batch in a `torch.utils.data.DataLoader` is a list of tensors. \n",
    "The length of of the list matches the length of the tuple in the underlying Dataset.\n",
    "Each tensor in the list/batch has a first dimenion matching the batch size.\n",
    "\n",
    "If you have specified the option `shuffle=False` (default),\n",
    "the order of the DataLoader is fixed. \n",
    "You get the same sequence each time you iterate the DataLoader.\n",
    "However, \n",
    "if you have specified the option `shuffle=True` (which should be used for training),\n",
    "the order of the DataLoader is random.\n",
    "**Each time you iterate the DataLoader, \n",
    "the underlying dataset is shuffled \n",
    "and thus you get a different sequence each time you iterate the DataLoader.\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0.0588, 0.2284, 0.0248, 0.3235, 0.4076, 0.7178, 0.5656, 0.5177, 0.9233,\n",
      "        0.8219])\n",
      "y: tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f2c42373518>\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10)\n",
    "y = torch.tensor([0, 1]).repeat(5)\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=3)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[tensor([0.4187, 0.3091, 0.6890]), tensor([0, 1, 0])]\n",
      "<class 'list'>\n",
      "[tensor([0.1153, 0.6208, 0.3379]), tensor([1, 0, 1])]\n",
      "<class 'list'>\n",
      "[tensor([0.4417, 0.0541, 0.3020]), tensor([0, 1, 0])]\n",
      "<class 'list'>\n",
      "[tensor([0.2231]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "for elem in data_loader:\n",
    "    print(type(elem))\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_DataLoader__initialized',\n",
       " '_DataLoader__multiprocessing_context',\n",
       " '_IterableDataset_len_called',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_auto_collation',\n",
       " '_dataset_kind',\n",
       " '_index_sampler',\n",
       " 'batch_sampler',\n",
       " 'batch_size',\n",
       " 'collate_fn',\n",
       " 'dataset',\n",
       " 'drop_last',\n",
       " 'multiprocessing_context',\n",
       " 'num_workers',\n",
       " 'pin_memory',\n",
       " 'sampler',\n",
       " 'timeout',\n",
       " 'worker_init_fn']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
       "the given dataset.\n",
       "\n",
       "The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
       "iterable-style datasets with single- or multi-process loading, customizing\n",
       "loading order and optional automatic batching (collation) and memory pinning.\n",
       "\n",
       "See :py:mod:`torch.utils.data` documentation page for more details.\n",
       "\n",
       "Arguments:\n",
       "    dataset (Dataset): dataset from which to load the data.\n",
       "    batch_size (int, optional): how many samples per batch to load\n",
       "        (default: ``1``).\n",
       "    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
       "        at every epoch (default: ``False``).\n",
       "    sampler (Sampler, optional): defines the strategy to draw samples from\n",
       "        the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
       "    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
       "        indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
       "        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
       "    num_workers (int, optional): how many subprocesses to use for data\n",
       "        loading. ``0`` means that the data will be loaded in the main process.\n",
       "        (default: ``0``)\n",
       "    collate_fn (callable, optional): merges a list of samples to form a\n",
       "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
       "        map-style dataset.\n",
       "    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
       "        into CUDA pinned memory before returning them.  If your data elements\n",
       "        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
       "        see the example below.\n",
       "    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
       "        if the dataset size is not divisible by the batch size. If ``False`` and\n",
       "        the size of dataset is not divisible by the batch size, then the last batch\n",
       "        will be smaller. (default: ``False``)\n",
       "    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
       "        from workers. Should always be non-negative. (default: ``0``)\n",
       "    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
       "        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
       "        input, after seeding and before data loading. (default: ``None``)\n",
       "\n",
       "\n",
       ".. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
       "             cannot be an unpicklable object, e.g., a lambda function. See\n",
       "             :ref:`multiprocessing-best-practices` on more details related\n",
       "             to multiprocessing in PyTorch.\n",
       "\n",
       ".. note:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
       "          When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
       "          ``len(dataset)`` (if implemented) is returned instead, regardless\n",
       "          of multi-process loading configurations, because PyTorch trust\n",
       "          user :attr:`dataset` code in correctly handling multi-process\n",
       "          loading to avoid duplicate data. See `Dataset Types`_ for more\n",
       "          details on these two types of datasets and how\n",
       "          :class:`~torch.utils.data.IterableDataset` interacts with `Multi-process data loading`_.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.utils.data.DataLoader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Useful Dataset Classes\n",
    "\n",
    "\n",
    "[torch.utils.data.ChainDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.ChainDataset)\n",
    "\n",
    "[torch.utils.data.Subset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset)\n",
    "\n",
    "[torchvision.datasets.vision.VisionDataset](https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py#L6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/pytorch/vision/blob/master/torchvision/datasets/mnist.py\n",
    "\n",
    "[VisionDataset](https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py#L6)\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "[Data loader without labels?](https://discuss.pytorch.org/t/data-loader-without-labels/67107)\n",
    "\n",
    "[How to load Images without using ImageFolder](https://discuss.pytorch.org/t/how-to-load-images-without-using-imagefolder/59999)\n",
    "\n",
    "[Is Pytorch DataLoader Iteration order stable?=](https://stackoverflow.com/questions/59314174/is-pytorch-dataloader-iteration-order-stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}