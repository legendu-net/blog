{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-06-27 12:57:10\n",
    "- Title: New Features in Spark 3\n",
    "- Slug: new-features-in-spark-3\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, big data, Spark, PySpark, Python, pandas, DataFrame, AQE, Spark 3\n",
    "- Modified: 2020-06-27 12:57:10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [AQE (Adaptive Query Execution)](https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html)\n",
    "\n",
    "To enable AQE,\n",
    "you have to set `spark.sql.adaptive.enabled` to `true`\n",
    "(using `--conf spark.sql.adaptive.enabled=true` in spark-submit \n",
    "or using `spark.config(\"spark.sql.adaptive,enabled\", \"true\") in Spark/PySpark code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDFs\n",
    "\n",
    "Pandas UDFs are user defined functions \n",
    "that are executed by Spark using Arrow \n",
    "to transfer data to Pandas to work with the data, \n",
    "which allows vectorized operations. \n",
    "A Pandas UDF is defined using `pandas_udf` as a decorator or to wrap the function, \n",
    "and no additional configuration is required. \n",
    "A Pandas UDF behaves as a regular PySpark function API in general.\n",
    "\n",
    "Pandas UDFs are introdduced in Spark 2.3. \n",
    "However, \n",
    "it has been greatly simplfied and made more Pythonic in Spark 3.0. \n",
    "Before Spark 3.0, \n",
    "Pandas UDFs used to be defined with `PandasUDFType`. \n",
    "From Spark 3.0 with Python 3.6+, \n",
    "you can also use Python type hints. \n",
    "Using Python type hints are preferred \n",
    "and using PandasUDFType will be deprecated in the future release.\n",
    "\n",
    "Note that the type hint should use `pandas.Series` in all cases \n",
    "but there is one variant that `pandas.DataFrame` should be used \n",
    "for its input or output type hint instead \n",
    "when the input or output column is of StructType. \n",
    "The following example shows a Pandas UDF \n",
    "which takes long column, string column and struct column, and outputs a struct column. \n",
    "It requires the function to specify the type hints of pandas.Series and pandas.DataFrame as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a sturct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\", )]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\"\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "# root\n",
    "# |-- long_column: long (nullable = true)\n",
    "# |-- string_column: string (nullable = true)\n",
    "# |-- struct_column: struct (nullable = true)\n",
    "# |    |-- col1: string (nullable = true)\n",
    "\n",
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()\n",
    "# |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
    "# |    |-- col1: string (nullable = true)\n",
    "# |    |-- col2: long (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html\n",
    "\n",
    "https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.htmlhttps://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}