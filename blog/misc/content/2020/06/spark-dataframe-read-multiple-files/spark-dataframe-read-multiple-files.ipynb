{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-06-17\n",
    "- Title: Read Multiple Files into a DataFrame in Spark\n",
    "- Slug: spark-dataframe-read-multiple-files\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, read, multiple, file\n",
    "- Modified: 2020-06-17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://hadoop.apache.org/docs/r2.7.2/api/org/apache/hadoop/fs/FileSystem.html#globStatus(org.apache.hadoop.fs.Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bd8a92-23f6-4c63-9dc2-88d7673f70fa",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.3.1\n",
    "org.apache.spark spark-sql_2.11 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@107f8243"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Spark Column Example\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/workdir/archives/projects/rilb/src/test/resources/abc_item_[1-2].parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/workdir/archives/projects/rilb/src/test/resources/abc_item_*.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/workdir/archives/projects/rilb/src/test/resources/abc_item_{1,2}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/workdir/archives/projects/rilb/src/test/resources/{abc_item_1.parquet,abc_item_2.parquet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/workdir/{archives/projects/rilb/src/test/resources/abc_item_1.parquet,archives/projects/rilb/src/test/resources/abc_item_2.parquet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_id: bigint, site_id: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/{workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Path does not exist",
     "output_type": "error",
     "text": "org.apache.spark.sql.AnalysisException: Path does not exist: file:/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet;\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:355)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\n  ... 48 elided\n",
     "traceback": [
      "\u001b[1;31morg.apache.spark.sql.AnalysisException: Path does not exist: file:/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet;\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.flatMap(List.scala:355)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Path does not exist",
     "output_type": "error",
     "text": "org.apache.spark.sql.AnalysisException: Path does not exist: file:/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file:/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet;\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:355)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\n  ... 48 elided\n",
     "traceback": [
      "\u001b[1;31morg.apache.spark.sql.AnalysisException: Path does not exist: file:/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file:/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet;\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.flatMap(List.scala:355)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file:///workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " Wrong FS",
     "output_type": "error",
     "text": "java.lang.IllegalArgumentException: Wrong FS: file://{/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet}, expected: file:///\n  at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\n  at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)\n  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:752)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)\n  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:252)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1625)\n  at org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:244)\n  at org.apache.spark.deploy.SparkHadoopUtil.globPathIfNecessary(SparkHadoopUtil.scala:254)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:707)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:355)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\n  ... 48 elided\n",
     "traceback": [
      "\u001b[1;31mjava.lang.IllegalArgumentException: Wrong FS: file://{/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet}, expected: file:///\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:752)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.Globber.glob(Globber.java:252)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1625)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.deploy.SparkHadoopUtil.globPath(SparkHadoopUtil.scala:244)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.deploy.SparkHadoopUtil.globPathIfNecessary(SparkHadoopUtil.scala:254)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:707)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.flatMap(List.scala:355)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"file://{/workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,/workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " java.net.URISyntaxException",
     "output_type": "error",
     "text": "java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0: {file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file://workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet%7D\n  at org.apache.hadoop.fs.Path.initialize(Path.java:206)\n  at org.apache.hadoop.fs.Path.<init>(Path.java:172)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:704)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:355)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\n  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\n  ... 48 elided\nCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 0: {file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file://workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet%7D\n  at java.net.URI$Parser.fail(URI.java:2848)\n  at java.net.URI$Parser.checkChars(URI.java:3021)\n  at java.net.URI$Parser.checkChar(URI.java:3031)\n  at java.net.URI$Parser.parse(URI.java:3047)\n  at java.net.URI.<init>(URI.java:746)\n  at org.apache.hadoop.fs.Path.initialize(Path.java:203)\n  ... 62 more\n",
     "traceback": [
      "\u001b[1;31mjava.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0: {file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file://workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet%7D\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.Path.initialize(Path.java:206)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.Path.<init>(Path.java:172)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:704)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.flatMap(List.scala:355)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:622)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:606)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m",
      "\u001b[1;31mCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 0: {file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file://workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet%7D\u001b[0;0m",
      "\u001b[1;31m  at java.net.URI$Parser.fail(URI.java:2848)\u001b[0;0m",
      "\u001b[1;31m  at java.net.URI$Parser.checkChars(URI.java:3021)\u001b[0;0m",
      "\u001b[1;31m  at java.net.URI$Parser.checkChar(URI.java:3031)\u001b[0;0m",
      "\u001b[1;31m  at java.net.URI$Parser.parse(URI.java:3047)\u001b[0;0m",
      "\u001b[1;31m  at java.net.URI.<init>(URI.java:746)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.hadoop.fs.Path.initialize(Path.java:203)\u001b[0;0m",
      "\u001b[1;31m  ... 62 more\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\n",
    "    \"{file:///workdir/archives/projects/rilb/src/test/resources/abc_item_1.parquet,file:///workdir/archives/projects/rilb/src/test/resources/abc_item_2.parquet}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}