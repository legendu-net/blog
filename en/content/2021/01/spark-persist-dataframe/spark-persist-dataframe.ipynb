{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Persist and Checkpoint DataFrames in Spark\n",
    "- Slug: spark-persist-checkpoint-dataframe\n",
    "- Date: 2021-01-24 08:52:28\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, persist, big data, cache, checkpoint\n",
    "- Author: Ben Du\n",
    "- Modified: 2021-03-24 08:52:28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist vs Checkpoint\n",
    "\n",
    "[Spark Internals - 6-CacheAndCheckpoint.md](https://github.com/JerryLead/SparkInternals/blob/master/markdown/english/6-CacheAndCheckpoint.md)\n",
    "has a good explanation of persist vs checkpoint. \n",
    "\n",
    "1. Persist/Cache in Spark is lazy and doesn't truncate the lineage\n",
    "    while checkpoint is eager (by default) and truncates the lineage.\n",
    "\n",
    "2. Generally speaking,\n",
    "    `DataFrame.persist` has a better performance than `DataFrame.checkpoint`. \n",
    "    However,\n",
    "    `DataFrame.checkpoint` is more robust and is preferred in any of the following situations.\n",
    "    \n",
    "    - When running Spark applications on a noisry cluster\n",
    "    - When a DataFrame is computed using lots of partitions (which increases the chance of node failures)\n",
    "    - When you want to be able to recover from a failed Spark application using checkpoints\n",
    "    \n",
    "3. Due to issues/bugs in persist/cache and checkpoint,\n",
    "    (see the next 2 sections for details),\n",
    "    manually writing a DataFrame into disk and then read it back\n",
    "    can be more efficient than persisting to disk and checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Traps for Persist/Cache\n",
    "\n",
    "1. The method `DataFrame.cache` is a special case of `DataFrame.persist`.\n",
    "    `DataFrame.cache` caches a DataFrame to the default storage level (`MEMORY_AND_DISK`)\n",
    "    which is equivalent to `DataFrame.persist()` (with the default behavior).\n",
    "    However,\n",
    "    `DataFrame.persist` is more flexible on the storage leve\n",
    "    and is preferred over `DataFrame.cache`. \n",
    "    \n",
    "2. The definition of the class `pyspark.StorageLevel` is as below.\n",
    "\n",
    "        :::python\n",
    "        class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)\n",
    "            ...\n",
    "\n",
    "    And it has the following pre-defined instances.\n",
    "\n",
    "    - DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
    "\n",
    "    - DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
    "\n",
    "    - MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
    "\n",
    "    - MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
    "\n",
    "    - MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\n",
    "\n",
    "    - MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\n",
    "\n",
    "    - MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "    - MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
    "\n",
    "    - MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\n",
    "\n",
    "    - MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\n",
    "\n",
    "    - OFF_HEAP = StorageLevel(True, True, True, False, 1)\n",
    "\n",
    "3. The method `DataFrame.persist` returns itself,\n",
    "    which means that you can chain methods after it.\n",
    "\n",
    "2. Persist a DataFrame which is used multiple times and expensive to recompute.\n",
    "    Remembe to unpersist it too when the DataFrame is no longer needed. \n",
    "    Even Spark evict data from memory using the LRU (least recently used) strategy\n",
    "    when the caching layer becomes full,\n",
    "    it is still beneficial to unpersist data as soon as it is no used any more to reduce memory usage.\n",
    "\n",
    "3. Persisting too many DataFrames into memory can cause memory issues.\n",
    "    There are a few ways to address memory issues caused by this.\n",
    "    - Increase memory.\n",
    "    - Persist only the most reused DataFrames into memory.\n",
    "    - Persist other DataFrame into disk. \n",
    "    Generally speaking,\n",
    "\n",
    "1. The option `spark.history.store.maxDiskUsage`\n",
    "    controls the maximum disk usage for the local directory \n",
    "    where the cache application history information are stored.\n",
    "    The default is 10G.\n",
    "    You can set it to a larger value if you need to persist large DataFrames to disk. \n",
    "    \n",
    "2. `DataFrame.persist` is lazy\n",
    "    which means that Spark does not compute and persist a DataFrame immediately\n",
    "    but waits until an RDD action on the DataFrame. \n",
    "    This might have 2 undesired side effects.\n",
    "    \n",
    "    - Other operations/transformation might get optimized together into the execution plan of the DataFrame \n",
    "        which significantly hurts the performance of Spark. \n",
    "        Please refer to the \n",
    "        [Tips and Traps](https://www.legendu.net/en/blog/control-number-of-partitions-of-a-dataframe-in-spark/#Tips-and-Traps)\n",
    "        section of the article\n",
    "        [Control Number of Partitions of a DataFrame in Spark](https://www.legendu.net/en/blog/control-number-of-partitions-of-a-dataframe-in-spark/)\n",
    "        for more detailed discussion of such an example \n",
    "        (and ways to address the problem).\n",
    "    - Too large execution plan without eager caching/persist\n",
    "        might make a Spark application fail.\n",
    "        \n",
    "    If you'd like persist/cache a Spark DataFrame eagerly,\n",
    "    you can manually call the method `DataFrame.count`\n",
    "    (which is a RDD action)\n",
    "    after `DataFrame.persist` to trigger it.\n",
    "    Do NOT use the method `DataFrame.first` instead of `DataFrame.count` in this case,\n",
    "    as even though `DataFrame.first` is also a RDD action,\n",
    "    it will not trigger a full DataFrame persist/caching \n",
    "    but instead only the partition from which a row was retrieved. \n",
    "\n",
    "        :::python\n",
    "        df.cache().count()\n",
    "        \n",
    "3. I encountered a tricky issue with persist/cache\n",
    "    that it computes the DataFrame to be cached twice\n",
    "    (no matter which persist option is specified)\n",
    "    ,\n",
    "    sort of like the \n",
    "    [bug on checkpoint](https://issues.apache.org/jira/browse/SPARK-8582)\n",
    "    .\n",
    "    Personally speaking, \n",
    "    I have no idea whether it is a bug in the community version of Spark\n",
    "    or it is a bug in the enterprise version that is used at my workplace\n",
    "    as I don't see any issue on this raised against the community version of Spark.\n",
    "    \n",
    "4. Another tricky issue that I countered with persist/cache\n",
    "    is that for a Spark DataFrame with a large execution plan,\n",
    "    persist/cache fails to work \n",
    "    (no matter which persist option is used \n",
    "    and how large the option `spark.history.store.maxDiskUsage`\n",
    "    is set to)\n",
    "    .\n",
    "    Triggering an eager persist by calling `DataFrame.count`\n",
    "    immediate after `DataFrame.persist` often helps. \n",
    "    Replacing `DataFrame.persist` with `DataFrame.checkpoint` \n",
    "    also makes the application work.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Trap for Checkpoint\n",
    "\n",
    "1. You have to manually specify a checkpoint directory \n",
    "    if you'd like to use `DataFrame.checkpoint`. \n",
    "    \n",
    "        :::python\n",
    "        spark.sparkContext.setCheckpointDir(\"/hdfs/path/for/checkpoints\")\n",
    "    \n",
    "    Spark does not clean a checkpoint directory by default,\n",
    "    so that you can reuse a checkpoint directory \n",
    "    for recovering failed applications or speed up other applications sharing identical computations.\n",
    "    You can manually remove a checkpoint directory \n",
    "    if it is no longer needed\n",
    "    or you can set the following configuration\n",
    "    if you want Spark to auto clean the checkpoint directory of an application after it completes running.\n",
    "        \n",
    "        :::bash\n",
    "        --conf spark.cleaner.referenceTracking.cleanCheckpoints=true\n",
    "\n",
    "2. `DataFrame.checkpoints` has \n",
    "    [a bug](https://issues.apache.org/jira/browse/SPARK-8582)\n",
    "    which computes the DataFrame twice\n",
    "    before Spark 3.3.0.\n",
    "    It is more efficient to manually write a DataFrame to the disk\n",
    "    if you need to store the computed DataFrame on disk, \n",
    "    triage the lineage,\n",
    "    but don't really care about auto recovery of jobs.\n",
    "    If you don't need to triage the lineage either \n",
    "    (which is often NOT the case)\n",
    "    ,\n",
    "    you can just persist the DataFrame to disk\n",
    "    instead of using checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "\n",
    "findspark.init(\"/opt/spark-3.0.2-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark UDF\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   1|   b| bar| 4.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=(\n",
    "            (1, \"a\", \"foo\", 3.0),\n",
    "            (1, \"b\", \"bar\", 4.0),\n",
    "            (3, \"c\", \"foo\", 5.0),\n",
    "            (4, \"d\", \"bar\", 7.0),\n",
    "        ),\n",
    "        columns=(\"col1\", \"col2\", \"col3\", \"col4\"),\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persist `df` to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: bigint, col2: string, col3: string, col4: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that `df` has been persisted to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.storageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Detecting Cache-Related Bugs in Spark Applications](http://www.tcse.cn/~wsdou/papers/2020-issta-cachecheck.pdf)\n",
    "\n",
    "- https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "- https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "- https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html\n",
    "\n",
    "- https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/storage/StorageLevel.html\n",
    "\n",
    "- [Caching Spark DataFrame â€” How & When](https://medium.com/swlh/caching-spark-dataframe-how-when-79a8c13254c0)\n",
    "\n",
    "- [PySpark - StorageLevel](https://www.tutorialspoint.com/pyspark/pyspark_storagelevel.htm)\n",
    "\n",
    "- [Where is my sparkDF.persist(DISK_ONLY) data stored?](https://stackoverflow.com/questions/48430366/where-is-my-sparkdf-persistdisk-only-data-stored/48432130)\n",
    "\n",
    "- https://luminousmen.com/post/explaining-the-mechanics-of-spark-caching\n",
    "\n",
    "- https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34\n",
    "\n",
    "- https://luminousmen.com/post/spark-tips-caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
