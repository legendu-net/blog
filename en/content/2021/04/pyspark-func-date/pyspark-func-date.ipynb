{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2021-04-27 16:03:37\n",
    "- Title: Date Functions in Spark\n",
    "- Slug: pyspark-func-date\n",
    "- Category: Computer Science\n",
    "- Tags: programming, PySpark, Spark, DataFrame, date, Spark SQL, function, SQL\n",
    "- Modified: 2021-09-30 17:35:09\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. HDFS table might contain invalid data (I'm not clear about the reasons at this time) \n",
    "    with respct to the column types (e.g., Date and Timestamp). \n",
    "    This will cause issues when Spark tries to load the data.\n",
    "    For more discussions,\n",
    "    please refer to [Unrecognized column type:TIMESTAMP_TYP](https://network.informatica.com/thread/85564).\n",
    "\n",
    " \n",
    "2. `datetime.datetime` or `datetime.date` objects CANNOT be used in date functions in PySpark (e.g., `datediff`) directly.\n",
    "    You have to wrap them in the function `lit` which converts `datetime.datetime` and `datetime.date` objects\n",
    "    to Columns of `TimestampType` and `DateType` in PySpark DataFrames respectively.\n",
    "    As a matter of fact, \n",
    "    it is suggested that you always use the `lit` function to explicit convert scalar values to Columns\n",
    "    no matter implicit conversion might happen or not.\n",
    "   \n",
    "3. Most date functions work on a string of the format `yyyy-MM-dd`\n",
    "    which is automatically casted to a date object.\n",
    "    Notice that other date format (e.g., `yyyy/MM/dd`) are not supported \n",
    "    and will cause null values to be returned.\n",
    "    Note that the function `to_date` also support `yyyy-MM-dd` as the default type\n",
    "    when a format string is not specified.\n",
    "    \n",
    "2. Functions `second`, `minute`, `day`/`dayofmonth`, `weekofyear`, `monthofyear`, `quarter` \n",
    "    and `year` extract the corresponding part from a date object/string.\n",
    "    \n",
    "3. `date_add`, `date_sub`, `datediff` and `add_months` performs arithmatical operations on dates.\n",
    "\n",
    "4. `to_date`, `to_timestamp`, `to_utc_timestamp`, `to_unix_timestamp` and `timestamp` \n",
    "    cast date objects/strings.\n",
    "    \n",
    "6. The method `Column.between` casts its parameters to be of the same type as the Column\n",
    "    and then perform comparsisons.\n",
    "    For example, \n",
    "    if `Column.betwen` is invoked on a string column,\n",
    "    it automatically casts its parameters to be the string type;\n",
    "    if `Column.between` is invoked on a date column,\n",
    "    it automatically casts its parameters to be the date type.\n",
    "    If the cast fails for a row,\n",
    "    a `null` value is generated. \n",
    "    Specifically,\n",
    "    `Column.between` does NOT trying to convert a string to a Column automatically\n",
    "    no matter it is invoked on a date Column or not\n",
    "    (even though most date-related Column functions automatically convert strings to Columns when applies).\n",
    "    To avoid confusion and tricky bugs,\n",
    "    it is suggested that you avoid relying on the feature of auto converting string to Columns when calling date-related functions\n",
    "    but instead always explicitly convert a string to a column by calling `col(\"col_name\")`.\n",
    "\n",
    "8. `java.sql.Date` can be used with spark but does not support arithmatical computation.\n",
    "    `java.time` support arithmatical computation but cannot be used as a Spark column directly.\n",
    "    You have to rely on `java.time` for arithmatical computation and then convert dates to `java.sql.Date` \n",
    "    so that they can be used in Spark DataFrames."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"2017-01-01\", \"2017-01-07\"), (\"2020-02-01\", \"2019-02-10\")),\n",
    "        columns=[\"d1_s\", \"d2_s\"]\n",
    "    )\n",
    ")\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+\n",
      "|      d1_s|      d2_s|\n",
      "+----------+----------+\n",
      "|2017-01-01|2017-01-07|\n",
      "|2020-02-01|2019-02-10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python type `datetime.date` is mapped to `DateType` in PySpark. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df.withColumn(\"d3_d\", lit(datetime.date.today())).schema"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(d1_s,StringType,true),StructField(d2_s,StringType,true),StructField(d3_d,DateType,false)))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Python type `datetime.datetime` is mapped to `TimestampType` in PySpark."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df.withColumn(\"d3_ts\", lit(datetime.datetime.today())).schema"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(d1_s,StringType,true),StructField(d2_s,StringType,true),StructField(d3_ts,TimestampType,false)))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing a string column with a str works as expected."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "df.filter(col(\"d1_s\") >= \"2017-01-15\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|      d1_s|      d2_s|      d3_d|\n",
      "+----------+----------+----------+\n",
      "|2020-02-01|2019-02-10|2021-03-24|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A `datetime.date` object is formatted as YYYY-mm-dd by default when converted to a string.\n",
    "This is the behavior in most programming languages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "d = datetime.date.today()\n",
    "d"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "datetime.date(2021, 3, 24)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "str(d)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-03-24'"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparing a string column of the format `YYYY-mm-dd` with a datetime.date object works as expecte."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "df.filter(col(\"d1_s\") >= datetime.date(2017, 1, 15)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|      d1_s|      d2_s|      d3_d|\n",
      "+----------+----------+----------+\n",
      "|2020-02-01|2019-02-10|2021-03-24|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly the `between` method of a string `Column` works when 2 `datetime.date` objects are passed to it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "df.filter(col(\"d1_s\").between(datetime.date(2017, 1, 15), datetime.date(2021, 1,\n",
    "                                                                      15))).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|      d1_s|      d2_s|      d3_d|\n",
      "+----------+----------+----------+\n",
      "|2020-02-01|2019-02-10|2021-03-24|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method `between` of a string Column works even when mixed types of parameters are pass to it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "df.filter(col(\"d1_s\").between(\"2017-01-15\", datetime.date(2021, 1, 15))).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|      d1_s|      d2_s|      d3_d|\n",
      "+----------+----------+----------+\n",
      "|2020-02-01|2019-02-10|2021-03-24|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `between` method of a date Column converts arguments passed to it to date before doing comparsions.\n",
    "Non-valid arguments get converted to `null`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "df.filter(col(\"d3_d\").between(\"2017-01-15\", \"abc\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+----+----+\n",
      "|d1_s|d2_s|d3_d|\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "df.filter(col(\"d3_d\").astype(StringType()).between(\"2017-01-15\", \"abc\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|      d1_s|      d2_s|      d3_d|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|2021-03-24|\n",
      "|2020-02-01|2019-02-10|2021-03-24|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the `between` method a a string Column does not automatically convert a column name to a Column. \n",
    "As a matter of fact, \n",
    "it is suggested that you avoid relying on automatically conversion from column names to Columns\n",
    "as it can cause tricky issues if not careful."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "df.filter(col(\"d2_s\").between(\"d1_s\", \"d3_d\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+----+----+\n",
      "|d1_s|d2_s|d3_d|\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "df.filter(col(\"d2_s\").between(col(\"d1_s\"), col(\"d3_d\"))).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|      d1_s|      d2_s|      d3_d|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|2021-03-24|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function `datediff` does NOT work with a string `Column` and a `datetime.date` object directly.\n",
    "However, \n",
    "it works if you convert a datetime.date object to a `Column` using `lit`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "df.select(datediff(\"d1\", lit(datetime.date(2017, 1, 15)))).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------------------+\n",
      "|datediff(d1, DATE '2017-01-15')|\n",
      "+-------------------------------+\n",
      "|                            -14|\n",
      "|                             17|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## +/- Operators\n",
    "\n",
    "The +/- operators are supported on date/time columns in Spark 3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date as today\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2021-01-04|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date + 10 as today\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2021-01-14|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        current_date - 1 as today\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+\n",
      "|     today|\n",
      "+----------+\n",
      "|2021-01-03|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## add_months"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## date_add"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"2017-01-01\", \"2017-01-07\"), (\"2017-02-01\", \"2019-02-10\")),\n",
    "        columns=[\"d1\", \"d2\"]\n",
    "    )\n",
    ")\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+\n",
      "|        d1|        d2|\n",
      "+----------+----------+\n",
      "|2017-01-01|2017-01-07|\n",
      "|2017-02-01|2019-02-10|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df1 = df.withColumn(\"d3\", date_sub(\"d1\", 30)) \\\n",
    "    .withColumn(\"d4\", date_add(\"d1\", 30)) \\\n",
    "    .withColumn(\"check\", col(\"d2\").between(\"d3\", \"d4\"))\n",
    "df1.show()\n",
    "df1.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+----------+-----+\n",
      "|        d1|        d2|        d3|        d4|check|\n",
      "+----------+----------+----------+----------+-----+\n",
      "|2017-01-01|2017-01-07|2016-12-02|2017-01-31|false|\n",
      "|2017-02-01|2019-02-10|2017-01-02|2017-03-03|false|\n",
      "+----------+----------+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(d1,StringType,true),StructField(d2,StringType,true),StructField(d3,DateType,true),StructField(d4,DateType,true),StructField(check,BooleanType,true)))"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## date_trunc"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## date_sub"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## datediff"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df2 = df.withColumn(\"diff\", datediff(\"d2\", \"d1\"))\n",
    "df2.show()\n",
    "df2.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----+\n",
      "|        d1|        d2|diff|\n",
      "+----------+----------+----+\n",
      "|2017-01-01|2017-01-07|   6|\n",
      "|2017-02-01|2019-02-10| 739|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(d1,StringType,true),StructField(d2,StringType,true),StructField(diff,IntegerType,true)))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df2 = df.withColumn(\"diff\", datediff(\"d2\", datetime.date.today()))\n",
    "df2.show()\n",
    "df2.schema"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 2021-01-05 of type <class 'datetime.date'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-97e1d1306b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"diff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mdatediff\u001b[0;34m(end, start)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \"\"\"\n\u001b[1;32m   1113\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;34m\"Invalid argument, not a string or column: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: 2021-01-05 of type <class 'datetime.date'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df2 = df.withColumn(\"diff\", datediff(\"d2\", lit(datetime.date.today())))\n",
    "df2.show()\n",
    "df2.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+-----+\n",
      "|        d1|        d2| diff|\n",
      "+----------+----------+-----+\n",
      "|2017-01-01|2017-01-07|-1459|\n",
      "|2017-02-01|2019-02-10| -695|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(d1,StringType,true),StructField(d2,StringType,true),StructField(diff,IntegerType,true)))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select datediff('2020-12-04', '2020-12-03')\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------+\n",
      "|datediff(CAST(2020-12-04 AS DATE), CAST(2020-12-03 AS DATE))|\n",
      "+------------------------------------------------------------+\n",
      "|                                                           1|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select datediff('2020/12/04', '2020/12/03')\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------+\n",
      "|datediff(CAST(2020/12/04 AS DATE), CAST(2020/12/03 AS DATE))|\n",
      "+------------------------------------------------------------+\n",
      "|                                                        null|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## current_date"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "val df3 = df.withColumn(\"current\", current_date())\n",
    "df3.show\n",
    "df3.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2|   current|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|2018-05-02|\n",
      "|2017-02-01|2019-02-10|2018-05-02|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(current,DateType,false)]]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## current_timestamp / now\n",
    "\n",
    "Both `current_timestamp` and `now` returns the current timestamp.\n",
    "The difference is that `now` must be called with parentheses \n",
    "while `curent_timestamp` can be called without parentheses.\n",
    "If you are not sure, \n",
    "always call functions with parentheses."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        current_timestamp\n",
    "    \"\"\").show(n=1, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+\n",
      "|current_timestamp()    |\n",
      "+-----------------------+\n",
      "|2020-09-07 10:58:58.381|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        current_timestamp()\n",
    "    \"\"\").show(n=1, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+\n",
      "|current_timestamp()   |\n",
      "+----------------------+\n",
      "|2020-09-07 11:00:53.47|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        now()\n",
    "    \"\"\").show(n=1, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+\n",
      "|now()                  |\n",
      "+-----------------------+\n",
      "|2020-09-07 10:59:37.629|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dayofmonth / day\n",
    "\n",
    "Returns the day from a given date or timestamp. This function is the same as the day function.\n",
    "`dayofmonth` is the same as the `day` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "val df4 = df.withColumn(\"day_of_d2\", dayofmonth($\"d2\"))\n",
    "df4.show\n",
    "df4.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+---------+\n",
      "|        d1|        d2|day_of_d2|\n",
      "+----------+----------+---------+\n",
      "|2017-01-01|2017-01-07|        7|\n",
      "|2017-02-01|2019-02-10|       10|\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(day_of_d2,IntegerType,true)]]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        dayofmonth(\"2017-01-07\") \n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------+\n",
      "|dayofmonth(CAST(2017-01-07 AS DATE))|\n",
      "+------------------------------------+\n",
      "|                                   7|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dayofyear"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "val df5 = df.withColumn(\"day_of_year_d1\", dayofyear($\"d1\")).withColumn(\"day_of_year_d2\", dayofyear($\"d2\"))\n",
    "df5.show\n",
    "df5.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+--------------+--------------+\n",
      "|        d1|        d2|day_of_year_d1|day_of_year_d2|\n",
      "+----------+----------+--------------+--------------+\n",
      "|2017-01-01|2017-01-07|             1|             7|\n",
      "|2017-02-01|2019-02-10|            32|            41|\n",
      "+----------+----------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(day_of_year_d1,IntegerType,true), StructField(day_of_year_d2,IntegerType,true)]]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## date_format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "val df6 = df.withColumn(\"format_d1\", date_format($\"d1\", \"dd/MM/yyyy\"))\n",
    "df6.show\n",
    "df6.schema"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+----------+----------+\n",
      "|        d1|        d2| format_d1|\n",
      "+----------+----------+----------+\n",
      "|2017-01-01|2017-01-07|01/01/2017|\n",
      "|2017-02-01|2019-02-10|01/02/2017|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[StructField(d1,StringType,true), StructField(d2,StringType,true), StructField(format_d1,StringType,true)]]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## date_trunc\n",
    "\n",
    "Returns a timestamp specified as (ts) truncated to the unit specified by format (fmt) \n",
    "[\u201cYEAR\u201d, \u201cYYYY\u201d, \u201cYY\u201d, \u201cMON\u201d, \u201cMONTH\u201d, \u201cMM\u201d, \u201cDAY\u201d, \u201cDD\u201d, \u201cHOUR\u201d, \u201cMINUTE\u201d, \u201cSECOND\u201d, \u201cWEEK\u201d, \u201cQUARTER\u201d]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## minute"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## month"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        month(\"2018-01-01\") as month\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## now\n",
    "\n",
    "Returns the current timestamp."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## next_day\n",
    "\n",
    "Returns the day after the start_date specified by day_of_week. \n",
    "Day of week can be specified as \u2018MON\u2019, \u2018TUE\u2019, \u2018WED\u2019, \u2018THU\u2019, \u2018FRI\u2019, \u2018SAT\u2019, \u2018SUN\u2019 \n",
    "or as \u2018MO\u2019, \u2018TU\u2019, \u2018WE\u2019, \u2018TH\u2019, \u2018FR\u2019, \u2018SA\u2019, \u2018SU\u2019."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## quarter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## second"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## timestamp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## to_date"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select to_date('2020-12-04') \n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+\n",
      "|to_date('2020-12-04')|\n",
      "+---------------------+\n",
      "|           2020-12-04|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select to_date('2020/12/04') \n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+\n",
      "|to_date('2020/12/04')|\n",
      "+---------------------+\n",
      "|                 null|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select to_date('2020/12/04', 'yyyy/MM/dd') \n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------+\n",
      "|to_date('2020/12/04', 'yyyy/MM/dd')|\n",
      "+-----------------------------------+\n",
      "|                         2020-12-04|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## to_utc_timestamp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## to_unix_timestamp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## to_timestamp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## unix_timestamp"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## weekofyear"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## year"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "spark.sql(\"\"\"\n",
    "    select\n",
    "        year(\"2018-01-01\") as year\n",
    "    \"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|2018|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aggregation w.r.t a Date Column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=(\n",
    "            (1, \"2017-01-01\", 1, 2),\n",
    "            (1, \"2017-01-02\", 2, 3),\n",
    "            (1, \"2017-02-01\", 10, 11),\n",
    "            (1, \"2017-02-02\", 20, 21),\n",
    "            (2, \"2017-01-03\", 3, 4),\n",
    "            (2, \"2017-01-04\", 4, 5),\n",
    "            (2, \"2017-02-07\", 11, 12),\n",
    "            (2, \"2017-02-08\", 22, 23),\n",
    "        ),\n",
    "        columns=[\"user\", \"date\", \"x\", \"y\"]\n",
    "    )\n",
    ")\n",
    "df1.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+----------+---+---+\n",
      "|user|      date|  x|  y|\n",
      "+----+----------+---+---+\n",
      "|   1|2017-01-01|  1|  2|\n",
      "|   1|2017-01-02|  2|  3|\n",
      "|   1|2017-02-01| 10| 11|\n",
      "|   1|2017-02-02| 20| 21|\n",
      "|   2|2017-01-03|  3|  4|\n",
      "|   2|2017-01-04|  4|  5|\n",
      "|   2|2017-02-07| 11| 12|\n",
      "|   2|2017-02-08| 22| 23|\n",
      "+----+----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=(\n",
    "            (11, \"2017-01-01\", 10),\n",
    "            (11, \"2017-01-02\", 20),\n",
    "            (11, \"2017-02-01\", 100),\n",
    "            (11, \"2017-02-02\", 200),\n",
    "            (22, \"2017-01-03\", 30),\n",
    "            (22, \"2017-01-04\", 40),\n",
    "            (22, \"2017-02-07\", 110),\n",
    "            (22, \"2017-02-08\", 220),\n",
    "        ),\n",
    "        columns=[\"user\", \"date\", \"x\"]\n",
    "    )\n",
    ")\n",
    "df2.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+----------+---+\n",
      "|user|      date|  x|\n",
      "+----+----------+---+\n",
      "|  11|2017-01-01| 10|\n",
      "|  11|2017-01-02| 20|\n",
      "|  11|2017-02-01|100|\n",
      "|  11|2017-02-02|200|\n",
      "|  22|2017-01-03| 30|\n",
      "|  22|2017-01-04| 40|\n",
      "|  22|2017-02-07|110|\n",
      "|  22|2017-02-08|220|\n",
      "+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def sum_col_date(\n",
    "    col_agg: str, col_date: str, date: datetime.date, days_before_1: int,\n",
    "    days_before_2: int\n",
    "):\n",
    "    date1 = date - datetime.timedelta(days=days_before_1)\n",
    "    date2 = date - datetime.timedelta(days=days_before_2)\n",
    "    return sum(when(col(col_date).between(date1, date2), col(col_agg)).otherwise(0)\n",
    "              ).alias(f\"sum_{col_agg}_{days_before_1}_{days_before_2}\")\n",
    "\n",
    "\n",
    "def agg_col(col_agg, col_date, date, days_before_1, days_before_2):\n",
    "    return [\n",
    "        avg(col_agg).alias(f\"avg_{col_agg}\"),\n",
    "        sum_col_date(col_agg, \"date\", datetime.date(2017, 1, 5), 7, 1)\n",
    "    ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "df1.groupBy(\"user\").agg(sum_col_date(\"x\", \"date\", datetime.date(2017, 1, 5), 7,\n",
    "                                     1)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---------+\n",
      "|user|sum_x_7_1|\n",
      "+----+---------+\n",
      "|   1|        3|\n",
      "|   2|        7|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "df1.select(*[\"user\", \"date\"], col(\"x\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+----------+---+\n",
      "|user|      date|  x|\n",
      "+----+----------+---+\n",
      "|   1|2017-01-01|  1|\n",
      "|   1|2017-01-02|  2|\n",
      "|   1|2017-02-01| 10|\n",
      "|   1|2017-02-02| 20|\n",
      "|   2|2017-01-03|  3|\n",
      "|   2|2017-01-04|  4|\n",
      "|   2|2017-02-07| 11|\n",
      "|   2|2017-02-08| 22|\n",
      "+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "df2.groupBy(\"user\").agg(sum_col_date(\"x\", \"date\", datetime.date(2017, 1, 5), 7,\n",
    "                                     1)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---------+\n",
      "|user|sum_x_7_1|\n",
      "+----+---------+\n",
      "|  22|       70|\n",
      "|  11|       30|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "df1.groupBy(\"user\").agg(\n",
    "    *agg_col(\"x\", \"date\", datetime.date(2017, 1, 5), 7, 1),\n",
    "    *agg_col(\"y\", \"date\", datetime.date(2017, 1, 5), 7, 1)\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+---------+-----+---------+\n",
      "|user|avg_x|sum_x_7_1|avg_y|sum_y_7_1|\n",
      "+----+-----+---------+-----+---------+\n",
      "|   1| 8.25|        3| 9.25|        5|\n",
      "|   2| 10.0|        7| 11.0|        9|\n",
      "+----+-----+---------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[Spark Scala Functions](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html)\n",
    "\n",
    "[Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "[A Comprehensive Look at Dates and Timestamps in Apache Spark\u2122 3.0](https://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html)\n",
    "\n",
    "https://obstkel.com/spark-sql-functions\n",
    "\n",
    "https://obstkel.com/spark-sql-date-functions\n",
    "\n",
    "[Unrecognized column type:TIMESTAMP_TYP](https://network.informatica.com/thread/85564)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}