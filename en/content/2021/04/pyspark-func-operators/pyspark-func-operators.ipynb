{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- Title: Column Functions and Operators in Spark\n",
    "- Slug: pyspark-func-operators\n",
    "- Date: 2021-04-26 10:38:08\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, column, functions, operators, func, fun\n",
    "- Author: Ben Du\n",
    "- Modified: 2021-12-08 20:43:28\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from typing import List, Tuple\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "#findspark.init(\"/opt/spark-2.3.0-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, ArrayType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=[([1, 2], \"how\", 1), ([2, 3], \"are\", 2), ([3, 4], \"you\", 3)],\n",
    "        columns=[\"col1\", \"col2\", \"col3\"]\n",
    "    )\n",
    ")\n",
    "df.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+----+\n",
      "|  col1|col2|col3|\n",
      "+------+----+----+\n",
      "|[1, 2]| how|   1|\n",
      "|[2, 3]| are|   2|\n",
      "|[3, 4]| you|   3|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## [Boolean Operators and Functions](http://www.legendu.net/misc/blog/boolean-column-operators-and-functions-in-spark)\n",
    "\n",
    "Please refer to\n",
    "[Boolean Operators and Functions](http://www.legendu.net/misc/blog/boolean-column-operators-and-functions-in-spark)\n",
    "for details.\n",
    "\n",
    "## [Rounding Functions](http://www.legendu.net/misc/blog/pyspark-dataframe-func-rounding)\n",
    "\n",
    "Please refer to \n",
    "[Rounding Functions in Spark](http://www.legendu.net/misc/blog/pyspark-dataframe-func-rounding)\n",
    "for details.\n",
    "\n",
    "## [String Functions](http://www.legendu.net/misc/blog/pyspark-dataframe-func-string)\n",
    "\n",
    "Please refer to \n",
    "[String Functions in Spark](http://www.legendu.net/misc/blog/pyspark-dataframe-func-string)\n",
    "for details.\n",
    "\n",
    "## [Statistical Functions](http://www.legendu.net/misc/blog/pyspark-stat-functions)\n",
    "\n",
    "Please refer to\n",
    "[Statistical Functions in Spark](http://www.legendu.net/misc/blog/pyspark-stat-functions)\n",
    "for details.\n",
    "\n",
    "## [Date Functions in Spark](http://www.legendu.net/misc/blog/pyspark-dataframe-func-date)\n",
    "\n",
    "Please refer to \n",
    "[Date Functions in Spark](http://www.legendu.net/misc/blog/pyspark-dataframe-func-date)\n",
    "for details.\n",
    "\n",
    "## [Window Functions in Spark](http://www.legendu.net/misc/blog/window-functions-in-spark)\n",
    "\n",
    "Please refer to \n",
    "[Window Functions in Spark](http://www.legendu.net/misc/blog/window-functions-in-spark)\n",
    "for details.\n",
    "\n",
    "## [Collection Functions](http://www.legendu.net/misc/blog/pyspark-dataframe-func-collection)\n",
    "\n",
    "Please refer to\n",
    "[Collection Functions](http://www.legendu.net/misc/blog/pyspark-dataframe-func-collection)\n",
    "for details.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## between"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df.filter(col(\"col2\").between(\"hoa\", \"hox\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+----+\n",
      "|  col1|col2|col3|\n",
      "+------+----+----+\n",
      "|[1, 2]| how|   1|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df.filter(col(\"col3\").between(2, 3)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+----+\n",
      "|  col1|col2|col3|\n",
      "+------+----+----+\n",
      "|[2, 3]| are|   2|\n",
      "|[3, 4]| you|   3|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## cast"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df2 = df.select(\n",
    "    col(\"col1\"),\n",
    "    col(\"col2\"),\n",
    "    col(\"col3\").astype(StringType())\n",
    ")\n",
    "df2.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+----+\n",
      "|  col1|col2|col3|\n",
      "+------+----+----+\n",
      "|[1, 2]| how|   1|\n",
      "|[2, 3]| are|   2|\n",
      "|[3, 4]| you|   3|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df2.schema"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(col1,ArrayType(LongType,true),true),StructField(col2,StringType,true),StructField(col3,StringType,true)))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df3 = df2.select(\n",
    "    col(\"col1\"),\n",
    "    col(\"col2\"),\n",
    "    col(\"col3\").cast(IntegerType())\n",
    ")\n",
    "df3.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+----+\n",
      "|  col1|col2|col3|\n",
      "+------+----+----+\n",
      "|[1, 2]| how|   1|\n",
      "|[2, 3]| are|   2|\n",
      "|[3, 4]| you|   3|\n",
      "+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df3.schema"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType(List(StructField(col1,ArrayType(LongType,true),true),StructField(col2,StringType,true),StructField(col3,IntegerType,true)))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x = lit(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "type(x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## hash"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df.withColumn(\"hash_code\", hash(\"col2\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+----+----+-----------+\n",
      "|  col1|col2|col3|  hash_code|\n",
      "+------+----+----+-----------+\n",
      "|[1, 2]| how|   1|-1205091763|\n",
      "|[2, 3]| are|   2| -422146862|\n",
      "|[3, 4]| you|   3| -315368575|\n",
      "+------+----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## when\n",
    "\n",
    "1. `null` in when condition is considered as false."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = spark.read.json(\"../data/people.json\")\n",
    "df.show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "df = [age: bigint, name: string]\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[age: bigint, name: string]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`null` in when condition is considered as `false`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.select(when($\"age\" > 20, 1).otherwise(0).alias(\"gt20\")).show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n",
      "|gt20|\n",
      "+----+\n",
      "|   0|\n",
      "|   1|\n",
      "|   0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.select(when($\"age\" <= 20, 1).otherwise(0).alias(\"le20\")).show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n",
      "|le20|\n",
      "+----+\n",
      "|   0|\n",
      "|   0|\n",
      "|   1|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df.select(when($\"age\".isNull, 0).when($\"age\" > 20 , 100).otherwise(10).alias(\"age\")).show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  0|\n",
      "|100|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df.select(when($\"age\".isNull, 0).alias(\"age\")).show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|   0|\n",
      "|null|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "[Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "\n",
    "[Spark Scala Functions](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html)\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}