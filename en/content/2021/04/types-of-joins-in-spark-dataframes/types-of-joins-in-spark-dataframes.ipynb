{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2021-04-22 14:57:56\n",
    "- Title: Types of Joins of Spark DataFrames\n",
    "- Slug: spark-dataframe-types-joins\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, PySpark, DataFrame, join, type, inner join, outer join, left join, right join, full join, big data\n",
    "- Modified: 2021-03-22 14:57:56\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "1. It is suggested that you always pass a list of columns to the parameter `on` even if there's only one column for joining. \n",
    "\n",
    "2. `None` in a pandas DataFrame is converted to `NaN` instead of `null`!\n",
    "\n",
    "3. Spark allows using following join types: \n",
    "    - `inner` (default)\n",
    "    - `cross`\n",
    "    - `outer`\n",
    "    - `full`, `fullouter`, `full_outer`\n",
    "    - `left`, `leftouter`, `left_outer`\n",
    "    - `right`, `rightouter`, `right_outer`\n",
    "    - `semi`, `leftsemi`, `left_semi`\n",
    "    - `anti`, `leftanti`, `left_anti`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.1.1-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"Join\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+\n| last_name|depart_id|\n+----------+---------+\n|  Rafferty|     31.0|\n|     Jones|     33.0|\n|Heisenberg|     33.0|\n|  Robinson|     34.0|\n|     Smith|     34.0|\n|       Ben|     50.0|\n|  Williams|      NaN|\n+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "employees = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (\"Rafferty\", 31), (\"Jones\", 33), (\"Heisenberg\", 33), (\"Robinson\", 34),\n",
    "            (\"Smith\", 34), (\"Ben\", 50), (\"Williams\", None)\n",
    "        ],\n",
    "        columns=[\"last_name\", \"depart_id\"]\n",
    "    )\n",
    ")\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+-----------+\n|depart_id|depart_name|\n+---------+-----------+\n|       31|      Sales|\n|       33|Engineering|\n|       34|   Clerical|\n|       35|  Marketing|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "departments = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        [(31, \"Sales\"), (33, \"Engineering\"), (34, \"Clerical\"), (35, \"Marketing\")],\n",
    "        columns=[\"depart_id\", \"depart_name\"]\n",
    "    )\n",
    ")\n",
    "departments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|depart_id| last_name|depart_name|\n",
      "+---------+----------+-----------+\n",
      "|     34.0|  Robinson|   Clerical|\n",
      "|     34.0|     Smith|   Clerical|\n",
      "|     31.0|  Rafferty|      Sales|\n",
      "|     33.0|     Jones|Engineering|\n",
      "|     33.0|Heisenberg|Engineering|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|depart_id| last_name|depart_name|\n",
      "+---------+----------+-----------+\n",
      "|     34.0|  Robinson|   Clerical|\n",
      "|     34.0|     Smith|   Clerical|\n",
      "|     31.0|  Rafferty|      Sales|\n",
      "|     33.0|     Jones|Engineering|\n",
      "|     33.0|Heisenberg|Engineering|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, employees[\"depart_id\"] == departments[\"depart_id\"]).select(\n",
    "    employees[\"depart_id\"],\n",
    "    employees[\"last_name\"],\n",
    "    departments[\"depart_name\"],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "| last_name|depart_id|depart_name|\n",
      "+----------+---------+-----------+\n",
      "|  Robinson|     34.0|   Clerical|\n",
      "|     Smith|     34.0|   Clerical|\n",
      "|  Rafferty|     31.0|      Sales|\n",
      "|     Jones|     33.0|Engineering|\n",
      "|Heisenberg|     33.0|Engineering|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.alias(\"l\").join(\n",
    "    departments.alias(\"r\"), employees[\"depart_id\"] == departments[\"depart_id\"]\n",
    ").select(\n",
    "    \"l.last_name\",\n",
    "    \"l.depart_id\",\n",
    "    \"r.depart_name\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|last_name|depart_id|depart_name|\n",
      "+---------+---------+-----------+\n",
      "| Rafferty|     31.0|      Sales|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.alias(\"l\").join(\n",
    "    departments.alias(\"r\"), (employees[\"depart_id\"] == departments[\"depart_id\"]) &\n",
    "    (departments[\"depart_name\"] == \"Sales\")\n",
    ").select(\n",
    "    \"l.last_name\",\n",
    "    \"l.depart_id\",\n",
    "    \"r.depart_name\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "| last_name|depart_id|depart_name|\n",
      "+----------+---------+-----------+\n",
      "|  Williams|      NaN|       null|\n",
      "|  Robinson|     34.0|       null|\n",
      "|     Smith|     34.0|       null|\n",
      "|       Ben|     50.0|       null|\n",
      "|  Rafferty|     31.0|      Sales|\n",
      "|     Jones|     33.0|       null|\n",
      "|Heisenberg|     33.0|       null|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Outer Join\n",
    "\n",
    "1. If you specify a (list) column name(s) for the joining condition,\n",
    "    there will be no duplicated column names in the joining result.\n",
    "    The joining column from the left table is used by default.\n",
    "    However,\n",
    "    you can still refer to joining columns in the right table by specifying full column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Name(s) vs Column Expression(s) as the Joining Condition\n",
    "\n",
    "Similar to inner join,\n",
    "there's no duplicated/identical column names when you use a (list of) column name(s) for joining.\n",
    "However,\n",
    "duplicated columns happen if the joining columns in the 2 tables have the same name \n",
    "and a column expression is used as the joining condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|depart_id| last_name|depart_name|\n",
      "+---------+----------+-----------+\n",
      "|      NaN|  Williams|       null|\n",
      "|     34.0|  Robinson|   Clerical|\n",
      "|     34.0|     Smith|   Clerical|\n",
      "|     50.0|       Ben|       null|\n",
      "|     31.0|  Rafferty|      Sales|\n",
      "|     33.0|     Jones|Engineering|\n",
      "|     33.0|Heisenberg|Engineering|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, \"depart_id\", \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|depart_id| last_name|depart_name|\n",
      "+---------+----------+-----------+\n",
      "|      NaN|  Williams|       null|\n",
      "|     34.0|  Robinson|   Clerical|\n",
      "|     34.0|     Smith|   Clerical|\n",
      "|     50.0|       Ben|       null|\n",
      "|     31.0|  Rafferty|      Sales|\n",
      "|     33.0|     Jones|Engineering|\n",
      "|     33.0|Heisenberg|Engineering|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------+\n",
      "| last_name|depart_id|depart_id|depart_name|\n",
      "+----------+---------+---------+-----------+\n",
      "|  Williams|      NaN|     null|       null|\n",
      "|  Robinson|     34.0|       34|   Clerical|\n",
      "|     Smith|     34.0|       34|   Clerical|\n",
      "|       Ben|     50.0|     null|       null|\n",
      "|  Rafferty|     31.0|       31|      Sales|\n",
      "|     Jones|     33.0|       33|Engineering|\n",
      "|Heisenberg|     33.0|       33|Engineering|\n",
      "+----------+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(\n",
    "    departments, employees[\"depart_id\"] == departments[\"depart_id\"], \"left_outer\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you specify a (list) column name(s) for the joining condition,\n",
    "there will be no duplicated column names in the joining result.\n",
    "The joining column from the left table is used by default.\n",
    "However,\n",
    "you can still refer to joining columns in the right table \n",
    "(by specifying full column names)\n",
    "before an action happens.\n",
    "Taking the following left join as illustration,\n",
    "the `DataFrame.filter` is a transform (not an action),\n",
    "so you can still refer to the joining column (`departments[\"depart_id\"]`) in the right table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|depart_id|last_name|depart_name|\n",
      "+---------+---------+-----------+\n",
      "|      NaN| Williams|       null|\n",
      "|     50.0|      Ben|       null|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"],\n",
    "               \"left_outer\").filter(departments[\"depart_id\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, \n",
    "`DataFrame.withColumn` is an action \n",
    "which means that at the time the `withColumn` operation is executed,\n",
    "previous operations on the DataFrame has been executed\n",
    "which means that the column `departments[\"depart_id\"]` is gone \n",
    "so you couldn't access it any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) depart_id#39L missing from depart_id#27,last_name#26,depart_name#40 in operator !Project [depart_id#27, last_name#26, depart_name#40, isnotnull(depart_id#39L) AS has_depart#414]. Attribute(s) with the same name appear in the operation: depart_id. Please check if the right attribute(s) are used.;\n!Project [depart_id#27, last_name#26, depart_name#40, isnotnull(depart_id#39L) AS has_depart#414]\n+- Project [depart_id#27, last_name#26, depart_name#40]\n   +- Join LeftOuter, (depart_id#27 = cast(depart_id#39L as double))\n      :- LogicalRDD [last_name#26, depart_id#27], false\n      +- LogicalRDD [depart_id#39L, depart_name#40], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d4acbe0a8957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memployees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepartments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"depart_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"has_depart\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepartments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"depart_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) depart_id#39L missing from depart_id#27,last_name#26,depart_name#40 in operator !Project [depart_id#27, last_name#26, depart_name#40, isnotnull(depart_id#39L) AS has_depart#414]. Attribute(s) with the same name appear in the operation: depart_id. Please check if the right attribute(s) are used.;\n!Project [depart_id#27, last_name#26, depart_name#40, isnotnull(depart_id#39L) AS has_depart#414]\n+- Project [depart_id#27, last_name#26, depart_name#40]\n   +- Join LeftOuter, (depart_id#27 = cast(depart_id#39L as double))\n      :- LogicalRDD [last_name#26, depart_id#27], false\n      +- LogicalRDD [depart_id#39L, depart_name#40], false\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\n",
    "    \"depart_id\"\n",
    "], \"left_outer\").withColumn(\"has_depart\", departments[\"depart_id\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Resolved attribute(s) depart_id#39L missing from depart_id#27,last_name#26,depart_name#40 in operator !Project [isnotnull(depart_id#39L) AS has_depart#517]. Attribute(s) with the same name appear in the operation: depart_id. Please check if the right attribute(s) are used.;\n!Project [isnotnull(depart_id#39L) AS has_depart#517]\n+- Project [depart_id#27, last_name#26, depart_name#40]\n   +- Join LeftOuter, (depart_id#27 = cast(depart_id#39L as double))\n      :- LogicalRDD [last_name#26, depart_id#27], false\n      +- LogicalRDD [depart_id#39L, depart_name#40], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-15381a8f7b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m employees.join(departments, [\"depart_id\"], \"left_outer\").select(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdepartments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"depart_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"has_depart\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ).show()\n",
      "\u001b[0;32m/opt/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Resolved attribute(s) depart_id#39L missing from depart_id#27,last_name#26,depart_name#40 in operator !Project [isnotnull(depart_id#39L) AS has_depart#517]. Attribute(s) with the same name appear in the operation: depart_id. Please check if the right attribute(s) are used.;\n!Project [isnotnull(depart_id#39L) AS has_depart#517]\n+- Project [depart_id#27, last_name#26, depart_name#40]\n   +- Join LeftOuter, (depart_id#27 = cast(depart_id#39L as double))\n      :- LogicalRDD [last_name#26, depart_id#27], false\n      +- LogicalRDD [depart_id#39L, depart_name#40], false\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"], \"left_outer\").select(\n",
    "    departments[\"depart_id\"].isNotNull().alias(\"has_depart\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course use a column expression as the joining condition \n",
    "which will keep the joining column(s) from both tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------+----------+\n",
      "| last_name|depart_id|depart_id|depart_name|has_depart|\n",
      "+----------+---------+---------+-----------+----------+\n",
      "|  Williams|      NaN|     null|       null|     false|\n",
      "|  Robinson|     34.0|       34|   Clerical|      true|\n",
      "|     Smith|     34.0|       34|   Clerical|      true|\n",
      "|       Ben|     50.0|     null|       null|     false|\n",
      "|  Rafferty|     31.0|       31|      Sales|      true|\n",
      "|     Jones|     33.0|       33|Engineering|      true|\n",
      "|Heisenberg|     33.0|       33|Engineering|      true|\n",
      "+----------+---------+---------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(\n",
    "    departments, employees[\"depart_id\"] == departments[\"depart_id\"], \"left_outer\"\n",
    ").withColumn(\"has_depart\", departments[\"depart_id\"].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of left join with a complicated joining condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+-----------+\n| last_name|depart_id|depart_name|\n+----------+---------+-----------+\n|  Williams|      NaN|       null|\n|  Robinson|     34.0|       null|\n|     Smith|     34.0|       null|\n|       Ben|     50.0|       null|\n|  Rafferty|     31.0|      Sales|\n|     Jones|     33.0|       null|\n|Heisenberg|     33.0|       null|\n+----------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employees.alias(\"l\").join(\n",
    "    departments.alias(\"r\"), (employees[\"depart_id\"] == departments[\"depart_id\"]) &\n",
    "    (departments[\"depart_name\"] == \"Sales\"), \"left_outer\"\n",
    ").select(\n",
    "    \"l.last_name\",\n",
    "    \"l.depart_id\",\n",
    "    \"r.depart_name\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+-----------+\n| last_name|depart_id|depart_name|\n+----------+---------+-----------+\n|  Williams|      NaN|       null|\n|  Robinson|     34.0|       null|\n|     Smith|     34.0|       null|\n|       Ben|     50.0|       null|\n|  Rafferty|     31.0|      Sales|\n|     Jones|     33.0|       null|\n|Heisenberg|     33.0|       null|\n+----------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employees.alias(\"l\").join(\n",
    "    departments.alias(\"r\"), (col(\"l.depart_id\") == col(\"r.depart_id\")) &\n",
    "    (col(\"r.depart_name\") == \"Sales\"), \"left_outer\"\n",
    ").select(\n",
    "    \"l.last_name\",\n",
    "    \"l.depart_id\",\n",
    "    \"r.depart_name\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+-----------+\n| last_name|depart_id|depart_name|\n+----------+---------+-----------+\n|  Williams|      NaN|       null|\n|  Robinson|     34.0|       null|\n|     Smith|     34.0|       null|\n|       Ben|     50.0|       null|\n|  Rafferty|     31.0|      Sales|\n|     Jones|     33.0|       null|\n|Heisenberg|     33.0|       null|\n+----------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "employees.alias(\"l\").join(\n",
    "    departments.alias(\"r\"), (col(\"l.depart_id\") == col(\"r.depart_id\")) &\n",
    "    (col(\"r.depart_name\") == \"Sales\"), \"left_outer\"\n",
    ").drop(\n",
    "    col(\"r.depart_id\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Outer Join\n",
    "\n",
    "Symmetric to left out join. \n",
    "Please refer to left out join above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|depart_id| last_name|depart_name|\n",
      "+---------+----------+-----------+\n",
      "|       35|      null|  Marketing|\n",
      "|       34|  Robinson|   Clerical|\n",
      "|       34|     Smith|   Clerical|\n",
      "|       31|  Rafferty|      Sales|\n",
      "|       33|     Jones|Engineering|\n",
      "|       33|Heisenberg|Engineering|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"], \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|depart_id| last_name|depart_name|\n",
      "+---------+----------+-----------+\n",
      "|      NaN|  Williams|       null|\n",
      "|     35.0|      null|  Marketing|\n",
      "|     34.0|  Robinson|   Clerical|\n",
      "|     34.0|     Smith|   Clerical|\n",
      "|     50.0|       Ben|       null|\n",
      "|     31.0|  Rafferty|      Sales|\n",
      "|     33.0|     Jones|Engineering|\n",
      "|     33.0|Heisenberg|Engineering|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"], \"full_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "| last_name|depart_id|\n",
      "+----------+---------+\n",
      "|  Rafferty|     31.0|\n",
      "|     Jones|     33.0|\n",
      "|Heisenberg|     33.0|\n",
      "|  Robinson|     34.0|\n",
      "|     Smith|     34.0|\n",
      "|       Ben|     50.0|\n",
      "|  Williams|      NaN|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|depart_id|depart_name|\n",
      "+---------+-----------+\n",
      "|       31|      Sales|\n",
      "|       33|Engineering|\n",
      "|       34|   Clerical|\n",
      "|       35|  Marketing|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|depart_id|last_name|depart_name|\n",
      "+---------+---------+-----------+\n",
      "|      NaN| Williams|       null|\n",
      "|     50.0|      Ben|       null|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments, [\"depart_id\"],\n",
    "               \"left_outer\").filter(departments[\"depart_id\"].isNull()).show()"
   ]
  },
  {
   "source": [
    "## A $\\triangle$ B (Symmetric Difference)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|depart_id|last_name|depart_name|\n",
      "+---------+---------+-----------+\n",
      "|      NaN| Williams|       null|\n",
      "|     35.0|     null|  Marketing|\n",
      "|     50.0|      Ben|       null|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(\n",
    "    departments, [\"depart_id\"], \"full_outer\"\n",
    ").filter(employees[\"depart_id\"].isNull() | departments[\"depart_id\"].isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartesian Join\n",
    "\n",
    "Notice that you have to have \"spark.sql.crossJoin.enabled\" set to `true` \n",
    "in order to perform cartesian join on 2 DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+-----------+\n",
      "| last_name|depart_id|depart_id|depart_name|\n",
      "+----------+---------+---------+-----------+\n",
      "|  Rafferty|     31.0|       31|      Sales|\n",
      "|  Rafferty|     31.0|       33|Engineering|\n",
      "|  Rafferty|     31.0|       34|   Clerical|\n",
      "|  Rafferty|     31.0|       35|  Marketing|\n",
      "|     Jones|     33.0|       31|      Sales|\n",
      "|     Jones|     33.0|       33|Engineering|\n",
      "|     Jones|     33.0|       34|   Clerical|\n",
      "|     Jones|     33.0|       35|  Marketing|\n",
      "|Heisenberg|     33.0|       31|      Sales|\n",
      "|Heisenberg|     33.0|       33|Engineering|\n",
      "|Heisenberg|     33.0|       34|   Clerical|\n",
      "|Heisenberg|     33.0|       35|  Marketing|\n",
      "|  Robinson|     34.0|       31|      Sales|\n",
      "|  Robinson|     34.0|       33|Engineering|\n",
      "|  Robinson|     34.0|       34|   Clerical|\n",
      "|  Robinson|     34.0|       35|  Marketing|\n",
      "|     Smith|     34.0|       31|      Sales|\n",
      "|     Smith|     34.0|       33|Engineering|\n",
      "|     Smith|     34.0|       34|   Clerical|\n",
      "|     Smith|     34.0|       35|  Marketing|\n",
      "+----------+---------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.join(departments).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+-----+\n",
      "| name| startDate|   endDate|price|\n",
      "+-----+----------+----------+-----+\n",
      "|steak|1990-01-01|2000-01-01|  150|\n",
      "|steak|2000-01-02|2020-01-01|  180|\n",
      "| fish|1990-01-01|2020-01-01|  100|\n",
      "+-----+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=(\n",
    "            (\"steak\", \"1990-01-01\", \"2000-01-01\",\n",
    "             150), (\"steak\", \"2000-01-02\", \"2020-01-01\", 180),\n",
    "            (\"fish\", \"1990-01-01\", \"2020-01-01\", 100)\n",
    "        ),\n",
    "        columns=(\"name\", \"startDate\", \"endDate\", \"price\")\n",
    "    )\n",
    ")\n",
    "products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      date|product|\n",
      "+----------+-------+\n",
      "|1995-01-01|  steak|\n",
      "|2000-01-01|   fish|\n",
      "|2005-01-01|  steak|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=((\"1995-01-01\", \"steak\"), (\"2000-01-01\", \"fish\"), (\"2005-01-01\", \"steak\")),\n",
    "        columns=(\"date\", \"product\")\n",
    "    )\n",
    ")\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----------+----------+-----+\n",
      "|      date|product| name| startDate|   endDate|price|\n",
      "+----------+-------+-----+----------+----------+-----+\n",
      "|1995-01-01|  steak|steak|1990-01-01|2000-01-01|  150|\n",
      "|2005-01-01|  steak|steak|2000-01-02|2020-01-01|  180|\n",
      "|2000-01-01|   fish| fish|1990-01-01|2020-01-01|  100|\n",
      "+----------+-------+-----+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.join(\n",
    "    products, (orders[\"product\"] == products[\"name\"]) &\n",
    "    orders[\"date\"].between(products[\"startDate\"], products[\"endDate\"])\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## References \n",
    "\n",
    "http://kirillpavlov.com/blog/2016/04/23/beyond-traditional-join-with-apache-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}