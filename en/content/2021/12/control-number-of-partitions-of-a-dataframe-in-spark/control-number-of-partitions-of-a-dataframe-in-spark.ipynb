{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Benjamin Du\n",
    "- Date: 2021-12-11 17:21:56\n",
    "- Modified: 2022-01-18 14:34:12\n",
    "- Title: Control Number of Partitions of a DataFrame in Spark\n",
    "- Slug: control-number-of-partitions-of-a-dataframe-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, programming, Spark, PySpark, big data, partition, repartition, maxPartitionBytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. `DataFrame.repartition` repartitions the DataFrame by **hash code** of each row. \n",
    "    If you specify a (multiple) column(s) (instead of number of partitions) \n",
    "    to the method `DataFrame.repartition`,\n",
    "    then hash code of the column(s) are calculated for repartition. \n",
    "    In some situations,\n",
    "    there are lots of hash conflictions \n",
    "    even if the total number of rows is small (e.g., a few thousand),\n",
    "    which means that\n",
    "    <span style=\"color:red\"> partitions generated might be skewed </span>.\n",
    "    and causes a few long-running tasks. \n",
    "    If this ever happens, \n",
    "    it is suggested that you manually add a column\n",
    "    which helps the hashing algoirthm. \n",
    "    Notice that *an existing integer column with distinct values in the DataFrame \n",
    "    is not necessarily a good column to repartition by* \n",
    "    especially when those integers are big (e.g., u64)\n",
    "    as hash code of those integers can easily conflicts. \n",
    "    It is best to add a column of random numbers \n",
    "    or a column of manually curated partition indexes\n",
    "    and ask Spark to repartition based on that column.\n",
    "    \n",
    "2. When loading files into a DataFrame,\n",
    "    Spark controls the size of each partition of the DataFrame \n",
    "    through the parameter `spark.sql.files.maxPartitionBytes` (128M by default).\n",
    "    If a file has a size larger than `spark.sql.files.maxPartitionBytes`,\n",
    "    it is splitted evenly into multiple smaller blocks \n",
    "    (whose sizes are less than or equal to 128M)\n",
    "    and each block is loaded into one partition of the DataFrame.\n",
    "    If file sizes are small,\n",
    "    Spark loads as many as possible files into one partition\n",
    "    with the total size of files less than or equal to 128M.\n",
    "    Generally speaking,\n",
    "    you want to keep the default value for `spark.sql.files.maxPartitionBytes` \n",
    "    as it yields good performance for Spark applications which are data intensive\n",
    "    (typical situation of Spark applications).\n",
    "    However,\n",
    "    if your Spark application is CPU intensive,\n",
    "    it makes more sense to set a much smaller value for `spark.sql.files.maxPartitionBytes`\n",
    "    so that there are more partitions generated and yield a higher level of parallelism.\n",
    "    You can, \n",
    "    of course,\n",
    "    repartition a DataFrame manually,\n",
    "    but it is more expensive to do so\n",
    "    and requires you to have access to the source code.\n",
    "    If you do play with `spark.sql.files.maxPartitionBytes`\n",
    "    to increase the number of partitions of a loaded DataFrame,\n",
    "    be aware that the final output DataFrame (after computation) \n",
    "    might also have a large number of partitions.\n",
    "    It is not a good idea to write lots of small files into the Hadoop filesystem \n",
    "    as it not only hurst the performance of the Hadoop filesystem\n",
    "    but might also exceed the namespace quota limitation.\n",
    "    In this case,\n",
    "    you want to reduce the number of partitions of the output DataFrame\n",
    "    before writing it into disk.\n",
    "    The rule of thumb is to make each partition file on disk has a size of 64M-128M.\n",
    "    There are a few ways to achieve this.\n",
    "\n",
    "    - Manually repartition the output DataFrame to reduce the number of partitions.\n",
    "    - Manually coalesce the output DataFrame to reduce the number of partitions.\n",
    "    \n",
    "    Manually repartition the output DataFrame is easy to carry out\n",
    "    but it causes a full data shuffling \n",
    "    which might be expensive.\n",
    "    Manually coalescing the output DataFrame (to reduce the number of partitions)\n",
    "    is less expensive\n",
    "    but it has a pitfall.\n",
    "    Spark optimizes the physical plan \n",
    "    and might reduce the number of partitions before computation of the DataFrame.\n",
    "    This is undesirable if you want to have a large number of partitions \n",
    "    to increase parallelism when computing the DataFrame\n",
    "    and reduce the number of partitions when outputing the DataFrame. \n",
    "    There are a few ways to solve this problem.\n",
    "\n",
    "    - Checkpoint the DataFrame before coalescing.\n",
    "    - Cache the DataFrame and trigger a RDD `count` action \n",
    "        (unlike checkpoint, caching itself does not trigger a RDD action) \n",
    "        before coalescing.\n",
    "\n",
    "    Generally speaking,\n",
    "    caching + triggering a RDD action has a better performance than checkpoint \n",
    "    but checkpoint is more robust (to noisy Spark cluster).\n",
    "    You can also manually output the DataFrame (before coalesing),\n",
    "    read it back,\n",
    "    and then coalesce (to reduce the number of partitions)\n",
    "    and output it.\n",
    "    It is equivalent to caching to disk and then trigger a RDD action,\n",
    "    theoretically speaking.\n",
    "    However,\n",
    "    in pratice I've enounctered performance issues with both \n",
    "    `checkpoint` \n",
    "    (due to the \n",
    "    [bug](https://issues.apache.org/jira/browse/SPARK-8582)\n",
    "    ) \n",
    "    and `cache` + triggering a RDD action\n",
    "    (First, \n",
    "    computing twice similar to the bug of checkpoint; \n",
    "    Second, \n",
    "    unstable and fails often even with when persisting to disk\n",
    "    ). \n",
    "    On the contrary, \n",
    "    manually writing the DataFrame,\n",
    "    read it back,\n",
    "    and then coalesce and output it works well.\n",
    "    Please refer to \n",
    "    [repart_hdfs](https://github.com/dclong/dsutil/blob/dev/dsutil/hadoop/utils.py#L78)\n",
    "    for a reference implementation.\n",
    "    For more discussion on cache/persist vs checkpoint,\n",
    "    please refer to\n",
    "    [Persist and Checkpoint DataFrames in Spark](http://www.legendu.net/en/blog/spark-persist-checkpoint-dataframe)\n",
    "    .\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- [Partition and Bucketing in Spark](http://www.legendu.net/misc/blog/partition-bucketing-in-spark/)\n",
    "\n",
    "- [Coalesce and Repartition in Spark DataFrame](http://www.legendu.net/misc/blog/spark-dataframe-coalesce-repartition/)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
