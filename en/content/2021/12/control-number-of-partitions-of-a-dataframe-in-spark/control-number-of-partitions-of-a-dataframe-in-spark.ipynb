{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Benjamin Du\n",
    "- Date: 2021-12-11 17:21:56\n",
    "- Modified: 2022-01-18 14:34:12\n",
    "- Title: Control Number of Partitions of a DataFrame in Spark\n",
    "- Slug: control-number-of-partitions-of-a-dataframe-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, programming, Spark, PySpark, big data, partition, repartition, maxPartitionBytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. `DataFrame.repartition` repartitions the DataFrame by **hash code** of each row. \n",
    "    If you specify a (multiple) column(s) (instead of number of partitions) \n",
    "    to the method `DataFrame.repartition`,\n",
    "    then hash code of the column(s) are calculated for repartition. \n",
    "    In some situations,\n",
    "    there are lots of hash conflictions \n",
    "    even if the total number of rows is small (e.g., a few thousand),\n",
    "    which means that\n",
    "    <span style=\"color:red\"> partitions generated might be skewed </span>.\n",
    "    and causes a few long-running tasks. \n",
    "    If this ever happens, \n",
    "    it is suggested that you manually add a column\n",
    "    which helps the hashing algoirthm. \n",
    "    Notice that *an existing integer column with distinct values in the DataFrame \n",
    "    is not necessarily a good column to repartition by* \n",
    "    especially when those integers are big (e.g., u64)\n",
    "    as hash code of those integers can easily conflicts. \n",
    "    It is best to add a column of random numbers \n",
    "    of a column of manually curated partition indexes\n",
    "    and ask Spark to repartition based on that column.\n",
    "    \n",
    "2. By default, \n",
    "    Spark automatically merges several small files into one partition \n",
    "    when loading a HDFS table into a DataFrame. \n",
    "    The behavior is control by the parameter `spark.sql.files.maxPartitionBytes`.\n",
    "    The default value for this option is 128M\n",
    "    which means that Spark keeps reading small files into one partition of a DataFrame\n",
    "    until reading another file makes the size of the partition exceedes 128M. \n",
    "    Generally speaking,\n",
    "    you want to keep the default value for this setting \n",
    "    as it yields optimal performance for handling large data table. \n",
    "    However,\n",
    "    if your Spark application deals with small data but is CPU intensive,\n",
    "    it makes more sense to set a much smaller value for `spark.sql.files.maxPartitionBytes`\n",
    "    so that there are more partitions generated and yield a higher level of parallelism.\n",
    "    Of course,\n",
    "    you can always repartition a DataFrame manually. \n",
    "\n",
    "    However, \n",
    "    there's a minor drawback if you manually set `spark.sql.files.maxPartitionBytes`\n",
    "    to a much smaller value (than the default one).\n",
    "    The Hadoop file system does not work well with a large number of small files\n",
    "    Having Spark/PySpark applications generating huge amount of small files\n",
    "    not only hurt the performance of the Hadoop file system\n",
    "    but might also exceed the namespace quota limitation \n",
    "    (of the directory that your application write data to). \n",
    "    A good practice in this situation is to manually reduce the number of partitions \n",
    "    after your Spark/PySpark application finish computing.\n",
    "    There are a few ways to achieve this.\n",
    "\n",
    "    - Manually repartition the output DataFrame to reduce the number of partitions.\n",
    "    - Manually coalesce the output DataFrame to reduce the number of partitions.\n",
    "    \n",
    "    Manually repartition the output DataFrame is easy to carry out\n",
    "    but it causes a full data shuffling \n",
    "    which might be expensive.\n",
    "    Manually coalescing the output DataFrame (to reduce the number of partitions)\n",
    "    is less expensive\n",
    "    but it has a pitfall.\n",
    "    Spark optimizes the physical plan \n",
    "    and might reduce the number of partitions before computation of the DataFrame.\n",
    "    This is undesirable if you want to have a large number of partitions \n",
    "    to increase parallelism when computing the DataFrame\n",
    "    and reduce the number of partitions when outputing the DataFrame. \n",
    "    There are a few ways to solve this problem.\n",
    "\n",
    "    - Checkpoint the DataFrame before coalescing.\n",
    "    - Cache the DataFrame and trigger a RDD `count` action \n",
    "        (unlike checkpoint, caching itself does not trigger a RDD action) \n",
    "        before coalescing.\n",
    "\n",
    "    Generally speaking,\n",
    "    caching + triggering a RDD action has a better performance than checkpoint \n",
    "    but checkpoint is more robust (to noisy Spark cluster).\n",
    "    You can also manually output the DataFrame (before coalesing),\n",
    "    read it back,\n",
    "    and then coalesce (to reduce the number of partitions)\n",
    "    and output it.\n",
    "    It is equivalent to caching to disk and then trigger a RDD action.  \n",
    "    Please refer to \n",
    "    [repart_hdfs](https://github.com/dclong/dsutil/blob/dev/dsutil/hadoop/utils.py#L78)\n",
    "    for such an example.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- [Partition and Bucketing in Spark](http://www.legendu.net/misc/blog/partition-bucketing-in-spark/)\n",
    "\n",
    "- [Coalesce and Repartition in Spark DataFrame](http://www.legendu.net/misc/blog/spark-dataframe-coalesce-repartition/)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
