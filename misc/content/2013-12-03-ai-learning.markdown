Status: published
Date: 2020-01-01 13:20:19
Title: AI Learning
Slug: ai-learning
Author: Ben Du
Category: AI
Tags: AI, machine learning, statistics, data science

**
Things on this page are fragmentary and immature notes/thoughts of the author.
It is not meant to readers but rather for convenient reference of the author and future improvement.
**

![](https://jixta.files.wordpress.com/2015/11/machinelearningalgorithms.png)
The picture comes from [Machine Learning Algorithms Mindmap](https://jixta.wordpress.com/2015/07/17/machine-learning-algorithms-mindmap/).

Open Neural Network Exchange (ONNX)

## Feature Engineering

[Handling Categorical Variables in Machine Learning](http://www.legendu.net/misc/blog/handling-categorical-variables-in-machine-learning/)

## [Regularization in Machine Learning Models](http://www.legendu.net/misc/blog/regularization-in-machine-learning-models/)

## [Ensemble](http://www.legendu.net/misc/blog/ai-ensemble/)

## Frameworks

[Libraries for Gradient Boosting](http://www.legendu.net/misc/blog/libraries-for-gradient-boosting/)



### Big-data (Spark) Friendly Frameworks

https://mmlspark.blob.core.windows.net/website/index.html

## [AutoML](http://www.legendu.net/misc/blog/automl-tips/)


## Questions

### Random Forest

1. Is discrete variables easier to handle than continous variables (in random forest)?
    Is there any advantage of discretize variables?
    The eseential question is how is categorical varialbes handled in RF?
    Does RF use category variables directly or does it have to convert it to numerical somehow?

2. Random forest has a way to impute missing values.
    What if I treat missing values in categorical predictors and a new class?
    It sounds like a good ...

## Imputation

1. mean, median, etc.

1. SVD imputation using low dimension to approximate high dimension data

## [Tips on Kaggle](http://www.legendu.net/misc/blog/tips-on-kaggle/)

## [Machine Learning Resources](http://www.legendu.net/misc/blog/machine-learning-resources/)
