{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Read/Write Parquet Files in Spark\n",
    "- Slug: spark-io-parquet\n",
    "- Date: 2019-11-26\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, Parquet\n",
    "- Author: Ben Du\n",
    "- Modified: 2019-11-26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader) \n",
    "APIs\n",
    "\n",
    "[DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)\n",
    "APIs\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54471567-a823-4f3b-b94e-7c7427578a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.3.1\n",
    "org.apache.spark spark-sql_2.11 2.3.1\n",
    "org.apache.spark spark-hive_2.11 2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "1. `.load` is a general method for reading data in different format. \n",
    "    You have to specify the format of the data via the method `.format` of course.\n",
    "    `.csv` (both for CSV and TSV), `.json` and `.parquet` are specializations of `.load`. \n",
    "    `.format` is optional if you use a specific loading function (csv, json, etc.).\n",
    "\n",
    "2. No header by default.\n",
    "\n",
    "3. `.coalesece(1)` or `repartition(1)` if you want to write to only 1 file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data in Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@5d88dad5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().master(\"local\")\n",
    "    .appName(\"IO\")\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+---------+-------+-------+------+------+----+--------+--------+----+---+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|cancelled|carrier|tailnum|flight|origin|dest|air_time|distance|hour|min|\n",
      "+----+-----+---+--------+---------+--------+---------+---------+-------+-------+------+------+----+--------+--------+----+---+\n",
      "|2014|    1|  1|     914|       14|    1238|       13|        0|     AA| N338AA|     1|   JFK| LAX|     359|    2475|   9| 14|\n",
      "|2014|    1|  1|    1157|       -3|    1523|       13|        0|     AA| N335AA|     3|   JFK| LAX|     363|    2475|  11| 57|\n",
      "|2014|    1|  1|    1902|        2|    2224|        9|        0|     AA| N327AA|    21|   JFK| LAX|     351|    2475|  19|  2|\n",
      "|2014|    1|  1|     722|       -8|    1014|      -26|        0|     AA| N3EHAA|    29|   LGA| PBI|     157|    1035|   7| 22|\n",
      "|2014|    1|  1|    1347|        2|    1706|        1|        0|     AA| N319AA|   117|   JFK| LAX|     350|    2475|  13| 47|\n",
      "|2014|    1|  1|    1824|        4|    2145|        0|        0|     AA| N3DEAA|   119|   EWR| LAX|     339|    2454|  18| 24|\n",
      "|2014|    1|  1|    2133|       -2|      37|      -18|        0|     AA| N323AA|   185|   JFK| LAX|     338|    2475|  21| 33|\n",
      "|2014|    1|  1|    1542|       -3|    1906|      -14|        0|     AA| N328AA|   133|   JFK| LAX|     356|    2475|  15| 42|\n",
      "|2014|    1|  1|    1509|       -1|    1828|      -17|        0|     AA| N5FJAA|   145|   JFK| MIA|     161|    1089|  15|  9|\n",
      "|2014|    1|  1|    1848|       -2|    2206|      -14|        0|     AA| N3HYAA|   235|   JFK| SEA|     349|    2422|  18| 48|\n",
      "|2014|    1|  1|    1655|       -5|    2003|      -17|        0|     AA| N5CFAA|   172|   EWR| MIA|     161|    1085|  16| 55|\n",
      "|2014|    1|  1|    1752|        7|    2120|       -5|        0|     AA| N332AA|   177|   JFK| SFO|     365|    2586|  17| 52|\n",
      "|2014|    1|  1|    1253|        3|    1351|        1|        0|     AA| N3JWAA|   178|   JFK| BOS|      39|     187|  12| 53|\n",
      "|2014|    1|  1|    1907|      142|    2223|      133|        0|     AA| N336AA|   181|   JFK| LAX|     345|    2475|  19|  7|\n",
      "|2014|    1|  1|    1720|       -5|    1819|      -26|        0|     AA| N3BCAA|   256|   JFK| BOS|      35|     187|  17| 20|\n",
      "|2014|    1|  1|    1733|       18|    2024|       69|        0|     AA| N3HPAA|   199|   JFK| ORD|     155|     740|  17| 33|\n",
      "|2014|    1|  1|    1640|       25|    2001|       36|        0|     AA| N3HFAA|   211|   JFK| IAH|     234|    1417|  16| 40|\n",
      "|2014|    1|  1|    1714|       -1|    2036|        1|        0|     AA| N3DVAA|   291|   JFK| AUS|     232|    1521|  17| 14|\n",
      "|2014|    1|  1|    1611|      191|    1910|      185|        0|     AA| N471AA|   300|   EWR| DFW|     214|    1372|  16| 11|\n",
      "|2014|    1|  1|     553|       -7|     739|       -6|        0|     AA| N3KHAA|   301|   LGA| ORD|     142|     733|   5| 53|\n",
      "+----+-----+---+--------+---------+--------+---------+---------+-------+-------+------+------+----+--------+--------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.parquet(\"f2.parquet\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253316"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   input_file_name()|\n",
      "+--------------------+\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "|file:///workdir/l...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(input_file_name()).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "89",
     "output_type": "error",
     "text": "<console>:89: error: not found: value spark\n       val df = spark.read.load(\"namesAndAges.parquet\")\n                ^\n",
     "traceback": [
      "\u001b[1;31m<console>:89: error: not found: value spark\u001b[0;0m",
      "\u001b[1;31m       val df = spark.read.load(\"namesAndAges.parquet\")\u001b[0;0m",
      "\u001b[1;31m                ^\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "val df = spark.read.load(\"namesAndAges.parquet\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|Michael|null|\n",
      "|   Andy|  30|\n",
      "| Justin|  19|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df = spark.sql(\"SELECT * FROM parquet.`namesAndAges.parquet`\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(./flights14.csv, ./f2.csv)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "new File(\".\").listFiles.filter(_.getPath.endsWith(\".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DataFrame to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "val flights = spark.read.\n",
    "    format(\"csv\").\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"mode\", \"DROPMALFORMED\").\n",
    "    csv(\"flights14.csv\")\n",
    "flights.write.parquet(\"f2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.select(\"name\", \"age\").write.format(\"parquet\").save(\"namesAndAges.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}