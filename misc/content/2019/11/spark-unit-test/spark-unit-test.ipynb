{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Unit Testing for Spark\n",
    "- Slug: spark-unit-test\n",
    "- Date: 2019-11-26\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, unit testing, unit test\n",
    "- Author: Ben Du\n",
    "- Modified: 2019-11-26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Analyzer\n",
    "\n",
    "If we get the execuation plan, \n",
    "then it is quite easy to analyze ...\n",
    "\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-lineage.html\n",
    "\n",
    "https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-dependencies.html\n",
    "\n",
    "http://hydronitrogen.com/in-the-code-spark-sql-query-planning-and-execution.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Testing Frameworks/Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Scala testing frameworks ScalaTest (recommended) and Specs, \n",
    "or you can use frameworks/tools developed based on them for Spark specifically.\n",
    "Various discussions suggests that **Spark Testing Base** is a good one.\n",
    "\n",
    "https://www.slideshare.net/SparkSummit/beyond-parallelize-and-collect-by-holden-karau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Unit Testing\n",
    "\n",
    "1. [Spark Testing Base](https://github.com/holdenk/spark-testing-base)\n",
    "\n",
    "3. [sscheck](https://github.com/juanrh/sscheck)\n",
    "\n",
    "### Spark Performance Test\n",
    "\n",
    "https://github.com/databricks/spark-perf\n",
    "\n",
    "### Spark Integration Test\n",
    "\n",
    "https://github.com/databricks/spark-integration-tests\n",
    "\n",
    "### Spark Job Validation\n",
    "\n",
    "https://www.slideshare.net/SparkSummit/beyond-parallelize-and-collect-by-holden-karau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuickCheck/ScalaCheck \n",
    "\n",
    "1. QuickCheck generates tests data under a set of constraints \n",
    "2. Scala version is ScalaCheck supported by the two unit testing libraries for Spark \n",
    "    - sscheck\n",
    "        + Awesome people\n",
    "        + supports generating DStreams too! \n",
    "    - spark-testing-base \n",
    "        + Awesome people\n",
    "        + generates more pathological (e.g. empty partitions etc.) RDDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Spark Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Discussions\n",
    "\n",
    "http://blog.ippon.tech/testing-strategy-apache-spark-jobs/\n",
    "\n",
    "http://blog.ippon.tech/testing-strategy-for-spark-streaming/\n",
    "\n",
    "https://www.youtube.com/watch?v=rOQEiTXNS0g\n",
    "\n",
    "https://www.slideshare.net/SparkSummit/beyond-parallelize-and-collect-by-holden-karau\n",
    "\n",
    "https://medium.com/@mrpowers/validating-spark-dataframe-schemas-28d2b3c69d2a\n",
    "\n",
    "### More\n",
    "\n",
    "https://medium.com/@mrpowers/testing-spark-applications-8c590d3215fa\n",
    "\n",
    "http://mkuthan.github.io/blog/2015/03/01/spark-unit-testing/\n",
    "\n",
    "https://dzone.com/articles/testing-spark-code\n",
    "\n",
    "https://spark-summit.org/2014/wp-content/uploads/2014/06/Testing-Spark-Best-Practices-Anupama-Shetty-Neil-Marshall.pdf\n",
    "\n",
    "https://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/\n",
    "\n",
    "https://opencredo.com/spark-testing/\n",
    "\n",
    "http://eugenezhulenev.com/blog/2014/10/18/run-tests-in-standalone-spark-cluster/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "Please refer to \n",
    "[Data for Testing](http://www.legendu.net/misc/blog/data-for-testing)\n",
    "for data generator tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality \n",
    "\n",
    "Please refer to \n",
    "[Data Quality](http://www.legendu.net/misc/blog/data-quality)\n",
    "for data quality related tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Testing Tools\n",
    "\n",
    "\n",
    "[Comparison of Locust and Other Load Testing Tools](https://news.ycombinator.com/item?id=9810274)\n",
    "\n",
    "[Open Source Load Testing Tool Review](http://blog.loadimpact.com/open-source-load-testing-tool-review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locust \n",
    "\n",
    "Locust is a tool/framework for writing code that simulates real user behaviour in a fairly realistic way. For example, it's very common to store state for each simulated user. Once you have written your \"user behaviour code\", you can then simulate a lot of simultaneous users by running it distributed across multiple machines, and hopefully get realistic load sent to you system.\n",
    "\n",
    "If I wanted to just send a lot of requests/s to one or very few URL endpoints, I would also use something like ApacheBench, and I'm author of Locust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ApacheBench](https://en.wikipedia.org/wiki/ApacheBench)\n",
    "\n",
    "ApacheBench (ab) is a single-threaded command line computer program \n",
    "for measuring the performance of HTTP web servers.[1] \n",
    "Originally designed to test the Apache HTTP Server, \n",
    "it is generic enough to test any web server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [PipelineAI](http://pipeline.ai/) looks really interesting!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " val sparkSession: SparkSession = SparkSession.builder()\n",
    "      .master(\"local[2]\")\n",
    "      .appName(\"TestSparkApp\")\n",
    "      .config(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "      .config(\"spark.sql.warehouse.dir\", \"java.io.tmpdir\")\n",
    "      .getOrCreate()\n",
    "  import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}