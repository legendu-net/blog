{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Row-based Mapping and Filtering on DataFrames in Spark\n",
    "- Slug: spark-row-mapping-filtering\n",
    "- Date: 2019-12-13 10:53:33\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, column, map, filter\n",
    "- Author: Ben Du\n",
    "- Modified: 2019-12-13 10:53:33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "Spark `DataFrame` is an alias to `Dataset[Row]`.\n",
    "Even though a Spark DataFrame is stored as Rows in a Dataset,\n",
    "built-in operations/functions (in org.apache.spark.sql.functions) for Spark DataFrame are Column-based.\n",
    "Sometimes, \n",
    "there might be transformations on a DataFrame that is hard to express as Column expressions\n",
    "but rather evey convenient to express as Row expressions. \n",
    "The traditional way to resolve this issue is to wrap the row-based function into a UDF.\n",
    "It is worthing knowing that Spark DataFrame supports map/flatMap APIs \n",
    "which works on Rows. \n",
    "They are still experimental as Spark 2.4.3.\n",
    "It is suggested that you stick to Column-based operations/functions until the Row-based methods mature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1384d1-a6ea-4a18-a432-582a1cf9bcef",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.3.1\n",
    "org.apache.spark spark-sql_2.11 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@33a3ab71"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Spark Example\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "|   4|   d| bar| 7.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (1L, \"a\", \"foo\", 3.0),\n",
    "    (2L, \"b\", \"bar\", 4.0),\n",
    "    (3L, \"c\", \"foo\", 5.0),\n",
    "    (4L, \"d\", \"bar\", 7.0)\n",
    ").toDF(\"col1\", \"col2\", \"col3\", \"col4\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   2|   b| bar| 4.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter{\n",
    "    row => row.getLong(0) < 3 \n",
    "}.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+\n",
      "|col1|col2|col3|col4|\n",
      "+----+----+----+----+\n",
      "|   1|   a| foo| 3.0|\n",
      "|   3|   c| foo| 5.0|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter{\n",
    "    row => row.getAs(\"col3\") == \"foo\" \n",
    "}.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "112",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m<console>:112: error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.\u001b[0;0m",
      "\u001b[1;31m       df.map {\u001b[0;0m",
      "\u001b[1;31m              ^\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "df.map {\n",
    "    row => Row(row.getInt(0) + row.getDouble(3))\n",
    "}.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map vs UDF\n",
    "\n",
    "https://stackoverflow.com/questions/38860808/performance-impact-of-rdd-api-vs-udfs-mixed-with-dataframe-api\n",
    "\n",
    "https://stackoverflow.com/questions/39039081/difference-between-a-map-and-udf\n",
    "\n",
    "https://stackoverflow.com/questions/43411234/spark-sql-whether-to-use-row-transformation-or-udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val df = spark.read.json(\"../data/people.json\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.map(s => s.getString(1)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   30|\n",
      "|   19|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter($\"age\".isNotNull).map(r => r.getLong(0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Operation to All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  x|  y|  z|\n",
      "+---+---+---+\n",
      "|  a|  B|  c|\n",
      "|  D|  e|  F|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{col, upper}\n",
    "\n",
    "val df = Seq(\n",
    "    (\"a\", \"B\", \"c\"), \n",
    "    (\"D\", \"e\", \"F\")\n",
    ").toDF(\"x\", \"y\", \"z\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  x|  y|  z|\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "|  D|  E|  F|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns.map(c => upper(col(c)).alias(c)): _*).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}