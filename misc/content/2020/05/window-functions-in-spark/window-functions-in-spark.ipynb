{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-05-03 23:52:16\n",
    "- Title: Window Functions in Spark\n",
    "- Slug: window-functions-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, window function, partition, over, analytics functions, big data\n",
    "- Modified: 2021-09-30 17:35:09\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Window with orderBy\n",
    "\n",
    "It is tricky!!!\n",
    "\n",
    "If you provide ORDER BY clause then the default frame is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW:\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/52273186/pyspark-spark-window-function-first-last-issue\n",
    "\n",
    "1. Avoid using last and use first with `descending order by` instead.\n",
    "   This gives less surprisings.\n",
    "   \n",
    "2. Do NOT use order by if not necessary. \n",
    "   It introduces unnecessary ..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark\").enableHiveSupport().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from pyspark.sql import Window"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cust_p = pd.DataFrame(\n",
    "    data=[\n",
    "        (\"Alice\", \"2016-05-01\", 50.00, 1), (\"Alice\", \"2016-05-01\", 45.00, 2),\n",
    "        (\"Alice\", \"2016-05-02\", 55.00, 3), (\"Alice\", \"2016-05-02\", 100.00, 4),\n",
    "        (\"Bob\", \"2016-05-01\", 25.00, 5), (\"Bob\", \"2016-05-01\", 29.00, 6),\n",
    "        (\"Bob\", \"2016-05-02\", 27.00, 7), (\"Bob\", \"2016-05-02\", 30.00, 8)\n",
    "    ],\n",
    "    columns=(\"name\", \"date\", \"amount\", \"id\")\n",
    ")\n",
    "cust_p"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    name        date  amount  id\n",
       "0  Alice  2016-05-01    50.0   1\n",
       "1  Alice  2016-05-01    45.0   2\n",
       "2  Alice  2016-05-02    55.0   3\n",
       "3  Alice  2016-05-02   100.0   4\n",
       "4    Bob  2016-05-01    25.0   5\n",
       "5    Bob  2016-05-01    29.0   6\n",
       "6    Bob  2016-05-02    27.0   7\n",
       "7    Bob  2016-05-02    30.0   8"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>amount</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bob</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bob</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bob</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "cust = spark.createDataFrame(cust_p)\n",
    "cust.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "cust.orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a temp view for testing Spark SQL (to compare with the result of PySpark DataFrame API)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "cust.createOrReplaceTempView(\"customers\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## max"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`max` works well on `over ... partition ...` when `order by` is not used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "cust.select(\n",
    "    col(\"name\"), col(\"date\"), col(\"amount\"), col(\"id\"),\n",
    "    max(col(\"amount\")).over(Window.partitionBy(\"name\", \"date\")).alias(\"max_amount\")\n",
    ").orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|     100.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      29.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      30.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        max(amount) over (partition by name, date) as max_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|     100.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      29.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      30.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following results might surprise people. \n",
    "There is nothing wrong in code. \n",
    "It is only that when `order by` is used,\n",
    "the default frame for window functions (`max` in this case) is unbounded preceding and the current row."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "cust.select(\n",
    "    col(\"name\"), col(\"date\"), col(\"amount\"), col(\"id\"),\n",
    "    max(col(\"amount\")).over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")\n",
    "                           ).alias(\"max_amount\")\n",
    ").orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        max(amount) over (partition by name, date order by id) as max_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The window function `rank` requires `order by` to be used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "cust.select(\n",
    "    col(\"name\"), col(\"date\"), col(\"amount\"), col(\"id\"),\n",
    "    rank().over(Window.partitionBy(\"name\", \"date\").orderBy(\"amount\")).alias(\"rank\")\n",
    ").orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----+\n",
      "| name|      date|amount| id|rank|\n",
      "+-----+----------+------+---+----+\n",
      "|Alice|2016-05-01|  45.0|  2|   1|\n",
      "|Alice|2016-05-01|  50.0|  1|   2|\n",
      "|Alice|2016-05-02|  55.0|  3|   1|\n",
      "|Alice|2016-05-02| 100.0|  4|   2|\n",
      "|  Bob|2016-05-01|  25.0|  5|   1|\n",
      "|  Bob|2016-05-01|  29.0|  6|   2|\n",
      "|  Bob|2016-05-02|  27.0|  7|   1|\n",
      "|  Bob|2016-05-02|  30.0|  8|   2|\n",
      "+-----+----------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        rank() over (partition by name, date ORDER BY amount DESC) as rank\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----+\n",
      "| name|      date|amount| id|rank|\n",
      "+-----+----------+------+---+----+\n",
      "|Alice|2016-05-01|  50.0|  1|   1|\n",
      "|Alice|2016-05-01|  45.0|  2|   2|\n",
      "|Alice|2016-05-02| 100.0|  4|   1|\n",
      "|Alice|2016-05-02|  55.0|  3|   2|\n",
      "|  Bob|2016-05-01|  29.0|  6|   1|\n",
      "|  Bob|2016-05-01|  25.0|  5|   2|\n",
      "|  Bob|2016-05-02|  30.0|  8|   1|\n",
      "|  Bob|2016-05-02|  27.0|  7|   2|\n",
      "+-----+----------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "cust.select(\n",
    "    col(\"name\"), col(\"date\"), col(\"amount\"), col(\"id\"),\n",
    "    rank().over(Window.partitionBy(\"name\").orderBy(\"date\")).alias(\"rank\")\n",
    ").orderBy(\"name\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----+\n",
      "| name|      date|amount| id|rank|\n",
      "+-----+----------+------+---+----+\n",
      "|Alice|2016-05-01|  50.0|  1|   1|\n",
      "|Alice|2016-05-01|  45.0|  2|   1|\n",
      "|Alice|2016-05-02|  55.0|  3|   3|\n",
      "|Alice|2016-05-02| 100.0|  4|   3|\n",
      "|  Bob|2016-05-01|  25.0|  5|   1|\n",
      "|  Bob|2016-05-02|  30.0|  8|   3|\n",
      "|  Bob|2016-05-01|  29.0|  6|   1|\n",
      "|  Bob|2016-05-02|  27.0|  7|   3|\n",
      "+-----+----------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        rank() over (partition by name ORDER BY date DESC) as rank\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----+\n",
      "| name|      date|amount| id|rank|\n",
      "+-----+----------+------+---+----+\n",
      "|Alice|2016-05-02|  55.0|  3|   1|\n",
      "|Alice|2016-05-02| 100.0|  4|   1|\n",
      "|Alice|2016-05-01|  50.0|  1|   3|\n",
      "|Alice|2016-05-01|  45.0|  2|   3|\n",
      "|  Bob|2016-05-02|  27.0|  7|   1|\n",
      "|  Bob|2016-05-02|  30.0|  8|   1|\n",
      "|  Bob|2016-05-01|  25.0|  5|   3|\n",
      "|  Bob|2016-05-01|  29.0|  6|   3|\n",
      "+-----+----------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## dense_rank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "cust.select(\n",
    "    col(\"name\"), col(\"date\"), col(\"amount\"), col(\"id\"),\n",
    "    dense_rank().over(Window.partitionBy(\"name\").orderBy(\"date\")).alias(\"dense_rank\")\n",
    ").orderBy(\"name\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|dense_rank|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|         1|\n",
      "|Alice|2016-05-01|  45.0|  2|         1|\n",
      "|Alice|2016-05-02|  55.0|  3|         2|\n",
      "|Alice|2016-05-02| 100.0|  4|         2|\n",
      "|  Bob|2016-05-01|  25.0|  5|         1|\n",
      "|  Bob|2016-05-01|  29.0|  6|         1|\n",
      "|  Bob|2016-05-02|  27.0|  7|         2|\n",
      "|  Bob|2016-05-02|  30.0|  8|         2|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        dense_rank() over (partition by name ORDER BY date DESC) as rank\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----+\n",
      "| name|      date|amount| id|rank|\n",
      "+-----+----------+------+---+----+\n",
      "|Alice|2016-05-02|  55.0|  3|   1|\n",
      "|Alice|2016-05-02| 100.0|  4|   1|\n",
      "|Alice|2016-05-01|  50.0|  1|   2|\n",
      "|Alice|2016-05-01|  45.0|  2|   2|\n",
      "|  Bob|2016-05-02|  27.0|  7|   1|\n",
      "|  Bob|2016-05-02|  30.0|  8|   1|\n",
      "|  Bob|2016-05-01|  25.0|  5|   2|\n",
      "|  Bob|2016-05-01|  29.0|  6|   2|\n",
      "+-----+----------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## first"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    first($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"first_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|        55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        25.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        27.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        first(amount) over (partition by name, date order by id) as first_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|        55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        25.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        27.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    last($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"last_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  50.0|  1|       50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|       55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        last(amount) over (partition by name, date order by id) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  50.0|  1|       50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-02|  55.0|  3|       55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    first($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy($\"id\".desc)).alias(\"last_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-01|  50.0|  1|       45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      100.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       29.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        first(amount) over (partition by name, date order by id desc) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-01|  50.0|  1|       45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      100.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       29.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## partition by with group by"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avoid doing so!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        first(amount) over (partition by name, date order by id desc) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    group by\n",
    "        name, date\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " expression 'customers.`amount`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;",
     "traceback": [
      "\u001b[1;31morg.apache.spark.sql.AnalysisException: expression 'customers.`amount`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\u001b[0;0m",
      "\u001b[1;31mProject [name#9, date#10, last_amount#30]\u001b[0;0m",
      "\u001b[1;31m+- Project [name#9, date#10, amount#11, id#12, last_amount#30, last_amount#30]\u001b[0;0m",
      "\u001b[1;31m   +- Window [first(amount#11, false) windowspecdefinition(name#9, date#10, id#12 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_amount#30], [name#9, date#10], [id#12 DESC NULLS LAST]\u001b[0;0m",
      "\u001b[1;31m      +- Aggregate [name#9, date#10], [name#9, date#10, amount#11, id#12]\u001b[0;0m",
      "\u001b[1;31m         +- SubqueryAlias `customers`\u001b[0;0m",
      "\u001b[1;31m            +- Project [_1#4 AS name#9, _2#5 AS date#10, _3#6 AS amount#11, _4#7 AS id#12]\u001b[0;0m",
      "\u001b[1;31m               +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\u001b[0;0m",
      "\u001b[1;31m\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:224)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        first(max(amount)) over (partition by name, date order by id desc) as last_amount\n",
    "    from\n",
    "        customers\n",
    "    group by\n",
    "        name, date\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " expression 'customers.`id`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;",
     "traceback": [
      "\u001b[1;31morg.apache.spark.sql.AnalysisException: expression 'customers.`id`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\u001b[0;0m",
      "\u001b[1;31mProject [name#9, date#10, last_amount#36]\u001b[0;0m",
      "\u001b[1;31m+- Project [name#9, date#10, _w0#39, id#12, last_amount#36, last_amount#36]\u001b[0;0m",
      "\u001b[1;31m   +- Window [first(_w0#39, false) windowspecdefinition(name#9, date#10, id#12 DESC NULLS LAST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS last_amount#36], [name#9, date#10], [id#12 DESC NULLS LAST]\u001b[0;0m",
      "\u001b[1;31m      +- Aggregate [name#9, date#10], [name#9, date#10, max(amount#11) AS _w0#39, id#12]\u001b[0;0m",
      "\u001b[1;31m         +- SubqueryAlias `customers`\u001b[0;0m",
      "\u001b[1;31m            +- Project [_1#4 AS name#9, _2#5 AS date#10, _3#6 AS amount#11, _4#7 AS id#12]\u001b[0;0m",
      "\u001b[1;31m               +- LocalRelation [_1#4, _2#5, _3#6, _4#7]\u001b[0;0m",
      "\u001b[1;31m\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:42)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:224)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$10.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.immutable.List.foreach(List.scala:392)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\u001b[0;0m",
      "\u001b[1;31m  ... 48 elided\u001b[0;0m"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "customers.orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        row_number() over (partition by name, date order by id desc) as rownum\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------+\n",
      "| name|      date|amount| id|rownum|\n",
      "+-----+----------+------+---+------+\n",
      "|Alice|2016-05-01|  45.0|  2|     1|\n",
      "|Alice|2016-05-01|  50.0|  1|     2|\n",
      "|Alice|2016-05-02| 100.0|  4|     1|\n",
      "|Alice|2016-05-02|  55.0|  3|     2|\n",
      "|  Bob|2016-05-01|  29.0|  6|     1|\n",
      "|  Bob|2016-05-01|  25.0|  5|     2|\n",
      "|  Bob|2016-05-02|  30.0|  8|     1|\n",
      "|  Bob|2016-05-02|  27.0|  7|     2|\n",
      "+-----+----------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select \n",
    "        *\n",
    "    from (\n",
    "        select\n",
    "            name,\n",
    "            date,\n",
    "            amount,\n",
    "            id,\n",
    "            row_number() over (partition by name, date order by id desc) as rownum\n",
    "        from\n",
    "            customers\n",
    "        ) A\n",
    "    where \n",
    "        rownum = 1\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------+\n",
      "| name|      date|amount| id|rownum|\n",
      "+-----+----------+------+---+------+\n",
      "|Alice|2016-05-01|  45.0|  2|     1|\n",
      "|Alice|2016-05-02| 100.0|  4|     1|\n",
      "|  Bob|2016-05-01|  29.0|  6|     1|\n",
      "|  Bob|2016-05-02|  30.0|  8|     1|\n",
      "+-----+----------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html\n",
    "\n",
    "https://spark.apache.org/docs/2.1.1/api/scala/index.html#org.apache.spark.sql.functions$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a window parition.\n",
    "It does not have to be associated with a table."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "Window.partitionBy(\"col1\", \"col2\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@4f336bdc"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "val customers = Seq(\n",
    "    (\"Alice\", \"2016-05-01\", 50.00, 1),\n",
    "    (\"Alice\", \"2016-05-01\", 45.00, 2),\n",
    "    (\"Alice\", \"2016-05-02\", 55.00, 3),\n",
    "    (\"Alice\", \"2016-05-02\", 100.00, 4),\n",
    "    (\"Bob\", \"2016-05-01\", 25.00, 5),\n",
    "    (\"Bob\", \"2016-05-01\", 29.00, 6),\n",
    "    (\"Bob\", \"2016-05-02\", 27.00,7 ),\n",
    "    (\"Bob\", \"2016-05-02\", 30.00, 8)\n",
    ").toDF(\"name\", \"date\", \"amount\", \"id\")\n",
    "customers.show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "val winSpec = Window.partitionBy(\"name\", \"date\")\n",
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    avg($\"amount\").over(winSpec).alias(\"avg_amt\"),\n",
    "    max($\"id\").over(winSpec).alias(\"max_id\")\n",
    ").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+-------+------+\n",
      "| name|      date|amount| id|avg_amt|max_id|\n",
      "+-----+----------+------+---+-------+------+\n",
      "|  Bob|2016-05-01|  25.0|  5|   27.0|     6|\n",
      "|  Bob|2016-05-01|  29.0|  6|   27.0|     6|\n",
      "|  Bob|2016-05-02|  27.0|  7|   28.5|     8|\n",
      "|  Bob|2016-05-02|  30.0|  8|   28.5|     8|\n",
      "|Alice|2016-05-02|  55.0|  3|   77.5|     4|\n",
      "|Alice|2016-05-02| 100.0|  4|   77.5|     4|\n",
      "|Alice|2016-05-01|  50.0|  1|   47.5|     2|\n",
      "|Alice|2016-05-01|  45.0|  2|   47.5|     2|\n",
      "+-----+----------+------+---+-------+------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "val winSpec = Window.partitionBy(\"name\", \"date\")\n",
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    (avg($\"amount\").over(winSpec) + max($\"id\").over(winSpec) * 100).alias(\"new_column\")\n",
    ").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|new_column|\n",
      "+-----+----------+------+---+----------+\n",
      "|  Bob|2016-05-01|  25.0|  5|     627.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|     627.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|     828.5|\n",
      "|  Bob|2016-05-02|  30.0|  8|     828.5|\n",
      "|Alice|2016-05-02|  55.0|  3|     477.5|\n",
      "|Alice|2016-05-02| 100.0|  4|     477.5|\n",
      "|Alice|2016-05-01|  50.0|  1|     247.5|\n",
      "|Alice|2016-05-01|  45.0|  2|     247.5|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "customers.withColumn(\"avg\", \n",
    "    avg($\"amount\").over(Window.partitionBy(\"name\", \"date\"))\n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+----+\n",
      "| name|      date|amount| avg|\n",
      "+-----+----------+------+----+\n",
      "|  Bob|2016-05-01|  25.0|27.0|\n",
      "|  Bob|2016-05-01|  29.0|27.0|\n",
      "|  Bob|2016-05-02|  27.0|28.5|\n",
      "|  Bob|2016-05-02|  30.0|28.5|\n",
      "|Alice|2016-05-02|  55.0|77.5|\n",
      "|Alice|2016-05-02| 100.0|77.5|\n",
      "|Alice|2016-05-01|  50.0|47.5|\n",
      "|Alice|2016-05-01|  45.0|47.5|\n",
      "+-----+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "val customers = Seq(\n",
    "    (\"Alice\", \"2016-05-01\", 50.00),\n",
    "    (\"Alice\", \"2016-05-03\", 45.00),\n",
    "    (\"Alice\", \"2016-05-04\", 55.00),\n",
    "    (\"Bob\", \"2016-05-01\", 25.00),\n",
    "    (\"Bob\", \"2016-05-04\", 29.00),\n",
    "    (\"Bob\", \"2016-05-06\", 27.00)\n",
    ").toDF(\"name\", \"date\", \"amountSpent\")\n",
    "customers.show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----------+\n",
      "| name|      date|amountSpent|\n",
      "+-----+----------+-----------+\n",
      "|Alice|2016-05-01|       50.0|\n",
      "|Alice|2016-05-03|       45.0|\n",
      "|Alice|2016-05-04|       55.0|\n",
      "|  Bob|2016-05-01|       25.0|\n",
      "|  Bob|2016-05-04|       29.0|\n",
      "|  Bob|2016-05-06|       27.0|\n",
      "+-----+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Moving Average"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "val wSpec1 = Window.partitionBy(\"name\").orderBy(\"date\").rowsBetween(-1, 1)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "customers.withColumn(\"movingAvg\", avg(customers(\"amountSpent\")).over(wSpec1)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----------+---------+\n",
      "| name|      date|amountSpent|movingAvg|\n",
      "+-----+----------+-----------+---------+\n",
      "|  Bob|2016-05-01|       25.0|     27.0|\n",
      "|  Bob|2016-05-04|       29.0|     27.0|\n",
      "|  Bob|2016-05-06|       27.0|     28.0|\n",
      "|Alice|2016-05-01|       50.0|     47.5|\n",
      "|Alice|2016-05-03|       45.0|     50.0|\n",
      "|Alice|2016-05-04|       55.0|     50.0|\n",
      "+-----+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cumulative Sum"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "val wSpec2 = Window.partitionBy(\"name\").orderBy(\"date\").rowsBetween(Long.MinValue, 0)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "customers.withColumn(\"cumSum\", sum(customers(\"amountSpent\")).over(wSpec2)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----------+------+\n",
      "| name|      date|amountSpent|cumSum|\n",
      "+-----+----------+-----------+------+\n",
      "|  Bob|2016-05-01|       25.0|  25.0|\n",
      "|  Bob|2016-05-04|       29.0|  54.0|\n",
      "|  Bob|2016-05-06|       27.0|  81.0|\n",
      "|Alice|2016-05-01|       50.0|  50.0|\n",
      "|Alice|2016-05-03|       45.0|  95.0|\n",
      "|Alice|2016-05-04|       55.0| 150.0|\n",
      "+-----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data from previous row"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "val wSpec3 = Window.partitionBy(\"name\").orderBy(\"date\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "customers.withColumn(\"prevAmountSpent\",\n",
    "                     lag(customers(\"amountSpent\"), 1).over(wSpec3)).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----------+---------------+\n",
      "| name|      date|amountSpent|prevAmountSpent|\n",
      "+-----+----------+-----------+---------------+\n",
      "|  Bob|2016-05-01|       25.0|           null|\n",
      "|  Bob|2016-05-04|       29.0|           25.0|\n",
      "|  Bob|2016-05-06|       27.0|           29.0|\n",
      "|Alice|2016-05-01|       50.0|           null|\n",
      "|Alice|2016-05-03|       45.0|           50.0|\n",
      "|Alice|2016-05-04|       55.0|           45.0|\n",
      "+-----+----------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## row_number"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "val wSpec3 = Window.partitionBy(\"name\").orderBy(\"date\")\n",
    "customers.withColumn(\"row_num\", \n",
    "    row_number().over(wSpec3) \n",
    ").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+-----------+-------+\n",
      "| name|      date|amountSpent|row_num|\n",
      "+-----+----------+-----------+-------+\n",
      "|  Bob|2016-05-01|       25.0|      1|\n",
      "|  Bob|2016-05-04|       29.0|      2|\n",
      "|  Bob|2016-05-06|       27.0|      3|\n",
      "|Alice|2016-05-01|       50.0|      1|\n",
      "|Alice|2016-05-03|       45.0|      2|\n",
      "|Alice|2016-05-04|       55.0|      3|\n",
      "+-----+----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## percentRank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ntile"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## first"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## last"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lag"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lead"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## cume_dist"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "val customers = Seq(\n",
    "    (\"Alice\", \"2016-05-01\", 50.00, 1),\n",
    "    (\"Alice\", \"2016-05-01\", 45.00, 2),\n",
    "    (\"Alice\", \"2016-05-02\", 55.00, 3),\n",
    "    (\"Alice\", \"2016-05-02\", 100.00, 4),\n",
    "    (\"Bob\", \"2016-05-01\", 25.00, 5),\n",
    "    (\"Bob\", \"2016-05-01\", 29.00, 6),\n",
    "    (\"Bob\", \"2016-05-02\", 27.00,7 ),\n",
    "    (\"Bob\", \"2016-05-02\", 30.00, 8)\n",
    ").toDF(\"name\", \"date\", \"amount\", \"id\")\n",
    "customers.orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+\n",
      "| name|      date|amount| id|\n",
      "+-----+----------+------+---+\n",
      "|Alice|2016-05-01|  50.0|  1|\n",
      "|Alice|2016-05-01|  45.0|  2|\n",
      "|Alice|2016-05-02|  55.0|  3|\n",
      "|Alice|2016-05-02| 100.0|  4|\n",
      "|  Bob|2016-05-01|  29.0|  6|\n",
      "|  Bob|2016-05-01|  25.0|  5|\n",
      "|  Bob|2016-05-02|  27.0|  7|\n",
      "|  Bob|2016-05-02|  30.0|  8|\n",
      "+-----+----------+------+---+\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "customers.createOrReplaceTempView(\"customers\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comment\n",
    "\n",
    "Do NOT use `orderBy` if the order does not matter!!!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "val wSpec = Window.partitionBy(\"name\", \"date\").orderBy(\"id\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@4f418d5a"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    max($\"amount\").over(Window.partitionBy(\"name\", \"date\")).alias(\"max_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|     100.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      29.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      30.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        max(amount) over (partition by name, date) as max_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|     100.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      29.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      30.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    max($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"max_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        max(amount) over (partition by name, date order by id) as max_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+----------+\n",
      "| name|      date|amount| id|max_amount|\n",
      "+-----+----------+------+---+----------+\n",
      "|Alice|2016-05-01|  50.0|  1|      50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|      50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|      55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|     100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|      25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|      29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|      27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|      30.0|\n",
      "+-----+----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    first($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"first_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|        55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        25.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        27.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        first(amount) over (partition by name, date order by id) as first_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        50.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|        55.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        25.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        27.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "customers.select(\n",
    "    $\"name\",\n",
    "    $\"date\",\n",
    "    $\"amount\",\n",
    "    $\"id\",\n",
    "    last($\"amount\").over(Window.partitionBy(\"name\", \"date\").orderBy(\"id\")).alias(\"last_amount\")\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+-----------+\n",
      "| name|      date|amount| id|last_amount|\n",
      "+-----+----------+------+---+-----------+\n",
      "|Alice|2016-05-01|  50.0|  1|       50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|       45.0|\n",
      "|Alice|2016-05-02|  55.0|  3|       55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|      100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|       25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|       29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|       27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|       30.0|\n",
      "+-----+----------+------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        last(amount) over (partition by name, date order by id) as first_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  50.0|  1|        50.0|\n",
      "|Alice|2016-05-01|  45.0|  2|        45.0|\n",
      "|Alice|2016-05-02|  55.0|  3|        55.0|\n",
      "|Alice|2016-05-02| 100.0|  4|       100.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        25.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        29.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        27.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        30.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "        name,\n",
    "        date,\n",
    "        amount,\n",
    "        id,\n",
    "        first(amount) over (partition by name, date order by id desc) as first_amount\n",
    "    from\n",
    "        customers\n",
    "    \"\"\"\n",
    ").orderBy(\"name\", \"date\").show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+----------+------+---+------------+\n",
      "| name|      date|amount| id|first_amount|\n",
      "+-----+----------+------+---+------------+\n",
      "|Alice|2016-05-01|  45.0|  2|        45.0|\n",
      "|Alice|2016-05-01|  50.0|  1|        45.0|\n",
      "|Alice|2016-05-02| 100.0|  4|       100.0|\n",
      "|Alice|2016-05-02|  55.0|  3|       100.0|\n",
      "|  Bob|2016-05-01|  29.0|  6|        29.0|\n",
      "|  Bob|2016-05-01|  25.0|  5|        29.0|\n",
      "|  Bob|2016-05-02|  30.0|  8|        30.0|\n",
      "|  Bob|2016-05-02|  27.0|  7|        30.0|\n",
      "+-----+----------+------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "val customers = Seq(\n",
    "    (\"Alice\", \"1\", \"2016-05-01\", 50.00),\n",
    "    (\"Alice\", \"1\", \"2016-05-03\", 45.00),\n",
    "    (\"Alice\", \"2\", \"2016-05-04\", 55.00),\n",
    "    (\"Bob\", \"2\", \"2016-05-01\", 25.00),\n",
    "    (\"Bob\", \"2\", \"2016-05-04\", 29.00),\n",
    "    (\"Bob\", \"2\", \"2016-05-06\", 27.00)\n",
    ").toDF(\"name\", \"group\", \"date\", \"amountSpent\")\n",
    "customers.show"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-----+----------+-----------+\n",
      "| name|group|      date|amountSpent|\n",
      "+-----+-----+----------+-----------+\n",
      "|Alice|    1|2016-05-01|       50.0|\n",
      "|Alice|    1|2016-05-03|       45.0|\n",
      "|Alice|    2|2016-05-04|       55.0|\n",
      "|  Bob|    2|2016-05-01|       25.0|\n",
      "|  Bob|    2|2016-05-04|       29.0|\n",
      "|  Bob|    2|2016-05-06|       27.0|\n",
      "+-----+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "val ws1 = Window.partitionBy(\"name\").orderBy(\"date\")\n",
    "val ws2 = Window.partitionBy(\"group\").orderBy(\"date\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "customers.\n",
    "    withColumn(\"i\", row_number().over(ws1)).\n",
    "    withColumn(\"j\", row_number().over(ws2)).\n",
    "    show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-----+----------+-----------+---+---+\n",
      "| name|group|      date|amountSpent|  i|  j|\n",
      "+-----+-----+----------+-----------+---+---+\n",
      "|Alice|    1|2016-05-01|       50.0|  1|  1|\n",
      "|Alice|    1|2016-05-03|       45.0|  2|  2|\n",
      "|  Bob|    2|2016-05-01|       25.0|  1|  1|\n",
      "|  Bob|    2|2016-05-04|       29.0|  2|  2|\n",
      "|Alice|    2|2016-05-04|       55.0|  3|  3|\n",
      "|  Bob|    2|2016-05-06|       27.0|  3|  4|\n",
      "+-----+-----+----------+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "- [Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "\n",
    "- [Spark Scala Functions](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html)\n",
    "\n",
    "- [Spark Window Functions for DataFrames](http://xinhstechblog.blogspot.com/2016/04/spark-window-functions-for-dataframes.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}