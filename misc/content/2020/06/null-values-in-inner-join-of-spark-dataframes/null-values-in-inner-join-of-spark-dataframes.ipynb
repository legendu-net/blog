{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-06-17 00:25:21\n",
    "- Title: Null Values in Inner Join of Spark Dataframes\n",
    "- Slug: null-values-in-inner-join-of-spark-dataframes\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, join, inner join, Spark, big data, DataFrame, null\n",
    "- Modified: 2020-06-17 00:25:21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.11 2.3.1\n",
    "org.apache.spark spark-sql_2.11 2.3.1\n",
    "org.apache.spark spark-hive_2.11 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "    .master(\"local[2]\")\n",
    "    .appName(\"Spark-Null\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. `null` values are excluded from inner joinning even if both tables have null values in the field for inner joining.\n",
    "\n",
    "2. It is almost always a good idea to filter out null value in the joinining columns before joining\n",
    "    no matter it is an inner join or an outer join \n",
    "    (of course if the rows containing null matters in your use case, you have to do a union of those records).\n",
    "    Spark (at least in Spark 2.3 and older) is stupid enough not to filter out joining keys/columns with null values before even INNER join\n",
    "    (even if null values are dropped after inner join).\n",
    "    This means that if a joining key/column has lots of null values, \n",
    "    it get shuffle into the same node in SortMergeJoin.\n",
    "    This can cause a serious data skew issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val left = Seq(\n",
    "    (\"bob\", \"2015-01-13\", 4), \n",
    "    (null, \"2015-04-23\",10)\n",
    ").toDF(\"name\",\"date\",\"duration\")\n",
    "left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val right = Seq(\n",
    "    (null, 100),\n",
    "    (\"bob\", 23)\n",
    ").toDF(\"name\",\"upload\")\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.join(right, Seq(\"name\")).show"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}