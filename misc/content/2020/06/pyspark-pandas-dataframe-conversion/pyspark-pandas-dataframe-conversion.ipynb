{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Conversion Between PySpark DataFrames and pandas DataFrames\n",
    "- Slug: pyspark-pandas-dataframe-conversion\n",
    "- Date: 2020-06-18 08:43:44\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Python, HPC, high performance computing, PySpark, DataFrame, construct\n",
    "- Author: Ben Du\n",
    "- Modified: 2020-06-18 08:43:44\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "1. A PySpark DataFrame can be converted to a pandas DataFrame by calling the method `DataFrame.toPandas`,\n",
    "    and a pandas DataFrame can be converted to a PySpark DataFrame by calling `SparkSession.createDataFrame`.\n",
    "    Notice that when you call `DataFrame.toPandas` \n",
    "    to convert a Spark DataFrame to a pandas DataFrame, \n",
    "    the whole Spark DataFrame is collected to the driver machine!\n",
    "    This means that you should only call the method `DataFrame.toPandas`\n",
    "    when the Spark DataFrame is small enough to fit into the memory of the driver machine.\n",
    "    When a Spark DataFrame is too big to be collected on to the driver machine\n",
    "    and you'd like to manipulate the DataFrame in Python \n",
    "    (e.g., do model inference/prediction leveraging Python machine learning libraries),\n",
    "    it is best to use pandas UDF to achieve this. \n",
    "    When applying a pandas UDF to a Spark DataFrame, \n",
    "    the padnas UDF is applied to each partition of the Spark DataFrame independently. \n",
    "    Of course each partition of the Spark DataFrame is automatically converted to a pandas DataFrame\n",
    "    and get converted back to a Spark DataFrame \n",
    "    after the manipulation in Python is done. \n",
    "    For more discussions on pandas UDF, \n",
    "    please refer to\n",
    "    [User-defined Functions (UDF) in PySpark](http://www.legendu.net/misc/blog/pyspark-udf/)\n",
    "    .\n",
    "\n",
    "2. Apache Arrow can be leveraged to convert between Spark DataFrame and pandas DataFrame without data copying. \n",
    "    However, \n",
    "    there are some restrictions on this.\n",
    "    \n",
    "    - Ensure that `pandas==0.24.2` and `pyArrow==0.15.1` are installed.\n",
    "    - Set `spark.sql.execution.arrow.pyspark.enabled` to be `true` to enable conversion using Apache Arrow.\n",
    "    - Since Spark 3, \n",
    "        all Spark SQL data types are supported by Arrow-based conversion \n",
    "        except MapType, ArrayType of TimestampType, and nested StructType.\n",
    "        NOTE that BinaryType is supported by Arrow-based conversion since Spark 2.4.\n",
    "        If you need to work on BinaryType data leveraing Apache Arrow, \n",
    "        the minimum Spark version you have to use is Spark 2.4.\n",
    "    - Data partitions in Spark are converted into Arrow record batches, \n",
    "        which can temporarily lead to high memory usage in the JVM. \n",
    "        To avoid possible out of memory exceptions, \n",
    "        the size of the Arrow record batches can be adjusted \n",
    "        by setting the conf `spark.sql.execution.arrow.maxRecordsPerBatch` \n",
    "        to an integer that will determine the maximum number of rows for each batch. \n",
    "        The default value is `10,000` records per batch. \n",
    "        If the number of columns is large, \n",
    "        the value should be adjusted accordingly. \n",
    "        Using this limit, \n",
    "        each data partition will be made into 1 or more record batches for processing.\n",
    "     - If you use Spark 2 with `pyArrow>=0.15.0`,\n",
    "         you need to set the environment variable `ARROW_PRE_0_15_IPC_FORMAT=1`,\n",
    "         which can be achieved using \n",
    "         `spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT=1`\n",
    "         and `spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT=1`.\n",
    "\n",
    "\n",
    "    Please refer to \n",
    "    [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html)\n",
    "    for more details.\n",
    "\n",
    "2. The perhaps most convenient way to create an ad hoc PySpark DataFrame \n",
    "    is to first [create a pandas DataFrame](http://www.legendu.net/en/blog/construct-pandas-dataframe-python/)\n",
    "    and then convert it to a PySpark DataFrame (using `SparkSession.createDataFrame`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_pandas\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ben</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dan</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  id  age\n",
       "0   Ben   2   30\n",
       "1   Dan   4   25\n",
       "2  Will   1   26"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p = pd.DataFrame(\n",
    "    data=[\n",
    "        [\"Ben\", 2, 30],\n",
    "        [\"Dan\", 4, 25],\n",
    "        [\"Will\", 1, 26],\n",
    "    ],\n",
    "    columns=[\"name\", \"id\", \"age\"]\n",
    ")\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|name| id|age|\n",
      "+----+---+---+\n",
      "| Ben|  2| 30|\n",
      "| Dan|  4| 25|\n",
      "|Will|  1| 26|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(df_p)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://stackoverflow.com/questions/37612622/spark-unionall-multiple-dataframes\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}