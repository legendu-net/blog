{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Date: 2020-09-26 09:01:58\n",
    "- Title: Partition and Bucketing in Spark\n",
    "- Slug: partition-bucketing-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, Spark, big data, bucket, partition\n",
    "- Modified: 2021-01-26 09:01:58\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. Bucketed column is only supported in Hive table at this time. \n",
    "\n",
    "2. A Hive table can have both partition and bucket columns.\n",
    "\n",
    "2. Suppose `t1` and `t2` are 2 bucketed tables and with the number of buckets `b1` and `b2` respecitvely.\n",
    "    For bucket optimization to kick in when joining them:\n",
    "\n",
    "        - The 2 tables must be bucketed on the same keys/columns.\n",
    "        - Must joining on the bucket keys/columns.\n",
    "        - `b1` is a multiple of `b2` or `b2` is a multiple of `b1`.\n",
    "        \n",
    "    When there are many bucketed table that might join with each other, \n",
    "    the number of buckets need to be carefully designed so that efficient bucket join can always be leveraged.\n",
    "\n",
    "2. Bucket for optimized filtering is available in Spark 2.4+.\n",
    "    For examples,\n",
    "    if the table `person` has a bucketed column `id` with an integer-compatible type,\n",
    "    then the following query in Spark 2.4+ will be optimized to avoid a scan of the whole table.\n",
    "    A few things to be aware here. \n",
    "    First, \n",
    "    you will still see a number of tasks close to the number of buckets in your Spark application.\n",
    "    This is becuase the optimized job will still have to check all buckets of the table \n",
    "    to see whether they are the right bucket corresponding to `id=123`.\n",
    "    (If yes, Spark will scan all rows in the bucket to filter records.\n",
    "    If not, the bucket will skipped to save time.)\n",
    "    Second, \n",
    "    the type of the value to compare must be compartible in order for Spark SQL to leverage bucket filtering.\n",
    "    For example,\n",
    "    if the `id` column in the `person` table is of the BigInt type \n",
    "    and `id = 123` is changed to `id = \"123\"` in the following query,\n",
    "    Spark will have to do a full table scan (even if it sounds extremely stupid to do so).\n",
    "\n",
    "        :::sql\n",
    "        SELECT *\n",
    "        FROM persons\n",
    "        WHERE id = 123\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricks and Trap on `DataFrame.write.partitionBy` and `DataFrame.write.bucketBy`\n",
    "\n",
    "Partition is an important concept in Spark\n",
    "which affects Spark performance in many ways. \n",
    "When reading a table to Spark,\n",
    "the number of partitions in memory equals to the number of files on disk if each file is smaller than the block size,\n",
    "otherwise, \n",
    "there will be more partitions in memory than the number of files on disk.\n",
    "Generally speaking,\n",
    "there shouldn't be too many small files in a table as this cause too many partitions (and thus small tasks) in the Spark job.\n",
    "When you write a Spark DataFrame into disk,\n",
    "the number of files on disk usually equals to the number of partitions in memory\n",
    "unless you use `partitionBy` or `bucketBy`.\n",
    "Suppose there is a DataFrame `df` which has `p` partitions in memory \n",
    "and it has a column named `col` which has `c` distinct values $v_1$, ..., $v_c$,\n",
    "when you write `df` to disk using `df.write.partitionBy(col)`,\n",
    "each of the `p` partitions in memory is written to separate partitions into the `c` directories on disk. \n",
    "This means that the final resulted number of partitions can be up to $c * p$.\n",
    "This is probably not what people want in most situations,\n",
    "instead,\n",
    "people often want exact $c$ partitions on disk when they call `df.write.partitionBy(col)`.\n",
    "According to the above explanation on how `Data.write.partitionBy` works,\n",
    "a simple fix is to have each partition in memory corresponding to a distinct value in the columnd `df.col`.\n",
    "That is a repartition of the DataFrame using the col `col` resolves the issue.\n",
    "\n",
    "    :::python\n",
    "    df.repartition(col).partitionBy(col)\n",
    "\n",
    "The above issue is not present when you `DataFrame.write.bucketBy` \n",
    "as `DataFrame.write.bucketBy` works by calculating hash code. \n",
    "There will always be the exact number of buckets/partitions on the disk \n",
    "as you specifed when you call the function `DataFrame.write.bucketBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.0.0-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Union\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------------------+------------------+------------------+\n",
      "|year|month|      date|                 x|                 y|                 z|\n",
      "+----+-----+----------+------------------+------------------+------------------+\n",
      "|2018|   12|2018-12-12|          15218.66|343419.90721800004|136.56000000000003|\n",
      "|2018|   12|2018-12-14|12127.650000000005|     252696.129202|125.28000000000002|\n",
      "|2018|   12|2018-12-05| 35484.22999999998|     442708.934149|            230.76|\n",
      "|2018|   10|2018-10-28|28418.420000000016|     515499.609327|268.80000000000007|\n",
      "|2019|    1|2019-01-07|          29843.17|     375139.756514|172.62000000000003|\n",
      "|2019|    1|2019-01-09|          30132.28|     212952.094433|            128.52|\n",
      "|2018|   11|2018-11-22| 38395.96999999998|     437842.863362|            237.12|\n",
      "|2018|   11|2018-11-23|          38317.15|391639.59950300003|212.22000000000003|\n",
      "|2018|   12|2018-12-30| 7722.129999999999|     210282.286054| 85.80000000000003|\n",
      "|2018|   10|2018-10-17|11101.180000000006|243019.40156300002|            150.84|\n",
      "|2018|   12|2018-12-16|          12058.41|     366604.422485|            154.62|\n",
      "|2018|   12|2018-12-04|35072.609999999986|     420124.638715|            222.78|\n",
      "|2018|   12|2018-12-20| 5015.930000000001|120790.77259400001| 46.13999999999999|\n",
      "|2018|   12|2018-12-15|11833.510000000006|276072.08876499993|            141.24|\n",
      "|2018|   11|2018-11-30|          38306.17|     395650.858456|            243.12|\n",
      "|2018|   11|2018-11-09|           25519.6|     287836.930741|184.01999999999995|\n",
      "|2018|   10|2018-10-14|11152.390000000005| 311944.5214740002|198.05999999999992|\n",
      "|2018|   10|2018-10-16|10974.380000000003|236304.82008200002|159.06000000000003|\n",
      "|2019|    1|2019-01-03|          30953.24|383834.70136999997|            197.52|\n",
      "|2019|    1|2019-01-06|          29520.23| 420714.7821390001|            217.98|\n",
      "+----+-----+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"../../home/media/data/daily.csv\")\n",
    "df = df.select(\n",
    "    year(\"date\").alias(\"year\"),\n",
    "    month(\"date\").alias(\"month\"), \"date\", \"x\", \"y\", \"z\"\n",
    ").repartition(2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"year\").parquet(\"part_by_year.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  \u001b[1m\u001b[36myear=2018\u001b[m\u001b[m \u001b[1m\u001b[36myear=2019\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls part_by_year.parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark support multiple levels of partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"year\",\n",
    "                                       \"month\").parquet(\"part_by_year_month.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mmonth=10\u001b[m\u001b[m \u001b[1m\u001b[36mmonth=11\u001b[m\u001b[m \u001b[1m\u001b[36mmonth=12\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls part_by_year_month.parquet/year=2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"daily.parquet\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(\"year\").write.mode(\"overwrite\").partitionBy(\"year\"\n",
    "                                                          ).parquet(\"daily.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00015-76ce0363-393a-4e1a-a387-488170fdcfbf.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls daily.parquet/year=2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00081-76ce0363-393a-4e1a-a387-488170fdcfbf.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls daily.parquet/year=2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"daily.parquet\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"year\").saveAsTable(\"daily_hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"daily_hive\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    create table daily_hive_2\n",
    "    using parquet     \n",
    "    partitioned by (year) as\n",
    "    select * from df\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"daily_hive_2\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Optimization Leveraging Bucketed Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-3.0.0-bin-hadoop3.2/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Union\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+\n",
      "|      date|                 x|                 y|                 z|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|2018-10-22|           10779.9|234750.19368899995|150.78000000000003|\n",
      "|2018-12-07|15637.329999999998|281424.52784600004|147.36000000000004|\n",
      "|2018-12-21|           4797.22|106753.64014699995|             47.46|\n",
      "|2018-10-17|11101.180000000006|243019.40156300002|            150.84|\n",
      "|2018-11-09|           25519.6|     287836.930741|184.01999999999995|\n",
      "|2018-11-28|           39134.8|446640.72524799994|             225.3|\n",
      "|2018-12-14|12127.650000000005|     252696.129202|125.28000000000002|\n",
      "|2018-12-09|          14820.05|     407420.724814|167.81999999999996|\n",
      "|2018-11-27|38929.669999999984|441879.99280600005|244.50000000000009|\n",
      "|2018-12-18|           7623.48|     189779.703736| 90.05999999999996|\n",
      "|2018-12-20| 5015.930000000001|120790.77259400001| 46.13999999999999|\n",
      "|2019-01-02|          29647.83|     379943.385348|             199.2|\n",
      "|2018-11-14|25252.369999999995|337906.34417199995|174.30000000000007|\n",
      "|2018-10-31|          27578.91| 352146.2405870001|            216.84|\n",
      "|2018-12-28| 7838.080000000003|     203588.781784|              84.6|\n",
      "|2018-12-13|13612.409999999998|321809.42337600014|             137.7|\n",
      "|2018-10-24| 24706.41000000001|     353630.071363|            249.78|\n",
      "|2018-11-04|25480.279999999995|446580.81299899996|            256.14|\n",
      "|2018-10-13|10977.450000000004|237820.89652399998|            143.88|\n",
      "|2018-11-25|          38150.71|492148.65518500004| 286.7399999999999|\n",
      "+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"../../home/media/data/daily.csv\")\n",
    "df = df.repartition(2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(10, \"date\").saveAsTable(\"daily_b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"daily_b2\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the execution plan does leverage bucketed columns for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [date#53, x#54, y#55, z#56]\n",
      "+- *(1) Filter (isnotnull(date#53) AND (date#53 = 2019-01-11))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet default.daily_b[date#53,x#54,y#55,z#56] Batched: true, DataFilters: [isnotnull(date#53), (date#53 = 2019-01-11)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/spark-3.0.0-bin-hadoop3.2/warehouse/daily_b], PartitionFilters: [], PushedFilters: [IsNotNull(date), EqualTo(date,2019-01-11)], ReadSchema: struct<date:string,x:string,y:string,z:string>, SelectedBucketsCount: 1 out of 10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select \n",
    "        * \n",
    "    from \n",
    "        daily_b\n",
    "    where\n",
    "        date = \"2019-01-11\"\n",
    "    \"\"\"\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/spark-2.3.4-bin-hadoop2.7/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark23 = SparkSession.builder.appName(\"PySpark_Union\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+\n",
      "|      date|                 x|                 y|                 z|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|2019-01-11|               0.0|               0.0|               0.0|\n",
      "|2019-01-10| 30436.96000000001|               0.0|               0.0|\n",
      "|2019-01-09|          30132.28|     212952.094433|            128.52|\n",
      "|2019-01-08|29883.240000000005|      352014.45016|            192.18|\n",
      "|2019-01-07|          29843.17|     375139.756514|172.62000000000003|\n",
      "|2019-01-06|          29520.23| 420714.7821390001|            217.98|\n",
      "|2019-01-05|          29308.36|376970.94769900007|             183.3|\n",
      "|2019-01-04|31114.940000000013|339321.70448899985|174.59999999999997|\n",
      "|2019-01-03|          30953.24|383834.70136999997|            197.52|\n",
      "|2019-01-02|          29647.83|     379943.385348|             199.2|\n",
      "|2019-01-01| 9098.830000000004|     221854.328826|             88.26|\n",
      "|2018-12-31|3522.9299999999994| 76976.74379300003| 30.83999999999999|\n",
      "|2018-12-30| 7722.129999999999|     210282.286054| 85.80000000000003|\n",
      "|2018-12-29|           7731.69|     184870.553121|             88.86|\n",
      "|2018-12-28| 7838.080000000003|     203588.781784|              84.6|\n",
      "|2018-12-27| 8031.669999999997|245940.99543200003|             90.84|\n",
      "|2018-12-26| 8819.809999999998|     194513.682991|             77.52|\n",
      "|2018-12-25| 6549.109999999998|136605.95935000002| 65.75999999999999|\n",
      "|2018-12-24|           5015.84| 87121.95556000003|             44.52|\n",
      "|2018-12-23| 5145.909999999998|     137563.979567|             52.02|\n",
      "+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark23.read.option(\"header\", \"true\").csv(\"../../home/media/data/daily.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(10, \"date\").saveAsTable(\"daily_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark23.table(\"daily_b\").rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the execution plan does not leverage bucketed columns for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [date#44, x#45, y#46, z#47]\n",
      "+- *(1) Filter (isnotnull(date#44) && (date#44 = 2019-01-11))\n",
      "   +- *(1) FileScan parquet default.daily_b[date#44,x#45,y#46,z#47] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/opt/spark-2.3.4-bin-hadoop2.7/warehouse/daily_b], PartitionFilters: [], PushedFilters: [IsNotNull(date), EqualTo(date,2019-01-11)], ReadSchema: struct<date:string,x:string,y:string,z:string>\n"
     ]
    }
   ],
   "source": [
    "spark23.sql(\n",
    "    \"\"\"\n",
    "    select \n",
    "        * \n",
    "    from \n",
    "        daily_b\n",
    "    where\n",
    "        date = \"2019-01-11\"\n",
    "    \"\"\"\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://mungingdata.com/apache-spark/partitionby/\n",
    "\n",
    "https://databricks.com/session_na20/bucketing-2-0-improve-spark-sql-performance-by-removing-shuffle\n",
    "\n",
    "https://issues.apache.org/jira/browse/SPARK-19256\n",
    "\n",
    "https://stackoverflow.com/questions/44808415/spark-parquet-partitioning-large-number-of-files\n",
    "\n",
    "https://stackoverflow.com/questions/48585744/why-is-spark-saveastable-with-bucketby-creating-thousands-of-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}