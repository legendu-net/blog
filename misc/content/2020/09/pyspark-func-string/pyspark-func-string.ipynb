{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-09-05 14:56:47\n",
    "- Title: String Functions in Spark\n",
    "- Slug: pyspark-func-string\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, string, round, Spark SQL, functions\n",
    "- Modified: 2020-09-05 14:56:47\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips and Traps\n",
    "\n",
    "1. You can use the `split` function to split a delimited string into an array.\n",
    "    It is suggested that removing trailing separators before you apply the `split` function.\n",
    "    Please refer to the split section before for more detailed discussions.\n",
    "\n",
    "1. Some string functions (e.g., `right`, etc.) are available in the Spark SQL APIs\n",
    "    but not available as Spark DataFrame APIs.\n",
    "\n",
    "2. Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters.\n",
    "\n",
    "2. `instr` and `locate` behaves similar to each other \n",
    "    except that their parameters are reversed.\n",
    "    \n",
    "2. Notice that `replace` is for replacing elements in a column \n",
    "    NOT for replacemnt inside each string element.\n",
    "    To replace substring with another one in a string,\n",
    "    you have to use either `regexp_replace` or `translate`.\n",
    "    \n",
    "6. The operator `+` does not work as concatenation for sting columns.\n",
    "    You have to use the function `concat` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|    1|\n",
      "|2017/02/01|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=[(\"2017/01/01\", 1), (\"2017/02/01\", 2)], columns=[\"date\", \"month\"]\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ascii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bit_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `+` operator does not work as concatenation for 2 string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|      date|month| col|\n",
      "+----------+-----+----+\n",
      "|2017/01/01|    1|null|\n",
      "|2017/02/01|    2|null|\n",
      "+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col\", col(\"date\") + col(\"month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `concat` concatenate 2 string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+\n",
      "|      date|month|        col|\n",
      "+----------+-----+-----------+\n",
      "|2017/01/01|    1|2017/01/011|\n",
      "|2017/02/01|    2|2017/02/012|\n",
      "+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col\", concat(col(\"date\"), col(\"month\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+\n",
      "|      date|month|         col|\n",
      "+----------+-----+------------+\n",
      "|2017/01/01|    1|2017/01/01_1|\n",
      "|2017/02/01|    2|2017/02/01_2|\n",
      "+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col\", concat(col(\"date\"), lit(\"_\"), col(\"month\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instr \n",
    "\n",
    "`instr` behaves similar to `locate` except that their parameters are reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select instr(\"abcd\", \"ab\") as index\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select instr(\"abcd\", \"AB\") as index\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lcase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| phrase|\n",
      "+-------+\n",
      "|how are|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        left(\"how are you doing?\", 7) as phrase\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|      2017|    1|\n",
      "|   2017/02|    2|\n",
      "|2018/02/05|    3|\n",
      "|      null|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (\"2017\", 1),\n",
    "    (\"2017/02\", 2),\n",
    "    (\"2018/02/05\", 3),\n",
    "    (null, 4)\n",
    ").toDF(\"date\", \"month\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      date|length(date)|\n",
      "+----------+------------+\n",
      "|      2017|           4|\n",
      "|   2017/02|           7|\n",
      "|2018/02/05|          10|\n",
      "|      null|        null|\n",
      "+----------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.length\n",
    "\n",
    "df.select($\"date\", length($\"date\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ltrim\n",
    "\n",
    "Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|after_ltrim|\n",
      "+-----------+\n",
      "|        bcd|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select ltrim(\"a \", \"a a abcd\") as after_ltrim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## locate\n",
    "\n",
    "`locate` behaves similar to `instr` except that their parameters are reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017-01-01|    1|\n",
      "|2017-02-01|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"date\", translate($\"date\", \"/\", \"-\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|    1|\n",
      "|2017/02/01|    2|\n",
      "|2018/02/05|    3|\n",
      "|      null|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (\"2017/01/01\", 1),\n",
    "    (\"2017/02/01\", 2),\n",
    "    (\"2018/02/05\", 3),\n",
    "    (null, 4)\n",
    ").toDF(\"date\", \"month\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/02/01|    2|\n",
      "|2018/02/05|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter($\"date\" rlike \"\\\\d{4}/02/\\\\d{2}\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "public static Column regexp_extract(Column e, String exp, int groupIdx)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|right('abcdefg', 3)|\n",
      "+-------------------+\n",
      "|                efg|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select right(\"abcdefg\", 3) \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017-01-01|    1|\n",
      "|2017-02-01|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date\", regexp_replace($\"date\", \"/\", \"-\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rtrim\n",
    "\n",
    "Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|     abcd\t|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select rtrim(\"abcd\\t \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|      abcd|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select rtrim(\" \\t\", \"abcd\\t \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|after_ltrim|\n",
      "+-----------+\n",
      "|   a a abcd|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select rtrim(\"a \", \"a a abcda a a\") as after_ltrim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split\n",
    "\n",
    "If there is a trailing separator, \n",
    "then an emptry string is generated at the end of the array.\n",
    "It is suggested that you get rid of the trailing separator \n",
    "before applying `split` \n",
    "to avoid unnecessary empty string generated.\n",
    "The benefit of doing this is 2-fold.\n",
    "\n",
    "1. Avoid generating non-neeed data (emtpy strings).\n",
    "2. Too many empty strings can causes serious data skew issues \n",
    "    if the corresponding column is used for joining with another table.\n",
    "    By avoiding generating those empty strings,\n",
    "    we avoid potential Spark issues in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    elements|\n",
      "+------------+\n",
      "|[ab, cd, ef]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select split(\"ab;cd;ef\", \";\") as elements\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      elements|\n",
      "+--------------+\n",
      "|[ab, cd, ef, ]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select split(\"ab;cd;ef;\", \";\") as elements\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## substring\n",
    "\n",
    "1. Uses 1-based index.\n",
    "\n",
    "2. `substring` on `null` returns `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|    1|\n",
      "|2017/02/01|    2|\n",
      "|      null|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = Seq(\n",
    "    (\"2017/01/01\", 1),\n",
    "    (\"2017/02/01\", 2),\n",
    "    (null, 3)\n",
    ").toDF(\"date\", \"month\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|      date|month|year|\n",
      "+----------+-----+----+\n",
      "|2017/01/01|    1|2017|\n",
      "|2017/02/01|    2|2017|\n",
      "|      null|    3|null|\n",
      "+----------+-----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"year\", substring($\"date\", 1, 4)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|   01|\n",
      "|2017/02/01|   02|\n",
      "|      null| null|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"month\", substring($\"date\", 6, 2)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|   01|\n",
      "|2017/02/01|   01|\n",
      "|      null| null|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"month\", substring($\"date\", 9, 2)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate\n",
    "\n",
    "Notice that translate is different from usual replacemnt!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trim\n",
    "\n",
    "Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|     abcd\t|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select trim(\"abcd\\t  \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|      abcd|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select trim(\" \\t\", \"abcd\\t \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "https://obstkel.com/spark-sql-functions\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}