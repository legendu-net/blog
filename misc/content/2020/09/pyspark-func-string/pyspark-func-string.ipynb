{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-09-05 14:56:47\n",
    "- Title: String Functions in Spark\n",
    "- Slug: pyspark-func-string\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, DataFrame, string, round, Spark SQL, functions\n",
    "- Modified: 2021-10-07 09:48:12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tips and Traps\n",
    "\n",
    "1. You can use the `split` function to split a delimited string into an array.\n",
    "    It is suggested that removing trailing separators before you apply the `split` function.\n",
    "    Please refer to the split section before for more detailed discussions.\n",
    "\n",
    "1. Some string functions (e.g., `right`, etc.) are available in the Spark SQL APIs\n",
    "    but not available as Spark DataFrame APIs.\n",
    "\n",
    "2. Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters.\n",
    "\n",
    "2. `instr` and `locate` behaves similar to each other \n",
    "    except that their parameters are reversed.\n",
    "    \n",
    "2. Notice that `replace` is for replacing elements in a column \n",
    "    NOT for replacemnt inside each string element.\n",
    "    To replace substring with another one in a string,\n",
    "    you have to use either `regexp_replace` or `translate`.\n",
    "    \n",
    "6. The operator `+` does not work as concatenation for sting columns.\n",
    "    You have to use the function `concat` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(4, 5), match=' '>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"\\\\s\", \"nima \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\\s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\s\\\\s'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\s\\\\s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\s\" == \"\\\\s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\" == \"\\\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/04 20:31:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/04 20:31:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "findspark.init(str(next(Path(\"/opt\").glob(\"spark-3*\"))))\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType\n",
    "spark = SparkSession.builder.appName(\"PySpark_Str_Func\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      col1|col2|\n",
      "+----------+----+\n",
      "|2017/01/01|   1|\n",
      "|2017/02/01|   2|\n",
      "|2018/02/05|   3|\n",
      "|      null|   4|\n",
      "|     how \t|   5|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        data=[\n",
    "            (\"2017/01/01\", 1), \n",
    "            (\"2017/02/01\", 2),\n",
    "            (\"2018/02/05\", 3),\n",
    "            (None, 4),\n",
    "            (\"how \\t\", 5),\n",
    "        ], columns=[\"col1\", \"col2\"]\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ascii](https://spark.apache.org/docs/latest/api/sql/index.html#ascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [base64](https://spark.apache.org/docs/latest/api/sql/index.html#base64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [bin](https://spark.apache.org/docs/latest/api/sql/index.html#bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [bit_length](https://spark.apache.org/docs/latest/api/sql/index.html#bit_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [char](https://spark.apache.org/docs/latest/api/sql/index.html#char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [char_length](https://spark.apache.org/docs/latest/api/sql/index.html#char_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [character_length](https://spark.apache.org/docs/latest/api/sql/index.html#character_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [chr](https://spark.apache.org/docs/latest/api/sql/index.html#chr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [coalesce](https://spark.apache.org/docs/latest/api/sql/index.html#coalesce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [concat](https://spark.apache.org/docs/latest/api/sql/index.html#concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `+` operator does not work as concatenation for 2 string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|      date|month| col|\n",
      "+----------+-----+----+\n",
      "|2017/01/01|    1|null|\n",
      "|2017/02/01|    2|null|\n",
      "+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col\", col(\"date\") + col(\"month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `concat` concatenate 2 string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+\n",
      "|      date|month|        col|\n",
      "+----------+-----+-----------+\n",
      "|2017/01/01|    1|2017/01/011|\n",
      "|2017/02/01|    2|2017/02/012|\n",
      "+----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col\", concat(col(\"date\"), col(\"month\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+\n",
      "|      date|month|         col|\n",
      "+----------+-----+------------+\n",
      "|2017/01/01|    1|2017/01/01_1|\n",
      "|2017/02/01|    2|2017/02/01_2|\n",
      "+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"col\", concat(col(\"date\"), lit(\"_\"), col(\"month\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [concat_ws](https://spark.apache.org/docs/latest/api/sql/index.html#concat_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [decode](https://spark.apache.org/docs/latest/api/sql/index.html#decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [encode](https://spark.apache.org/docs/latest/api/sql/index.html#encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [format_string](https://spark.apache.org/docs/latest/api/sql/index.html#format_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [hash](https://spark.apache.org/docs/latest/api/sql/index.html#hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [hex](https://spark.apache.org/docs/latest/api/sql/index.html#hex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [initcap](https://spark.apache.org/docs/latest/api/sql/index.html#initcap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [input_file_name](https://spark.apache.org/docs/latest/api/sql/index.html#input_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [instr](https://spark.apache.org/docs/latest/api/sql/index.html#instr)\n",
    "\n",
    "`instr` behaves similar to `locate` except that their parameters are reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select instr(\"abcd\", \"ab\") as index\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|index|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select instr(\"abcd\", \"AB\") as index\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [lcase](https://spark.apache.org/docs/latest/api/sql/index.html#lcase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [left](https://spark.apache.org/docs/latest/api/sql/index.html#left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| phrase|\n",
      "+-------+\n",
      "|how are|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        left(\"how are you doing?\", 7) as phrase\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [length](https://spark.apache.org/docs/latest/api/sql/index.html#length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|      2017|    1|\n",
      "|   2017/02|    2|\n",
      "|2018/02/05|    3|\n",
      "|      null|    4|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (\"2017\", 1),\n",
    "    (\"2017/02\", 2),\n",
    "    (\"2018/02/05\", 3),\n",
    "    (null, 4)\n",
    ").toDF(\"date\", \"month\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      date|length(date)|\n",
      "+----------+------------+\n",
      "|      2017|           4|\n",
      "|   2017/02|           7|\n",
      "|2018/02/05|          10|\n",
      "|      null|        null|\n",
      "+----------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.length\n",
    "\n",
    "df.select($\"date\", length($\"date\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [like](https://spark.apache.org/docs/latest/api/sql/index.html#like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [lpad](https://spark.apache.org/docs/latest/api/sql/index.html#lpad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ltrim](https://spark.apache.org/docs/latest/api/sql/index.html#ltrim)\n",
    "\n",
    "Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|after_ltrim|\n",
      "+-----------+\n",
      "|        bcd|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select ltrim(\"a \", \"a a abcd\") as after_ltrim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [locate](https://spark.apache.org/docs/latest/api/sql/index.html#locate)\n",
    "\n",
    "`locate` behaves similar to `instr` except that their parameters are reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017-01-01|    1|\n",
      "|2017-02-01|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"date\", translate($\"date\", \"/\", \"-\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [md5](https://spark.apache.org/docs/latest/api/sql/index.html#md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [octet_length](https://spark.apache.org/docs/latest/api/sql/index.html#octet_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [parse_url](https://spark.apache.org/docs/latest/api/sql/index.html#parse_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [position](https://spark.apache.org/docs/latest/api/sql/index.html#position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [printf](https://spark.apache.org/docs/latest/api/sql/index.html#printf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [regex_extract](https://spark.apache.org/docs/latest/api/sql/index.html#regexp_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "public static Column regexp_extract(Column e, String exp, int groupIdx)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [regex_extract_all](https://spark.apache.org/docs/latest/api/sql/index.html#regexp_extract_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [regexp_replace](https://spark.apache.org/docs/latest/api/sql/index.html#regexp_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017-01-01|    1|\n",
      "|2017-02-01|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date\", regexp_replace(col(\"date\"), \"/\", \"-\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [repeat](https://spark.apache.org/docs/latest/api/sql/index.html#repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [replace](https://spark.apache.org/docs/latest/api/sql/index.html#replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [reverse](https://spark.apache.org/docs/latest/api/sql/index.html#reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [right](https://spark.apache.org/docs/latest/api/sql/index.html#right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|right('abcdefg', 3)|\n",
      "+-------------------+\n",
      "|                efg|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select right(\"abcdefg\", 3) \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [rlike](https://spark.apache.org/docs/latest/api/sql/index.html#rlike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      col1|col2|\n",
      "+----------+----+\n",
      "|2017/01/01|   1|\n",
      "|2017/02/01|   2|\n",
      "|2018/02/05|   3|\n",
      "|      null|   4|\n",
      "|     how \t|   5|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      col1|col2|\n",
      "+----------+----+\n",
      "|2017/02/01|   2|\n",
      "|2018/02/05|   3|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"col1\").rlike(\"\\\\d{4}/02/\\\\d{2}\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| col1|col2|\n",
      "+-----+----+\n",
      "|how \t|   5|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"col1\").rlike(r\"\\s\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"t1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|      col1|col2|\n",
      "+----------+----+\n",
      "|2017/01/01|   1|\n",
      "|2017/02/01|   2|\n",
      "|2018/02/05|   3|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(r\"\"\"\n",
    "    select \n",
    "        *\n",
    "    from \n",
    "        t1 \n",
    "    where\n",
    "        col1 rlike '\\\\d'\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [rpad](https://spark.apache.org/docs/latest/api/sql/index.html#rpad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [rtrim](https://spark.apache.org/docs/latest/api/sql/index.html#rtrim)\n",
    "\n",
    "Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|     abcd\t|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select rtrim(\"abcd\\t \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|      abcd|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/04 20:32:27 WARN Analyzer$ResolveFunctions: Two-parameter TRIM/LTRIM/RTRIM function signatures are deprecated. Use SQL syntax `TRIM((BOTH | LEADING | TRAILING)? trimStr FROM str)` instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select rtrim(\" \\t\", \"abcd\\t \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|after_ltrim|\n",
      "+-----------+\n",
      "|   a a abcd|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select rtrim(\"a \", \"a a abcda a a\") as after_ltrim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [sentences](https://spark.apache.org/docs/latest/api/sql/index.html#sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [sha](https://spark.apache.org/docs/latest/api/sql/index.html#sha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [sha1](https://spark.apache.org/docs/latest/api/sql/index.html#sha1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [sha2](https://spark.apache.org/docs/latest/api/sql/index.html#sha2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [split](https://spark.apache.org/docs/latest/api/sql/index.html#split)\n",
    "\n",
    "If there is a trailing separator, \n",
    "then an emptry string is generated at the end of the array.\n",
    "It is suggested that you get rid of the trailing separator \n",
    "before applying `split` \n",
    "to avoid unnecessary empty string generated.\n",
    "The benefit of doing this is 2-fold.\n",
    "\n",
    "1. Avoid generating non-neeed data (emtpy strings).\n",
    "2. Too many empty strings can causes serious data skew issues \n",
    "    if the corresponding column is used for joining with another table.\n",
    "    By avoiding generating those empty strings,\n",
    "    we avoid potential Spark issues in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    elements|\n",
      "+------------+\n",
      "|[ab, cd, ef]|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select split(\"ab;cd;ef\", \";\") as elements\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      elements|\n",
      "+--------------+\n",
      "|[ab, cd, ef, ]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select split(\"ab;cd;ef;\", \";\") as elements\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [string](https://spark.apache.org/docs/latest/api/sql/index.html#string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [substr](https://spark.apache.org/docs/latest/api/sql/index.html#substr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [substring](https://spark.apache.org/docs/latest/api/sql/index.html#substring)\n",
    "\n",
    "1. Uses 1-based index.\n",
    "\n",
    "2. `substring` on `null` returns `null`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|    1|\n",
      "|2017/02/01|    2|\n",
      "|      null|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = Seq(\n",
    "    (\"2017/01/01\", 1),\n",
    "    (\"2017/02/01\", 2),\n",
    "    (null, 3)\n",
    ").toDF(\"date\", \"month\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|      date|month|year|\n",
      "+----------+-----+----+\n",
      "|2017/01/01|    1|2017|\n",
      "|2017/02/01|    2|2017|\n",
      "|      null|    3|null|\n",
      "+----------+-----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"year\", substring($\"date\", 1, 4)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|   01|\n",
      "|2017/02/01|   02|\n",
      "|      null| null|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"month\", substring($\"date\", 6, 2)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      date|month|\n",
      "+----------+-----+\n",
      "|2017/01/01|   01|\n",
      "|2017/02/01|   01|\n",
      "|      null| null|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"month\", substring($\"date\", 9, 2)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [substring_index](https://spark.apache.org/docs/latest/api/sql/index.html#substring_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [translate](https://spark.apache.org/docs/latest/api/sql/index.html#translate)\n",
    "\n",
    "Notice that translate is different from usual replacemnt!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [trim](https://spark.apache.org/docs/latest/api/sql/index.html#trim)\n",
    "\n",
    "Notice that functions `trim`/`rtrim`/`ltrim` behaves a little counter-intuitive.\n",
    "    First, \n",
    "    they trim spaces only rather than white spaces by default.\n",
    "    Second,\n",
    "    when explicitly passing the characters to trim,\n",
    "    the 1st parameter is the characters to trim \n",
    "    and the 2nd parameter is the string from which to trim characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|     abcd\t|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select trim(\"abcd\\t  \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|after_trim|\n",
      "+----------+\n",
      "|      abcd|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select trim(\" \\t\", \"abcd\\t \") as after_trim\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [trunc](https://spark.apache.org/docs/latest/api/sql/index.html#trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ucase](https://spark.apache.org/docs/latest/api/sql/index.html#ucase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [unbase64](https://spark.apache.org/docs/latest/api/sql/index.html#unbase64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [unhex](https://spark.apache.org/docs/latest/api/sql/index.html#unhex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [upper](https://spark.apache.org/docs/latest/api/sql/index.html#upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [uuid](https://spark.apache.org/docs/latest/api/sql/index.html#uuid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [xxhash64](https://spark.apache.org/docs/latest/api/sql/index.html#xxhash64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "[Spark Scala Functions](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html)\n",
    "\n",
    "[Spark SQL Built-in Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "\n",
    "https://obstkel.com/spark-sql-functions\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
