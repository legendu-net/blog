{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Title: Read/Write Files/Tables in Spark\n",
    "- Slug: spark-io\n",
    "- Date: 2020-09-27 23:00:01\n",
    "- Category: Computer Science\n",
    "- Tags: programming, Scala, Spark, IO, read, write, file, table\n",
    "- Author: Ben Du\n",
    "- Modified: 2020-09-27 23:00:01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader) \n",
    "APIs\n",
    "\n",
    "[DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)\n",
    "APIs\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources\n",
    "\n",
    "## Comments\n",
    "\n",
    "1. It is suggested that you \n",
    "    [specify a schema when reading text files](https://stackoverflow.com/questions/39926411/provide-schema-while-reading-csv-file-as-a-dataframe).\n",
    "    If a schema is not specified when reading text files,\n",
    "    it is good practice to check the types of columns (as the types are inferred).\n",
    "    \n",
    "3. Do NOT read data from and write data to the same path in Spark!\n",
    "    Due to lazy evaluation of Spark, \n",
    "    the path will likely be cleared before it is read into Spark,\n",
    "    which will throw IO exceptions.\n",
    "    And the worst part is that your data on HDFS is removed but recoverable.\n",
    "\n",
    "2. When you want to keep headers in the data,\n",
    "    it is suggested that you use the Parquet format to save data for multiple reasons.\n",
    "    It is fast, takes less space\n",
    "    and you don't have to worry about duplicated headers encountered when merging CSV files with headers.\n",
    "    There is no need to merge Parquet files.\n",
    "    You can directly read in a folder of Parquet files in all kinds of programming languages,\n",
    "    which is more convenient than a folder of CSV files.\n",
    "\n",
    "3. Writing to existing files using `RDD.saveAsTextFile` throws file already exist exception.\n",
    "    This s because Hadoop filesystem does not overwrite files that already exist by default.\n",
    "    `RDD.saveAsTextFile` does not provide an option to manually overwrite existing files.\n",
    "    To avoid the issue,\n",
    "    you have to manually remove the existing file before writing to them.\n",
    "    Of course,\n",
    "    it is no longer suggested to use RDD directly any more in Spark.\n",
    "    You use should DataFrame as much as possible.\n",
    "    `DataFrame.write` allows you to overwrite existing HDFS files via `DataFrame.write.option(\"overwrite\")`.\n",
    "    It is suggested that you always use the `overwrite` mode when writing files/tables in Spark.\n",
    "    \n",
    "3. `DataFrame.write` does not overwriting existing files by default\n",
    "    and it simply throws an exception.\n",
    "    You can use the option `.option(\"overwrite\")` to force overwrite existing files.\n",
    "    And it is suggested that always use the `overwrite` mode \n",
    "    when writting to files/tables in Spark.\n",
    "    \n",
    "        DataFrame.write.option(\"overwrite\").parquet(\"/path/to/write/files\")\n",
    "\n",
    "2. It takes lots of time for `hadoop fs -getmerge`\n",
    "    to extract and merge large number of compressed text files.\n",
    "    So it is suggested that you turn off compression\n",
    "    when saving results into text files,\n",
    "    especially when there are huge number of partitions.\n",
    "\n",
    "5. `spark.table` is known for reading a Hive table. \n",
    "    However, \n",
    "    it can be used to read any HDFS table too.\n",
    "    For example, \n",
    "    it can be used to read a Parquet file.\n",
    "\n",
    "        spark.table(\"parquet.`/path/to/table`\")\n",
    "\n",
    "    It is not recommended to use `spark.table` to read text format HDFS table though\n",
    "    as you won't be able to specify options for loading the table.\n",
    "\n",
    "## Readiness of Data on HDFS\n",
    "\n",
    "1. Data in a Hive table guarantees completeness\n",
    "    which means that if you see data of a certain date in the table,\n",
    "    the complete data is there.\n",
    "    However, if you work with other format (Parquet, Avro, Sequence, etc.),\n",
    "    you'd better check for data readiness before you use it.\n",
    "    A simple way to do this is to check whether the `_SUCESS` file exists in the same directory.\n",
    "\n",
    "\n",
    "## Parquet\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery\n",
    "\n",
    "1. You have issues saving a DataFrame read in from Parquet to a CSV file.\n",
    "    The reason is that Parquet support more complex data structures (e.g., array)\n",
    "    which is not supported in CSV.\n",
    "\n",
    "2. Shell-like syntax (curly brace, wildcard, etc.) of matching multiple files is supported in both the Scala API\n",
    "    and the SQL API when querying files directly\n",
    "    as long as it does cause CONFLICTING directory structure.\n",
    "\n",
    "\n",
    "        files = 's3a://dev/2017/01/{02,03}/data.parquet'\n",
    "        df = session.read.parquet(files)\n",
    "\n",
    "        spark.read.parquet(\"some_path/2019-02-10_05-38-11/SITE_NAME=*\")\n",
    "\n",
    "        select\n",
    "            count(*) as n\n",
    "        from\n",
    "            parquet.`/some_path/2019-02-10_05-38-11/SITE_NAME=*`\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "\n",
    "https://docs.databricks.com/spark/latest/data-sources/read-parquet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html#input_file_name--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `input_file_name` in the package `org.apache.spark.sql.functions` \n",
    "creates a string column for the file name of the current Spark tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.write.option(\"compression\", \"none\").mode(\"overwrite\").save(\"testoutput.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.\n",
    "    mode(\"overwrite\").\n",
    "    format(\"parquet\").\n",
    "    option(\"compression\", \"none\").\n",
    "    save(\"/tmp/file_no_compression_parq\")\n",
    "df.write.\n",
    "    mode(\"overwrite\").\n",
    "    format(\"parquet\").\n",
    "    option(\"compression\", \"gzip\").\n",
    "    save(\"/tmp/file_with_gzip_parq\")\n",
    "df.write.\n",
    "    mode(\"overwrite\").\n",
    "    format(\"parquet\").\n",
    "    option(\"compression\", \"snappy\").\n",
    "    save(\"/tmp/file_with_snappy_parq\")\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"orc\").option(\"compression\", \"none\").mode(\"overwrite\").save(\"/tmp/file_no_compression_orc\")\n",
    "df.write.mode(\"overwrite\").format(\"orc\").option(\"compression\", \"snappy\").mode(\"overwrite\").save(\"/tmp/file_with_snappy_orc\")\n",
    "df.write.mode(\"overwrite\").format(\"orc\").option(\"compression\", \"zlib\").mode(\"overwrite\").save(\"/tmp/file_with_zlib_orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode(\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option(\"compression\", \"none\")\n",
    "option(\"compression\", \"gzip\")\n",
    "option(\"compression\", \"snappy\")\n",
    "option(\"compression\", \"zlib\")\n",
    "\n",
    "option(\"inferSchema\", true)\n",
    ".option(\"nullValue\", \"NA\")\n",
    ".option(\"quote\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are old-fashioned RDD-related tips. \n",
    "Skip them if you do not have to use RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD.saveAsTextFile\n",
    "\n",
    "1. `RDD.saveAsTextFile` does not provide options to control the format of output files.\n",
    "    You have to manually format the RDD to one containings string in the format that you want.\n",
    "    For example,\n",
    "    if rdd contains tuples and you want to output it into TSV files,\n",
    "    you can format it first using the following code.\n",
    "\n",
    "        rdd.map { x => x.productIterator.mkString(\"\\t\") }\n",
    "\n",
    "2. You can use the following statement to turn off compression \n",
    "    when saving results to files (suggested when writing to text files).\n",
    "    \n",
    "        sc.hadoopConfiguration.set(\"mapred.output.compress\", \"false\")\n",
    "    \n",
    "3. Writing to existing files using `RDD.saveAsTextFile` throws file already exist exception.\n",
    "    This s because Hadoop filesystem does not overwrite files that already exist by default.\n",
    "    `RDD.saveAsTextFile` does not provide an option to manually overwrite existing files.\n",
    "    To avoid the issue,\n",
    "    you have to manually remove the existing file before writing to them.\n",
    "    Of course,\n",
    "    it is no longer suggested to use RDD directly any more in Spark.\n",
    "    You use should use DataFrame as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/functions.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Row.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}