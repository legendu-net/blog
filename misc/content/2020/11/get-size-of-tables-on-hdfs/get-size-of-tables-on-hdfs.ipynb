{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Ben Du\n",
    "- Date: 2020-11-10 12:41:56\n",
    "- Title: Get Size of Tables on HDFS\n",
    "- Slug: get-size-of-tables-on-hdfs\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, programming, HDFS, table, size, Spark, SQL, Spark SQL, Hive\n",
    "- Modified: 2021-01-10 12:41:56\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement! **  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The HDFS Way \n",
    "\n",
    "You can use the `hdfs dfs -du /path/to/table` command \n",
    "or `hdfs dfs -count -q -v -h /path/to/table`\n",
    "to get the size of an HDFS path (or table). \n",
    "However, \n",
    "this only works if the cluster supports HDFS. \n",
    "If a Spark cluster exposes only JDBC/ODBC APIs, \n",
    "this method does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SQL Query Way\n",
    "\n",
    "### Size of One Table\n",
    "\n",
    "tblproperties will give the size of the table and can be used to grab just that value if needed.\n",
    "\n",
    "    :::sql\n",
    "    describe formatted table_name;\n",
    "    show tblproperties table_name\n",
    "    -- or\n",
    "    show tblproperties table_ame(\"rawDataSize\")\n",
    "\n",
    "Yes the output is bytes. Also, this only works for non-partitioned tables which have had stats run on them.\n",
    "\n",
    "### Size of Multiple Tables \n",
    "\n",
    "    :::sql\n",
    "    show table extended in some_db "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYZE TABLE tablename [PARTITION(partcol1[=val1], partcol2[=val2], ...)] COMPUTE STATISTICS [noscan];\n",
    "\n",
    "\n",
    "ANALYZE TABLE ops_bc_log PARTITION(day) COMPUTE STATISTICS noscan;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "https://medium.com/@gomzvicky/finding-total-size-of-hive-databases-data-d2ce8fa96cbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}