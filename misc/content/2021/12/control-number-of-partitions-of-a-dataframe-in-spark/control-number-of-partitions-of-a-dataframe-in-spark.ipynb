{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Benjamin Du\n",
    "- Date: 2021-12-11 17:21:56\n",
    "- Modified: 2021-12-11 17:21:56\n",
    "- Title: Control Number of Partitions of a DataFrame in Spark\n",
    "- Slug: control-number-of-partitions-of-a-dataframe-in-spark\n",
    "- Category: Computer Science\n",
    "- Tags: Computer Science, programming, Spark, PySpark, big data, partition, repartition, maxPartitionBytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things on this page are fragmentary and immature notes/thoughts of the author. Please read with your own judgement!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Traps\n",
    "\n",
    "1. `DataFrame.repartition` repartitions the DataFrame by **hash code** of each row. \n",
    "    If you specify a (multiple) column(s) (instead of number of partitions) \n",
    "    to the method `DataFrame.repartition`,\n",
    "    then hash code of the column(s) are calculated for repartition. \n",
    "    In some situations,\n",
    "    there are lots of hash conflictions \n",
    "    even if the total number of rows is small (e.g., a few thousand),\n",
    "    which means that partitions generated might be skewed\n",
    "    and causes a few long-running tasks. \n",
    "    If this ever happens, \n",
    "    it is suggested that you manually add a column\n",
    "    which helps the hashing algoirthm. \n",
    "    Notice that an existing integer column in the DataFrame \n",
    "    is not necessarily a good column to repartition by \n",
    "    especially when those integers are big (e.g., u64)\n",
    "    as hash code of those integers can easily conflicts. \n",
    "    It is best to add a column of random numbers \n",
    "    of a column of manually curated partition indexes\n",
    "    and ask Spark to repartition based on that column.\n",
    "    \n",
    "2. By default, \n",
    "    Spark automatically merges several small files into one partition \n",
    "    when loading a HDFS table into a DataFrame. \n",
    "    The behavior is control by the parameter `spark.sql.files.maxPartitionBytes`.\n",
    "    The default value for this option is 128M\n",
    "    which means that Spark keeps reading small files into one partition of a DataFrame\n",
    "    until reading another file makes the size of the partition exceedes 128M. \n",
    "    Generally speaking,\n",
    "    you want to keep the default value for this setting \n",
    "    as it yields optimal performance for handling large data table. \n",
    "    However,\n",
    "    if your Spark application deals with small data but is CPU intensive,\n",
    "    it makes more sense to set a much smaller value for `spark.sql.files.maxPartitionBytes`\n",
    "    so that there are more partitions generated and yield a higher level of parallelism.\n",
    "    Of course,\n",
    "    you can always repartition a DataFrame manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
